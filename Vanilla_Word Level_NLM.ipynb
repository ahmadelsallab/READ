{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import array\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import string\n",
    "os.environ['KERAS_BACKEND']='tensorflow'\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout, LSTM, GRU, Bidirectional, TimeDistributed\n",
    "from keras.models import Model, Sequential\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from pickle import dump\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpu_alloc(device_id):\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]=device_id\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_alloc(\"1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLM\n",
    "https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ï»¿BOOK I.\n",
      "\n",
      "I went down yesterday to the Piraeus with Glaucon the son of Ariston,\n",
      "that I might offer up my prayers to the goddess (Bendis, the Thracian\n",
      "Artemis.); and also because I wanted to see in wha\n",
      "['i', 'i', 'went', 'down', 'yesterday', 'to', 'the', 'piraeus', 'with', 'glaucon', 'the', 'son', 'of', 'ariston', 'that', 'i', 'might', 'offer', 'up', 'my', 'prayers', 'to', 'the', 'goddess', 'bendis', 'the', 'thracian', 'artemis', 'and', 'also', 'because', 'i', 'wanted', 'to', 'see', 'in', 'what', 'manner', 'they', 'would', 'celebrate', 'the', 'festival', 'which', 'was', 'a', 'new', 'thing', 'i', 'was', 'delighted', 'with', 'the', 'procession', 'of', 'the', 'inhabitants', 'but', 'that', 'of', 'the', 'thracians', 'was', 'equally', 'if', 'not', 'more', 'beautiful', 'when', 'we', 'had', 'finished', 'our', 'prayers', 'and', 'viewed', 'the', 'spectacle', 'we', 'turned', 'in', 'the', 'direction', 'of', 'the', 'city', 'and', 'at', 'that', 'instant', 'polemarchus', 'the', 'son', 'of', 'cephalus', 'chanced', 'to', 'catch', 'sight', 'of', 'us', 'from', 'a', 'distance', 'as', 'we', 'were', 'starting', 'on', 'our', 'way', 'home', 'and', 'told', 'his', 'servant', 'to', 'run', 'and', 'bid', 'us', 'wait', 'for', 'him', 'the', 'servant', 'took', 'hold', 'of', 'me', 'by', 'the', 'cloak', 'behind', 'and', 'said', 'polemarchus', 'desires', 'you', 'to', 'wait', 'i', 'turned', 'round', 'and', 'asked', 'him', 'where', 'his', 'master', 'was', 'there', 'he', 'is', 'said', 'the', 'youth', 'coming', 'after', 'you', 'if', 'you', 'will', 'only', 'wait', 'certainly', 'we', 'will', 'said', 'glaucon', 'and', 'in', 'a', 'few', 'minutes', 'polemarchus', 'appeared', 'and', 'with', 'him', 'adeimantus', 'glaucons', 'brother', 'niceratus', 'the', 'son', 'of', 'nicias', 'and', 'several', 'others', 'who', 'had', 'been', 'at', 'the', 'procession', 'polemarchus', 'said', 'to']\n",
      "Total Tokens: 118683\n",
      "Unique Tokens: 7409\n",
      "Total Sequences: 118632\n"
     ]
    }
   ],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "    # replace '--' with a space ' '\n",
    "    doc = doc.replace('--', ' ')\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # make lower case\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    return tokens\n",
    "\n",
    "# save tokens to file, one dialog per line\n",
    "def save_doc(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "\n",
    "# load document\n",
    "in_filename = './dat/republic_clean.txt'\n",
    "doc = load_doc(in_filename)\n",
    "print(doc[:200])\n",
    "\n",
    "# clean document\n",
    "tokens = clean_doc(doc)\n",
    "print(tokens[:200])\n",
    "print('Total Tokens: %d' % len(tokens))\n",
    "print('Unique Tokens: %d' % len(set(tokens)))\n",
    "\n",
    "# organize into sequences of tokens\n",
    "length = 50 + 1\n",
    "sequences = list()\n",
    "for i in range(length, len(tokens)):\n",
    "    # select sequence of tokens\n",
    "    seq = tokens[i-length:i]\n",
    "    # convert into a line\n",
    "    line = ' '.join(seq)\n",
    "    # store\n",
    "    sequences.append(line)\n",
    "print('Total Sequences: %d' % len(sequences))\n",
    "\n",
    "# save sequences to file\n",
    "out_filename = 'republic_sequences.txt'\n",
    "save_doc(sequences, out_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 50, 50)            370500    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 50, 100)           60400     \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 7410)              748410    \n",
      "=================================================================\n",
      "Total params: 1,269,810\n",
      "Trainable params: 1,269,810\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "118632/118632 [==============================] - 111s 937us/step - loss: 6.1725 - acc: 0.0696\n",
      "Epoch 2/100\n",
      "118632/118632 [==============================] - 109s 918us/step - loss: 5.7437 - acc: 0.1018\n",
      "Epoch 3/100\n",
      "118632/118632 [==============================] - 109s 915us/step - loss: 5.5273 - acc: 0.1233\n",
      "Epoch 4/100\n",
      "118632/118632 [==============================] - 109s 917us/step - loss: 5.3765 - acc: 0.1379\n",
      "Epoch 5/100\n",
      "118632/118632 [==============================] - 109s 923us/step - loss: 5.2399 - acc: 0.1501\n",
      "Epoch 6/100\n",
      "118632/118632 [==============================] - 110s 926us/step - loss: 5.1706 - acc: 0.1538\n",
      "Epoch 7/100\n",
      "118632/118632 [==============================] - 109s 917us/step - loss: 5.0676 - acc: 0.1610\n",
      "Epoch 8/100\n",
      "118632/118632 [==============================] - 109s 916us/step - loss: 4.9908 - acc: 0.1652\n",
      "Epoch 9/100\n",
      "118632/118632 [==============================] - 109s 918us/step - loss: 4.9167 - acc: 0.1698\n",
      "Epoch 10/100\n",
      "118632/118632 [==============================] - 108s 914us/step - loss: 4.8459 - acc: 0.1741\n",
      "Epoch 11/100\n",
      "118632/118632 [==============================] - 110s 923us/step - loss: 4.7813 - acc: 0.1776\n",
      "Epoch 12/100\n",
      "118632/118632 [==============================] - 109s 919us/step - loss: 4.7169 - acc: 0.1808\n",
      "Epoch 13/100\n",
      "118632/118632 [==============================] - 110s 923us/step - loss: 4.6565 - acc: 0.1835\n",
      "Epoch 14/100\n",
      "118632/118632 [==============================] - 109s 920us/step - loss: 4.6020 - acc: 0.1864\n",
      "Epoch 15/100\n",
      "118632/118632 [==============================] - 109s 920us/step - loss: 4.5642 - acc: 0.1888\n",
      "Epoch 16/100\n",
      "118632/118632 [==============================] - 109s 923us/step - loss: 4.5091 - acc: 0.1908\n",
      "Epoch 17/100\n",
      "118632/118632 [==============================] - 109s 920us/step - loss: 4.4581 - acc: 0.1937\n",
      "Epoch 18/100\n",
      "118632/118632 [==============================] - 109s 916us/step - loss: 4.4195 - acc: 0.1962\n",
      "Epoch 19/100\n",
      "118632/118632 [==============================] - 109s 920us/step - loss: 4.4241 - acc: 0.1949\n",
      "Epoch 20/100\n",
      "118632/118632 [==============================] - 109s 921us/step - loss: 4.3659 - acc: 0.1980\n",
      "Epoch 21/100\n",
      "118632/118632 [==============================] - 109s 918us/step - loss: 4.3424 - acc: 0.1986\n",
      "Epoch 22/100\n",
      "118632/118632 [==============================] - 109s 922us/step - loss: 4.2845 - acc: 0.2025\n",
      "Epoch 23/100\n",
      "118632/118632 [==============================] - 110s 924us/step - loss: 4.2360 - acc: 0.2056\n",
      "Epoch 24/100\n",
      "118632/118632 [==============================] - 109s 917us/step - loss: 4.1980 - acc: 0.2086\n",
      "Epoch 25/100\n",
      "118632/118632 [==============================] - 109s 922us/step - loss: 4.1576 - acc: 0.2108\n",
      "Epoch 26/100\n",
      "118632/118632 [==============================] - 109s 920us/step - loss: 4.1218 - acc: 0.2132\n",
      "Epoch 27/100\n",
      "118632/118632 [==============================] - 109s 923us/step - loss: 4.0909 - acc: 0.2161\n",
      "Epoch 28/100\n",
      "118632/118632 [==============================] - 109s 922us/step - loss: 4.0677 - acc: 0.2183\n",
      "Epoch 29/100\n",
      "118632/118632 [==============================] - 110s 923us/step - loss: 4.0268 - acc: 0.2215\n",
      "Epoch 30/100\n",
      "118632/118632 [==============================] - 109s 915us/step - loss: 3.9943 - acc: 0.2253\n",
      "Epoch 31/100\n",
      "118632/118632 [==============================] - 109s 922us/step - loss: 3.9818 - acc: 0.2265\n",
      "Epoch 32/100\n",
      "118632/118632 [==============================] - 110s 924us/step - loss: 4.0068 - acc: 0.2266\n",
      "Epoch 33/100\n",
      "118632/118632 [==============================] - 110s 930us/step - loss: 3.9695 - acc: 0.2286\n",
      "Epoch 34/100\n",
      "118632/118632 [==============================] - 109s 923us/step - loss: 3.9286 - acc: 0.2320\n",
      "Epoch 35/100\n",
      "118632/118632 [==============================] - 110s 928us/step - loss: 3.9007 - acc: 0.2353\n",
      "Epoch 36/100\n",
      "118632/118632 [==============================] - 110s 927us/step - loss: 3.8641 - acc: 0.2379\n",
      "Epoch 37/100\n",
      "118632/118632 [==============================] - 110s 923us/step - loss: 3.8329 - acc: 0.2415\n",
      "Epoch 38/100\n",
      "118632/118632 [==============================] - 110s 925us/step - loss: 3.7922 - acc: 0.2444\n",
      "Epoch 39/100\n",
      "118632/118632 [==============================] - 110s 924us/step - loss: 3.7634 - acc: 0.2481\n",
      "Epoch 40/100\n",
      "118632/118632 [==============================] - 110s 931us/step - loss: 3.7503 - acc: 0.2501\n",
      "Epoch 41/100\n",
      "118632/118632 [==============================] - 109s 922us/step - loss: 4.1677 - acc: 0.2115\n",
      "Epoch 42/100\n",
      "118632/118632 [==============================] - 110s 923us/step - loss: 4.0589 - acc: 0.2194\n",
      "Epoch 43/100\n",
      "118632/118632 [==============================] - 110s 926us/step - loss: 3.9965 - acc: 0.2248\n",
      "Epoch 44/100\n",
      "118632/118632 [==============================] - 110s 924us/step - loss: 3.9461 - acc: 0.2290\n",
      "Epoch 45/100\n",
      "118632/118632 [==============================] - 110s 927us/step - loss: 3.9043 - acc: 0.2327\n",
      "Epoch 46/100\n",
      "118632/118632 [==============================] - 111s 932us/step - loss: 3.8622 - acc: 0.2376\n",
      "Epoch 47/100\n",
      "118632/118632 [==============================] - 110s 927us/step - loss: 3.8231 - acc: 0.2418\n",
      "Epoch 48/100\n",
      "118632/118632 [==============================] - 111s 932us/step - loss: 3.7852 - acc: 0.2460\n",
      "Epoch 49/100\n",
      "118632/118632 [==============================] - 112s 947us/step - loss: 3.7474 - acc: 0.2493\n",
      "Epoch 50/100\n",
      "118632/118632 [==============================] - 110s 928us/step - loss: 3.7106 - acc: 0.2536\n",
      "Epoch 51/100\n",
      "118632/118632 [==============================] - 110s 925us/step - loss: 3.6755 - acc: 0.2582\n",
      "Epoch 52/100\n",
      "118632/118632 [==============================] - 110s 926us/step - loss: 3.6406 - acc: 0.2623\n",
      "Epoch 53/100\n",
      "118632/118632 [==============================] - 109s 920us/step - loss: 3.6032 - acc: 0.2669\n",
      "Epoch 54/100\n",
      "118632/118632 [==============================] - 110s 925us/step - loss: 3.5707 - acc: 0.2702\n",
      "Epoch 55/100\n",
      "118632/118632 [==============================] - 110s 930us/step - loss: 3.5363 - acc: 0.2744\n",
      "Epoch 56/100\n",
      "118632/118632 [==============================] - 110s 928us/step - loss: 3.5008 - acc: 0.2798\n",
      "Epoch 57/100\n",
      "118632/118632 [==============================] - 111s 933us/step - loss: 3.4704 - acc: 0.2819\n",
      "Epoch 58/100\n",
      "118632/118632 [==============================] - 146s 1ms/step - loss: 3.4354 - acc: 0.2867\n",
      "Epoch 59/100\n",
      "118632/118632 [==============================] - 166s 1ms/step - loss: 3.4038 - acc: 0.2918\n",
      "Epoch 60/100\n",
      "118632/118632 [==============================] - 165s 1ms/step - loss: 3.3718 - acc: 0.2950\n",
      "Epoch 61/100\n",
      "118632/118632 [==============================] - 165s 1ms/step - loss: 3.3392 - acc: 0.2999\n",
      "Epoch 62/100\n",
      "118632/118632 [==============================] - 165s 1ms/step - loss: 3.3072 - acc: 0.3042\n",
      "Epoch 63/100\n",
      "118632/118632 [==============================] - 205s 2ms/step - loss: 3.2758 - acc: 0.3090\n",
      "Epoch 64/100\n",
      "118632/118632 [==============================] - 189s 2ms/step - loss: 3.2463 - acc: 0.3130\n",
      "Epoch 65/100\n",
      "118632/118632 [==============================] - 165s 1ms/step - loss: 3.2148 - acc: 0.3165\n",
      "Epoch 66/100\n",
      "118632/118632 [==============================] - 165s 1ms/step - loss: 3.1852 - acc: 0.3218\n",
      "Epoch 67/100\n",
      "118632/118632 [==============================] - 166s 1ms/step - loss: 3.1536 - acc: 0.3268\n",
      "Epoch 68/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118632/118632 [==============================] - 165s 1ms/step - loss: 3.1234 - acc: 0.3308\n",
      "Epoch 69/100\n",
      "118632/118632 [==============================] - 201s 2ms/step - loss: 3.0946 - acc: 0.3345\n",
      "Epoch 70/100\n",
      "118632/118632 [==============================] - 211s 2ms/step - loss: 3.0657 - acc: 0.3399\n",
      "Epoch 71/100\n",
      "118632/118632 [==============================] - 209s 2ms/step - loss: 3.0376 - acc: 0.3423\n",
      "Epoch 72/100\n",
      "  6016/118632 [>.............................] - ETA: 3:19 - loss: 2.8875 - acc: 0.3743"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118632/118632 [==============================] - 210s 2ms/step - loss: 2.7906 - acc: 0.3829\n",
      "Epoch 81/100\n",
      "118632/118632 [==============================] - 211s 2ms/step - loss: 2.7636 - acc: 0.3878\n",
      "Epoch 82/100\n",
      "118632/118632 [==============================] - 211s 2ms/step - loss: 2.7333 - acc: 0.3928\n",
      "Epoch 83/100\n",
      " 79104/118632 [===================>..........] - ETA: 1:10 - loss: 2.6731 - acc: 0.4050"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118632/118632 [==============================] - 211s 2ms/step - loss: 2.6380 - acc: 0.4088\n",
      "Epoch 87/100\n",
      "118632/118632 [==============================] - 211s 2ms/step - loss: 2.6132 - acc: 0.4155\n",
      "Epoch 88/100\n",
      "118632/118632 [==============================] - 211s 2ms/step - loss: 2.5901 - acc: 0.4176\n",
      "Epoch 89/100\n",
      "118632/118632 [==============================] - 211s 2ms/step - loss: 2.5662 - acc: 0.4221\n",
      "Epoch 90/100\n",
      "118632/118632 [==============================] - 211s 2ms/step - loss: 2.5413 - acc: 0.4270\n",
      "Epoch 91/100\n",
      "118632/118632 [==============================] - 212s 2ms/step - loss: 2.5174 - acc: 0.4304\n",
      "Epoch 92/100\n",
      "118632/118632 [==============================] - 210s 2ms/step - loss: 2.4957 - acc: 0.4345\n",
      "Epoch 93/100\n",
      "118632/118632 [==============================] - 211s 2ms/step - loss: 2.4753 - acc: 0.4390\n",
      "Epoch 94/100\n",
      "118632/118632 [==============================] - 211s 2ms/step - loss: 2.4503 - acc: 0.4437\n",
      "Epoch 95/100\n",
      "118632/118632 [==============================] - 211s 2ms/step - loss: 2.4248 - acc: 0.4487\n",
      "Epoch 96/100\n",
      "118632/118632 [==============================] - 210s 2ms/step - loss: 2.4074 - acc: 0.4508\n",
      "Epoch 97/100\n",
      "118632/118632 [==============================] - 211s 2ms/step - loss: 2.3879 - acc: 0.4551\n",
      "Epoch 98/100\n",
      "118632/118632 [==============================] - 211s 2ms/step - loss: 2.3625 - acc: 0.4595\n",
      "Epoch 99/100\n",
      "118632/118632 [==============================] - 210s 2ms/step - loss: 2.3401 - acc: 0.4654\n",
      "Epoch 100/100\n",
      "118632/118632 [==============================] - 211s 2ms/step - loss: 2.3214 - acc: 0.4670\n"
     ]
    }
   ],
   "source": [
    "# load\n",
    "in_filename = 'republic_sequences.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')\n",
    "\n",
    "# integer encode sequences of words\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines)\n",
    "# vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# separate into input and output\n",
    "sequences = array(sequences)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "seq_length = X.shape[1]\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())\n",
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit model\n",
    "model.fit(X, y, batch_size=128, epochs=100)\n",
    "\n",
    "# save the model to file\n",
    "model.save('model.h5')\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open('tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a battle between them and at last they agreed to distribute their land and houses among individual owners and they enslaved their friends and maintainers whom they had formerly protected in the condition of freemen and made of them subjects and servants and they themselves were engaged in war and in\n",
      "\n",
      "keeping a watch home and yet worth speaking of patroclus and slaughtered the captives at the pyre of a wellordered state and the just is not the spectator is decided that the one is expressive of what is internal and domestic the other of the soul and institutions which we\n"
     ]
    }
   ],
   "source": [
    "# Use LM\n",
    "from random import randint\n",
    "from pickle import load\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "# generate a sequence from a language model\n",
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "    result = list()\n",
    "    in_text = seed_text\n",
    "    # generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "        # encode the text as integer\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # truncate sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "        # predict probabilities for each word\n",
    "        yhat = model.predict_classes(encoded, verbose=0)\n",
    "        # map predicted word index to word\n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                out_word = word\n",
    "                break\n",
    "        # append to input\n",
    "        in_text += ' ' + out_word\n",
    "        result.append(out_word)\n",
    "    return ' '.join(result)\n",
    "\n",
    "# load cleaned text sequences\n",
    "in_filename = 'republic_sequences.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')\n",
    "seq_length = len(lines[0].split()) - 1\n",
    "\n",
    "# load the model\n",
    "model = load_model('model.h5')\n",
    "\n",
    "# load the tokenizer\n",
    "tokenizer = load(open('tokenizer.pkl', 'rb'))\n",
    "\n",
    "# select a seed text\n",
    "seed_text = lines[randint(0,len(lines))]\n",
    "print(seed_text + '\\n')\n",
    "\n",
    "# generate new text\n",
    "generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\n",
    "print(generated)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
