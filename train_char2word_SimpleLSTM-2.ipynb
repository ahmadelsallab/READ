{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Bidirectional, Concatenate, GRU, Dot, TimeDistributed, Activation, Embedding, Lambda, Concatenate, Reshape\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, LearningRateScheduler\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from keras.models import load_model\n",
    "from nltk.tokenize import word_tokenize\n",
    "from autocorrect import spell\n",
    "import re\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit gpu allocation. allow_growth, or gpu_fraction\n",
    "def gpu_alloc():\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_alloc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_WER_sent(gt, pred):\n",
    "    '''\n",
    "    calculate_WER('calculating wer between two sentences', 'calculate wer between two sentences')\n",
    "    '''\n",
    "    gt_words = gt.lower().split(' ')\n",
    "    pred_words = pred.lower().split(' ')\n",
    "    d = np.zeros(((len(gt_words) + 1), (len(pred_words) + 1)), dtype=np.uint8)\n",
    "    # d = d.reshape((len(gt_words)+1, len(pred_words)+1))\n",
    "\n",
    "    # Initializing error matrix\n",
    "    for i in range(len(gt_words) + 1):\n",
    "        for j in range(len(pred_words) + 1):\n",
    "            if i == 0:\n",
    "                d[0][j] = j\n",
    "            elif j == 0:\n",
    "                d[i][0] = i\n",
    "\n",
    "    # computation\n",
    "    for i in range(1, len(gt_words) + 1):\n",
    "        for j in range(1, len(pred_words) + 1):\n",
    "            if gt_words[i - 1] == pred_words[j - 1]:\n",
    "                d[i][j] = d[i - 1][j - 1]\n",
    "            else:\n",
    "                substitution = d[i - 1][j - 1] + 1\n",
    "                insertion = d[i][j - 1] + 1\n",
    "                deletion = d[i - 1][j] + 1\n",
    "                d[i][j] = min(substitution, insertion, deletion)\n",
    "    return d[len(gt_words)][len(pred_words)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_WER(gt, pred):\n",
    "    '''\n",
    "\n",
    "    :param gt: list of sentences of the ground truth\n",
    "    :param pred: list of sentences of the predictions\n",
    "    both lists must have the same length\n",
    "    :return: accumulated WER\n",
    "    '''\n",
    "#    assert len(gt) == len(pred)\n",
    "    WER = 0\n",
    "    nb_w = 0\n",
    "    for i in range(len(gt)):\n",
    "        #print(gt[i])\n",
    "        #print(pred[i])\n",
    "        WER += calculate_WER_sent(gt[i], pred[i])\n",
    "        nb_w += len(gt[i])\n",
    "\n",
    "    return WER / nb_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Artificial noisy spelling mistakes\n",
    "def noise_maker(sentence, threshold):\n",
    "    '''Relocate, remove, or add characters to create spelling mistakes'''\n",
    "    letters = ['a','b','c','d','e','f','g','h','i','j','k','l','m',\n",
    "           'n','o','p','q','r','s','t','u','v','w','x','y','z',]\n",
    "    noisy_sentence = []\n",
    "    i = 0\n",
    "    while i < len(sentence):\n",
    "        random = np.random.uniform(0, 1, 1)\n",
    "        # Most characters will be correct since the threshold value is high\n",
    "        if random < threshold:\n",
    "            noisy_sentence.append(sentence[i])\n",
    "        else:\n",
    "            new_random = np.random.uniform(0, 1, 1)\n",
    "            # ~33% chance characters will swap locations\n",
    "            if new_random > 0.67:\n",
    "                if i == (len(sentence) - 1):\n",
    "                    # If last character in sentence, it will not be typed\n",
    "                    continue\n",
    "                else:\n",
    "                    # if any other character, swap order with following character\n",
    "                    noisy_sentence.append(sentence[i + 1])\n",
    "                    noisy_sentence.append(sentence[i])\n",
    "                    i += 1\n",
    "            # ~33% chance an extra lower case letter will be added to the sentence\n",
    "            elif new_random < 0.33:\n",
    "                random_letter = np.random.choice(letters, 1)[0]\n",
    "                noisy_sentence.append(random_letter)\n",
    "                noisy_sentence.append(sentence[i])\n",
    "            # ~33% chance a character will not be typed\n",
    "            else:\n",
    "                pass\n",
    "        i += 1\n",
    "\n",
    "    return ''.join(noisy_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_with_gt(file_name, num_samples, max_sent_len, min_sent_len, delimiter='\\t', gt_index=1, prediction_index=0):\n",
    "    '''Load data from txt file, with each line has: <TXT><TAB><GT>. The  target to the decoder muxt have \\t as the start trigger and \\n as the stop trigger.'''\n",
    "    cnt = 0  \n",
    "    input_texts = []\n",
    "    gt_texts = []\n",
    "    target_texts = []\n",
    "    for row in open(file_name, encoding='utf8'):\n",
    "        if cnt < num_samples :\n",
    "            #print(row)\n",
    "            sents = row.split(delimiter)\n",
    "            if (len(sents) < 2):\n",
    "                continue             \n",
    "            input_text = sents[prediction_index]\n",
    "            \n",
    "            target_text = '\\t' + sents[gt_index].strip() + '\\n'\n",
    "            if len(input_text) > min_sent_len and len(input_text) < max_sent_len and len(target_text) > min_sent_len and len(target_text) < max_sent_len:\n",
    "                cnt += 1\n",
    "                \n",
    "                input_texts.append(input_text)\n",
    "                target_texts.append(target_text)\n",
    "                gt_texts.append(sents[gt_index])\n",
    "    return input_texts, target_texts, gt_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_with_noise(file_name, num_samples, noise_threshold, max_sent_len, min_sent_len):\n",
    "    '''Load data from txt file, with each line has: <TXT>. The GT is just a noisy version of TXT. The  target to the decoder muxt have \\t as the start trigger and \\n as the stop trigger.'''\n",
    "    cnt = 0  \n",
    "    input_texts = []\n",
    "    gt_texts = []\n",
    "    target_texts = []\n",
    "    while cnt < num_samples :\n",
    "        for row in open(file_name, encoding='utf8'):\n",
    "        #for row in open(file_name):\n",
    "            if cnt < num_samples :\n",
    "                sents = row.split(\"\\t\")\n",
    "                if (len(sents) < 2):\n",
    "                    continue                 \n",
    "                input_text = noise_maker(sents[1], noise_threshold)\n",
    "                input_text = input_text[:-1]\n",
    "\n",
    "                target_text = '\\t' + sents[1].strip() + '\\n'            \n",
    "                if len(input_text) > min_sent_len and len(input_text) < max_sent_len and len(target_text) > min_sent_len and len(target_text) < max_sent_len:\n",
    "                    cnt += 1\n",
    "                    input_texts.append(input_text)\n",
    "                    target_texts.append(target_text)\n",
    "                    gt_texts.append(target_text[1:-1])\n",
    "                    \n",
    "    return input_texts, target_texts, gt_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_words_vocab(all_texts):\n",
    "    '''Build vocab dictionary to victorize chars into ints'''\n",
    "    vocab_to_int = {}\n",
    "    count = 0 # Start index for any char will be 1, as 0 is masked by the Embedding/Masking layer\n",
    "    codes = ['UNK', ' ', '\\t','\\n']# Start 'UNK' at the first entry, to keep its index=0 to be masked\n",
    "    for code in codes:\n",
    "        if code not in vocab_to_int:\n",
    "            vocab_to_int[code] = count\n",
    "            count += 1    \n",
    "    \n",
    "    for sentence in all_texts:       \n",
    "        for word in word_tokenize(sentence):\n",
    "            word = process_word(word)\n",
    "            if word not in vocab_to_int:\n",
    "                vocab_to_int[word] = count\n",
    "                count += 1\n",
    "\n",
    "\n",
    "    '''''Build inverse translation from int to word'''\n",
    "    int_to_vocab = {}\n",
    "    for word, value in vocab_to_int.items():\n",
    "        int_to_vocab[value] = word\n",
    "        \n",
    "    return vocab_to_int, int_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_chars_vocab(all_texts):\n",
    "    '''Build vocab dictionary to victorize chars into ints'''\n",
    "    vocab_to_int = {}\n",
    "    count = 0 # Start index for any char will be 1, as 0 is masked by the Embedding/Masking layer\n",
    "    codes = ['UNK', ' ', '\\t','\\n']# Start 'UNK' at the first entry, to keep its index=0 to be masked\n",
    "    for code in codes:\n",
    "        if code not in vocab_to_int:\n",
    "            vocab_to_int[code] = count\n",
    "            count += 1    \n",
    "    \n",
    "    for sentence in all_texts:\n",
    "        for char in sentence:\n",
    "            if char not in vocab_to_int:\n",
    "                vocab_to_int[char] = count\n",
    "                count += 1\n",
    "\n",
    "\n",
    "    '''''Build inverse translation from int to char'''\n",
    "    int_to_vocab = {}\n",
    "    for character, value in vocab_to_int.items():\n",
    "        int_to_vocab[value] = character\n",
    "        \n",
    "    return vocab_to_int, int_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_char_data(input_texts, target_texts, max_encoder_seq_length, num_encoder_tokens, vocab_to_int):\n",
    "    '''Prepares the input text and targets into the proper seq2seq numpy arrays'''\n",
    "    encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length),\n",
    "    dtype='float32')\n",
    "    decoder_input_data = np.zeros(\n",
    "        (len(input_texts), max_decoder_seq_length),\n",
    "        dtype='float32')\n",
    "    decoder_target_data = np.zeros(\n",
    "        (len(input_texts), max_decoder_seq_length, num_encoder_tokens),\n",
    "        dtype='float32')\n",
    "\n",
    "    for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "        for t, char in enumerate(input_text):\n",
    "            # c0..cn\n",
    "            encoder_input_data[i, t] = vocab_to_int[char]\n",
    "        for t, char in enumerate(target_text):\n",
    "            # c0'..cm'\n",
    "            # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "            decoder_input_data[i, t] = vocab_to_int[char]\n",
    "            if t > 0:\n",
    "                # decoder_target_data will be ahead by one timestep\n",
    "                # and will not include the start character.\n",
    "                decoder_target_data[i, t - 1, vocab_to_int[char]] = 1.\n",
    "                \n",
    "    return encoder_input_data, decoder_input_data, decoder_target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_hier_data(input_texts, target_texts, max_words_seq_length, max_chars_seq_length, num_char_tokens, num_word_tokens, word2int, char2int):\n",
    "\n",
    "    '''Prepares the input text and targets into the proper seq2seq numpy arrays'''\n",
    "    # \n",
    "    encoder_char_input_data_lst = []\n",
    "    \n",
    "    decoder_word_input_data_lst = []\n",
    "    \n",
    "    decoder_word_target_data_lst = []\n",
    "\n",
    "    for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "        encoder_char_input_data = np.zeros((max_words_seq_length, max_chars_seq_length), dtype='float32')\n",
    "\n",
    "        decoder_word_input_data = np.zeros(max_words_seq_length, dtype='float32')\n",
    "\n",
    "        decoder_word_target_data = np.zeros((max_words_seq_length, num_word_tokens), dtype='float32')\n",
    "        words_lst = word_tokenize(input_text)\n",
    "        \n",
    "        if(len(words_lst) > max_words_seq_length):\n",
    "            continue\n",
    "        for j, word in enumerate(words_lst):\n",
    "            if(len(word) > max_chars_seq_length):\n",
    "                continue\n",
    "            for k, char in enumerate(word):\n",
    "                # c0..cn\n",
    "                if(char in char2int):\n",
    "                    encoder_char_input_data[j, k] = char2int[char]\n",
    "                    \n",
    "        words_lst = word_tokenize(target_text)# word_tokenize removes the \\t and \\n, we need them to start and end a sequence\n",
    "        words_lst.insert(0, '\\t')\n",
    "        words_lst.append('\\n')        \n",
    "        if(len(words_lst) > max_words_seq_length):\n",
    "            continue                \n",
    "        for j, word in enumerate(words_lst):\n",
    "            #processed_word = process_word(word)\n",
    "            processed_word = word\n",
    "            if not processed_word in word2int:\n",
    "                continue\n",
    "            # c0'..cm'\n",
    "            # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "            decoder_word_input_data[j] = word2int[processed_word]\n",
    "            if j > 0:\n",
    "                # decoder_target_data will be ahead by one timestep\n",
    "                # and will not include the start character.\n",
    "                decoder_word_target_data[j - 1, word2int[processed_word]] = 1.\n",
    "                \n",
    "        encoder_char_input_data_lst.append(encoder_char_input_data)\n",
    "    \n",
    "        decoder_word_input_data_lst.append(decoder_word_input_data)\n",
    "    \n",
    "        decoder_word_target_data_lst.append(decoder_word_target_data)\n",
    "        \n",
    "    return np.array(encoder_char_input_data_lst), np.array(decoder_word_input_data_lst), np.array(decoder_word_target_data_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_gt_sequence(input_seq, int_to_vocab):\n",
    "\n",
    "    decoded_sentence = ''\n",
    "    for i in range(input_seq.shape[1]):\n",
    "        \n",
    "        # Sample a token\n",
    "        sampled_token_index = input_seq[0][i]\n",
    "        sampled_word = int_to_vocab[sampled_token_index]\n",
    "        decoded_sentence += sampled_word + ' '\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq, encoder_model, decoder_model, max_words_seq_len, num_decoder_tokens, int_to_vocab):\n",
    "    # Encode the input as state vectors.\n",
    "    encoder_outputs, h, c  = encoder_model.predict(input_seq)\n",
    "    states_value = [h,c]\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, max_words_seq_len))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0] = vocab_to_int['\\t']\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    #print(input_seq)\n",
    "    attention_density = []\n",
    "\n",
    "    special_chars = ['\\\\', '/', '-', '—' , ':', '[', ']', ',', '.', '\"', ';', '%', '~', '(', ')', '{', '}', '$']\n",
    "    i = 0\n",
    "    while not stop_condition:\n",
    "        #print(target_seq)\n",
    "        output_tokens, attention, h, c  = decoder_model.predict(\n",
    "            [target_seq, encoder_outputs] + states_value)\n",
    "        #print(attention.shape)\n",
    "        attention_density.append(attention[0][0])# attention is max_sent_len x 1 since we have num_time_steps = 1 for the output\n",
    "        # Sample a token\n",
    "        #print(output_tokens.shape)\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        \n",
    "        #print(sampled_token_index)\n",
    "        sampled_char = int_to_vocab[sampled_token_index]\n",
    "       \n",
    "        \n",
    "        #orig_char = int_to_vocab[int(input_seq[:,i][0])]\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(word_tokenize(decoded_sentence)) > max_words_seq_len):\n",
    "            stop_condition = True\n",
    "            sampled_char = ''\n",
    "\n",
    "        # Copy digits as it, since the spelling corrector is not good at digit corrections\n",
    "        '''\n",
    "        if(orig_char.isdigit() or orig_char in special_chars):\n",
    "            decoded_sentence += orig_char            \n",
    "        else:\n",
    "            if(sampled_char.isdigit() or sampled_char in special_chars):\n",
    "                decoded_sentence += ''\n",
    "            else:\n",
    "                decoded_sentence += sampled_char\n",
    "        '''\n",
    "        decoded_sentence += sampled_char + ' '\n",
    "\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        #target_seq = np.zeros((1, max_words_seq_len))\n",
    "        if i < max_words_seq_len:\n",
    "            target_seq[0, i] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "        i += 1\n",
    "        #if i > 48:\n",
    "        #    i = 0\n",
    "        \n",
    "\n",
    "    attention_density = np.array(attention_density)\n",
    "    return decoded_sentence, attention_density\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_char_sequence(input_seq, encoder_model, decoder_model, num_decoder_tokens, int_to_vocab):\n",
    "    # Encode the input as state vectors.\n",
    "    encoder_outputs, h, c  = encoder_model.predict(input_seq)\n",
    "    states_value = [h,c]\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0] = vocab_to_int['\\t']\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    #print(input_seq)\n",
    "    attention_density = []\n",
    "    i = 0\n",
    "    special_chars = ['\\\\', '/', '-', '—' , ':', '[', ']', ',', '.', '\"', ';', '%', '~', '(', ')', '{', '}', '$']\n",
    "    while not stop_condition:\n",
    "        #print(target_seq)\n",
    "        output_tokens, attention, h, c  = decoder_model.predict(\n",
    "            [target_seq, encoder_outputs] + states_value)\n",
    "        #print(attention.shape)\n",
    "        attention_density.append(attention[0][0])# attention is max_sent_len x 1 since we have num_time_steps = 1 for the output\n",
    "        # Sample a token\n",
    "        #print(output_tokens.shape)\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        \n",
    "        #print(sampled_token_index)\n",
    "        sampled_char = int_to_vocab[sampled_token_index]\n",
    "        orig_char = int_to_vocab[int(input_seq[:,i][0])]\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "            sampled_char = ''\n",
    "\n",
    "        # Copy digits as it, since the spelling corrector is not good at digit corrections\n",
    "        if(orig_char.isdigit() or orig_char in special_chars):\n",
    "            decoded_sentence += orig_char            \n",
    "        else:\n",
    "            if(sampled_char.isdigit() or sampled_char in special_chars):\n",
    "                decoded_sentence += ''\n",
    "            else:\n",
    "                decoded_sentence += sampled_char\n",
    "        \n",
    "\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "        \n",
    "        i += 1\n",
    "        if(i > 48):\n",
    "            i = 0\n",
    "    attention_density = np.array(attention_density)\n",
    "    return decoded_sentence, attention_density\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_char_model(num_encoder_tokens, latent_dim):\n",
    "    # Define an input sequence and process it.\n",
    "    encoder_inputs = Input(shape=(None,), dtype='float32')\n",
    "    encoder_inputs_ = Embedding(num_encoder_tokens, num_encoder_tokens,                           \n",
    "                            weights=[np.eye(num_encoder_tokens)],\n",
    "                            mask_zero=True, trainable=False)(encoder_inputs)    \n",
    "    #encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "    encoder = Bidirectional(LSTM(latent_dim, return_state=True, return_sequences=True)) # Bi LSTM\n",
    "    encoder_outputs, state_f_h, state_f_c, state_b_h, state_b_c = encoder(encoder_inputs_)# Bi LSTM\n",
    "    state_h = Concatenate()([state_f_h, state_b_h])# Bi LSTM\n",
    "    state_c = Concatenate()([state_f_c, state_b_c])# Bi LSTM\n",
    "\n",
    "    # We discard `encoder_outputs` and only keep the states.\n",
    "    encoder_states = [state_h, state_c]# Bi GRU, LSTM, BHi LSTM\n",
    "    #print(encoder_states)\n",
    "    \n",
    "    decoder_inputs = Input(shape=(None,))\n",
    "    decoder_inputs_ = Embedding(num_encoder_tokens, num_encoder_tokens,                           \n",
    "                            weights=[np.eye(num_encoder_tokens)],\n",
    "                            mask_zero=True, trainable=False)(decoder_inputs)    \n",
    "    # We set up our decoder to return full output sequences,\n",
    "    # and to return internal states as well. We don't use the\n",
    "    # return states in the training model, but we will use them in inference.\n",
    "    decoder_lstm = LSTM(latent_dim*2, return_sequences=True, return_state=True)# Bi LSTM\n",
    "    \n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs_, initial_state=encoder_states)\n",
    "\n",
    "    # Equation (7) with 'dot' score from Section 3.1 in the paper.\n",
    "    # Note that we reuse Softmax-activation layer instead of writing tensor calculation\n",
    "    #print(decoder_outputs)\n",
    "    #print(encoder_outputs)\n",
    "    att_dot = Dot(axes=[2, 2])\n",
    "    attention = att_dot([decoder_outputs, encoder_outputs])\n",
    "    att_activation = Activation('softmax', name='attention')\n",
    "    attention = att_activation(attention)\n",
    "    #print('attention', attention)\n",
    "    context_dot = Dot(axes=[2,1])\n",
    "    context = context_dot([attention, encoder_outputs])\n",
    "    #print('context', context)\n",
    "    att_context_concat = Concatenate()\n",
    "    decoder_combined_context = att_context_concat([context, decoder_outputs])\n",
    "    #print('decoder_combined_context', decoder_combined_context)\n",
    "\n",
    "    # Has another weight + tanh layer as described in equation (5) of the paper\n",
    "    #decoder_outputs = TimeDistributed(Dense(64, activation=\"tanh\"))(decoder_combined_context)\n",
    "    #decoder_outputs = TimeDistributed(Dense(num_encoder_tokens, activation=\"softmax\"))(decoder_outputs)\n",
    "\n",
    "    decoder_dense = Dense(num_encoder_tokens, activation='softmax')\n",
    "    #decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_outputs = decoder_dense(decoder_combined_context)\n",
    "\n",
    "    # Define the model that will turn\n",
    "    # `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "    #model = Model(decoder_inputs, decoder_outputs)\n",
    "    #print('encoder-decoder  model:')\n",
    "    print(model.summary()) \n",
    "    \n",
    "    #print(encoder_inputs)\n",
    "    #print(encoder_outputs)\n",
    "    #print(encoder_states)\n",
    "    #encoder_model = Model(encoder_inputs, [encoder_outputs, encoder_states])\n",
    "    encoder_model = Model(input=encoder_inputs, output=[encoder_outputs] + encoder_states)\n",
    "    print(encoder_outputs.shape)\n",
    "    encoder_embedding_output = Lambda(lambda x: x[:,-1,:])(encoder_outputs)\n",
    "    encoder_word_embedding_model = Model(input=encoder_inputs, output=encoder_embedding_output)\n",
    "\n",
    "    #decoder_state_input_h = Input(shape=(latent_dim,))# LSTM\n",
    "    #decoder_state_input_c = Input(shape=(latent_dim,))# LSTM\n",
    "    decoder_encoder_inputs = Input(shape=(None, latent_dim*2,))\n",
    "    decoder_state_input_h = Input(shape=(latent_dim*2,))# Bi LSTM\n",
    "    decoder_state_input_c = Input(shape=(latent_dim*2,)) # Bi LSTM\n",
    "    #decoder_state_input = Input(shape=(latent_dim*2,)) # Bi GRU\n",
    "\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    #decoder_states_inputs = [decoder_state_input] # Bi GRU\n",
    "\n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs_, initial_state=decoder_states_inputs)\n",
    "\n",
    "    #decoder_outputs, state = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "\n",
    "    decoder_states = [state_h, state_c]\n",
    "    \n",
    "    # Equation (7) with 'dot' score from Section 3.1 in the paper.\n",
    "    # Note that we reuse Softmax-activation layer instead of writing tensor calculation\n",
    "    \n",
    "    attention = att_dot([decoder_outputs, decoder_encoder_inputs])\n",
    "    \n",
    "    attention = att_activation(attention)\n",
    "    #print('attention', attention)\n",
    "    \n",
    "    context = context_dot([attention, decoder_encoder_inputs])\n",
    "    #print('context', context)\n",
    "    \n",
    "    \n",
    "    \n",
    "    decoder_combined_context = att_context_concat([context, decoder_outputs])\n",
    "    #print('decoder_combined_context', decoder_combined_context)\n",
    "\n",
    "    # Has another weight + tanh layer as described in equation (5) of the paper\n",
    "    #decoder_outputs = TimeDistributed(Dense(64, activation=\"tanh\"))(decoder_combined_context)\n",
    "    #decoder_outputs = TimeDistributed(Dense(num_encoder_tokens, activation=\"softmax\"))(decoder_outputs)\n",
    "    \n",
    "    #decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "    #decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_outputs = decoder_dense(decoder_combined_context)\n",
    "    \n",
    "    decoder_model = Model(\n",
    "        [decoder_inputs, decoder_encoder_inputs] + decoder_states_inputs,\n",
    "        [decoder_outputs, attention] + decoder_states)\n",
    "    \n",
    "    return model, encoder_model, decoder_model, encoder_word_embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_hier_model(encoder_word_embedding_model, max_words_seq_len, max_char_seq_len, num_word_tokens, num_char_tokens, latent_dim):\n",
    "    # Define an input sequence and process it.\n",
    "\n",
    "    inputs = Input(shape=(max_words_seq_len, max_char_seq_len,), dtype='float32')\n",
    "    decoder_inputs_words = Input(shape=(max_words_seq_len,), dtype='float32')\n",
    "    words_states = []\n",
    "    '''\n",
    "    for w in range(max_words_seq_len):\n",
    "        \n",
    "        encoder_char_inputs = Lambda(lambda x: x[:,w,:])(inputs)\n",
    "        _, h, c = encoder_char_model(encoder_char_inputs)\n",
    "        encoder_chars_states = Concatenate()([h,c])\n",
    "        #print(encoder_chars_states)\n",
    "        encoder_chars_states = Reshape((1,latent_dim*4))(encoder_chars_states)\n",
    "        words_states.append(encoder_chars_states)\n",
    "    \n",
    "    input_words = Concatenate(axis=-2)(words_states)\n",
    "\n",
    "    '''\n",
    "    #input_words = TimeDistributed(Dense(10))(inputs)\n",
    "\n",
    "    input_words = TimeDistributed(encoder_word_embedding_model)(inputs)\n",
    "\n",
    "    encoder_inputs_ = input_words   \n",
    "    #encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "    encoder = Bidirectional(LSTM(latent_dim, return_state=True, return_sequences=True)) # Bi LSTM\n",
    "    encoder_outputs, state_f_h, state_f_c, state_b_h, state_b_c = encoder(encoder_inputs_)# Bi LSTM\n",
    "    state_h = Concatenate()([state_f_h, state_b_h])# Bi LSTM\n",
    "    state_c = Concatenate()([state_f_c, state_b_c])# Bi LSTM\n",
    "\n",
    "    # We discard `encoder_outputs` and only keep the states.\n",
    "    encoder_states = [state_h, state_c]# Bi GRU, LSTM, BHi LSTM\n",
    "    \n",
    "    decoder_inputs = decoder_inputs_words\n",
    "    decoder_inputs_ = Embedding(num_word_tokens, latent_dim*4,                           \n",
    "                            #weights=[np.eye(num_word_tokens)],\n",
    "                            mask_zero=True, trainable=True)(decoder_inputs)    \n",
    "    # We set up our decoder to return full output sequences,\n",
    "    # and to return internal states as well. We don't use the\n",
    "    # return states in the training model, but we will use them in inference.\n",
    "    decoder_lstm = LSTM(latent_dim*2, return_sequences=True, return_state=True)# Bi LSTM\n",
    "    \n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs_, initial_state=encoder_states)\n",
    "\n",
    "    att_dot = Dot(axes=[2, 2])\n",
    "    attention = att_dot([decoder_outputs, encoder_outputs])\n",
    "    att_activation = Activation('softmax')\n",
    "    attention = att_activation(attention)\n",
    "    #print('attention', attention)\n",
    "    context_dot = Dot(axes=[2,1])\n",
    "    context = context_dot([attention, encoder_outputs])\n",
    "    #print('context', context)\n",
    "    att_context_concat = Concatenate()\n",
    "    decoder_combined_context = att_context_concat([context, decoder_outputs])\n",
    "    #print('decoder_combined_context', decoder_combined_context)\n",
    "\n",
    "    # Has another weight + tanh layer as described in equation (5) of the paper\n",
    "    #decoder_outputs = TimeDistributed(Dense(64, activation=\"tanh\"))(decoder_combined_context)\n",
    "    #decoder_outputs = TimeDistributed(Dense(num_encoder_tokens, activation=\"softmax\"))(decoder_outputs)\n",
    "\n",
    "    decoder_dense = Dense(num_word_tokens, activation='softmax')\n",
    "    #decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_outputs = decoder_dense(decoder_combined_context)\n",
    "\n",
    "    # Define the model that will turn\n",
    "    # `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "    model = Model([inputs, decoder_inputs_words], decoder_outputs)\n",
    "    #model = Model(decoder_inputs, decoder_outputs)\n",
    "    print('encoder-decoder  model:')\n",
    "    print(model.summary()) \n",
    "    \n",
    "    #encoder_model = Model(encoder_inputs, [encoder_outputs, encoder_states])\n",
    "    encoder_model = Model(input=inputs, output=[encoder_outputs] + encoder_states)\n",
    "\n",
    "    #decoder_state_input_h = Input(shape=(latent_dim,))# LSTM\n",
    "    #decoder_state_input_c = Input(shape=(latent_dim,))# LSTM\n",
    "    decoder_encoder_inputs = Input(shape=(max_words_seq_len, latent_dim*2,))\n",
    "    decoder_state_input_h = Input(shape=(latent_dim*2,))# Bi LSTM\n",
    "    decoder_state_input_c = Input(shape=(latent_dim*2,)) # Bi LSTM\n",
    "    #decoder_state_input = Input(shape=(latent_dim*2,)) # Bi GRU\n",
    "\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    #decoder_states_inputs = [decoder_state_input] # Bi GRU\n",
    "\n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs_, initial_state=decoder_states_inputs)\n",
    "\n",
    "    #decoder_outputs, state = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "\n",
    "    decoder_states = [state_h, state_c]\n",
    "    \n",
    "    # Equation (7) with 'dot' score from Section 3.1 in the paper.\n",
    "    # Note that we reuse Softmax-activation layer instead of writing tensor calculation\n",
    "    \n",
    "    attention = att_dot([decoder_outputs, decoder_encoder_inputs])\n",
    "    \n",
    "    attention = att_activation(attention)\n",
    "    #print('attention', attention)\n",
    "    \n",
    "    context = context_dot([attention, decoder_encoder_inputs])\n",
    "    #print('context', context)\n",
    "    \n",
    "    \n",
    "    \n",
    "    decoder_combined_context = att_context_concat([context, decoder_outputs])\n",
    "    #print('decoder_combined_context', decoder_combined_context)\n",
    "\n",
    "    # Has another weight + tanh layer as described in equation (5) of the paper\n",
    "    #decoder_outputs = TimeDistributed(Dense(64, activation=\"tanh\"))(decoder_combined_context)\n",
    "    #decoder_outputs = TimeDistributed(Dense(num_encoder_tokens, activation=\"softmax\"))(decoder_outputs)\n",
    "    \n",
    "    #decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "    #decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_outputs = decoder_dense(decoder_combined_context)\n",
    "    \n",
    "    decoder_model = Model(\n",
    "        [decoder_inputs_words, decoder_encoder_inputs] + decoder_states_inputs,\n",
    "        [decoder_outputs, attention] + decoder_states)\n",
    "    \n",
    "    return model, encoder_model, decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def visualize_attention(text, encoder_model, decoder_model, max_encoder_seq_length, num_decoder_tokens, vocab_to_int, int_to_vocab):\n",
    "\n",
    "    encoder_input_data = np.zeros((1, max_encoder_seq_length), dtype='float32')\n",
    "    \n",
    "    for t, char in enumerate(text):\n",
    "        # c0..cn\n",
    "        encoder_input_data[0, t] = vocab_to_int[char]\n",
    "\n",
    "    input_seq = encoder_input_data[0:1]\n",
    "\n",
    "    decoded_sentence, attention_density = decode_sequence(input_seq, encoder_model, decoder_model, num_decoder_tokens, int_to_vocab)\n",
    "\n",
    "    plt.clf()\n",
    "    plt.figure(figsize=(28,12))\n",
    "    \n",
    "    ax = sns.heatmap(attention_density[:, : len(text) + 2],\n",
    "        xticklabels=[w for w in text],\n",
    "        yticklabels=[w for w in decoded_sentence])\n",
    "\n",
    "    ax.invert_yaxis()\n",
    "    plt.show()\n",
    "    \n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_word(word):\n",
    "    # Try to correct the word from known dict\n",
    "    #word = spell(word)\n",
    "    # Option 1: Replace special chars and digits\n",
    "    #processed_word = re.sub(r'[\\\\\\/\\-\\—\\:\\[\\]\\,\\.\\\"\\;\\%\\~\\(\\)\\{\\}\\$\\#\\?\\●\\@\\+\\-\\*\\d]', r'', w.lower())\n",
    "    \n",
    "    # Option 2: skip all words with special chars or digits\n",
    "    if(len(re.findall(r'[\\\\\\/\\-\\—\\:\\[\\]\\,\\.\\\"\\;\\%\\~\\(\\)\\{\\}\\$\\#\\?\\●\\@\\+\\-\\*\\d]', word.lower())) == 0):\n",
    "        #processed_word = word.lower()\n",
    "        processed_word = word\n",
    "    else:\n",
    "        processed_word = 'UNK'\n",
    "\n",
    "    # Skip stop words\n",
    "    #stop_words = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]        \n",
    "    stop_words = []\n",
    "    if processed_word in stop_words:\n",
    "        processed_word = 'UNK'\n",
    "        \n",
    "    return processed_word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-train char model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../dat/'\n",
    "max_sent_len = 50\n",
    "min_sent_len = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_ocr_data_2.txt\n",
      "field_class_21.txt\n",
      "field_class_22.txt\n",
      "field_class_23.txt\n",
      "field_class_24.txt\n",
      "field_class_25.txt\n",
      "field_class_26.txt\n",
      "field_class_27.txt\n",
      "field_class_28.txt\n",
      "field_class_29.txt\n",
      "field_class_30.txt\n",
      "field_class_31.txt\n",
      "field_class_32.txt\n",
      "field_class_33.txt\n",
      "field_class_34.txt\n",
      "NL-14622714.txt\n",
      "NL-14627449.txt\n",
      "NL-14628986.txt\n",
      "NL-14631911.txt\n",
      "NL-14640007.txt\n"
     ]
    }
   ],
   "source": [
    "num_samples = 1000000\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "#files_list = ['all_ocr_data_2.txt', 'field_class_21.txt', 'field_class_32.txt', 'field_class_30.txt']\n",
    "files_list = ['all_ocr_data_2.txt', 'field_class_21.txt', 'field_class_22.txt', 'field_class_23.txt', 'field_class_24.txt', 'field_class_25.txt', 'field_class_26.txt', 'field_class_27.txt', 'field_class_28.txt', 'field_class_29.txt', 'field_class_30.txt', 'field_class_31.txt', 'field_class_32.txt', 'field_class_33.txt', 'field_class_34.txt', 'NL-14622714.txt', 'NL-14627449.txt', 'NL-14628986.txt', 'NL-14631911.txt', 'NL-14640007.txt']\n",
    "#desired_file_sizes = [num_samples, num_samples, num_samples, num_samples]\n",
    "desired_file_sizes = []\n",
    "for i in range(len(files_list)):\n",
    "    desired_file_sizes.append(num_samples)\n",
    "noise_threshold = 0.9\n",
    "\n",
    "for file_name, num_file_samples in zip(files_list, desired_file_sizes):\n",
    "    print(file_name)\n",
    "    tess_correction_data = os.path.join(data_path, file_name)\n",
    "    input_texts_OCR, target_texts_OCR, gt_OCR = load_data_with_gt(tess_correction_data, num_file_samples, max_sent_len, min_sent_len, delimiter='\\t', gt_index=1, prediction_index=0)\n",
    "\n",
    "    input_texts += input_texts_OCR\n",
    "    target_texts += target_texts_OCR\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9150"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9150\n",
      "Claim Type: VB Accident - Accidental Injury \n",
      " \tClaim Type: VB Accident - Accidental Injury\n",
      "\n",
      "Pol inyhold elm-Chm er [11 form arlon \n",
      " \tPolicyholder/Owner Information\n",
      "\n",
      "First Name: \n",
      " \tFirst Name:\n",
      "\n",
      "Middle Nameﬂnitial: \n",
      " \tMiddle Name/Initial:\n",
      "\n",
      "Last Name: \n",
      " \tLast Name:\n",
      "\n",
      "Social S ecurity Number: \n",
      " \tSocial Security Number:\n",
      "\n",
      "Birth Date: \n",
      " \tBirth Date:\n",
      "\n",
      "Gender: \n",
      " \tGender:\n",
      "\n",
      "Language Preference: \n",
      " \tLanguage Preference:\n",
      "\n",
      "Address Line 1: \n",
      " \tAddress Line 1:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample data\n",
    "print(len(input_texts))\n",
    "for i in range(10):\n",
    "    print(input_texts[i], '\\n', target_texts[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram of lenghts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chars_per_words_lengths = []\n",
    "words_per_sents_lengths = []\n",
    "\n",
    "# Chars per word should be on all text\n",
    "for text in (input_texts_OCR+target_texts_OCR):\n",
    "    words = word_tokenize(text)\n",
    "    #words_per_sents_lengths.append(len(words))\n",
    "    for word in words:\n",
    "        chars_per_words_lengths.append(len(word))\n",
    "\n",
    "# Words in sent should be on target only        \n",
    "for text in target_texts_OCR:\n",
    "    words = word_tokenize(text)\n",
    "    words_per_sents_lengths.append(len(words))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7768cf17b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "h_w = plt.hist(words_per_sents_lengths, bins=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7768bd5f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "h_c = plt.hist(chars_per_words_lengths, bins=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build char vocab (all text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts = target_texts + input_texts\n",
    "vocab_to_int, int_to_vocab = build_chars_vocab(all_texts)\n",
    "np.savez('vocab_char-{}'.format(max_sent_len), vocab_to_int=vocab_to_int, int_to_vocab=int_to_vocab, max_sent_len=max_sent_len, min_sent_len=min_sent_len )\n",
    "char2int = vocab_to_int\n",
    "int2char = int_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_characters = sorted(list(vocab_to_int))\n",
    "target_characters = sorted(list(vocab_to_int))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 9150\n",
      "Number of unique input tokens: 126\n",
      "Number of unique output tokens: 126\n",
      "Max sequence length for inputs: 49\n",
      "Max sequence length for outputs: 49\n"
     ]
    }
   ],
   "source": [
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\t': 2,\n",
       " '\\n': 3,\n",
       " '\\x0c': 120,\n",
       " ' ': 1,\n",
       " '!': 111,\n",
       " '\"': 95,\n",
       " '#': 67,\n",
       " '$': 79,\n",
       " '%': 85,\n",
       " '&': 73,\n",
       " \"'\": 83,\n",
       " '(': 61,\n",
       " ')': 62,\n",
       " '*': 77,\n",
       " '+': 76,\n",
       " ',': 69,\n",
       " '-': 21,\n",
       " '.': 54,\n",
       " '/': 29,\n",
       " '0': 63,\n",
       " '1': 43,\n",
       " '2': 53,\n",
       " '3': 52,\n",
       " '4': 66,\n",
       " '5': 74,\n",
       " '6': 65,\n",
       " '7': 70,\n",
       " '8': 58,\n",
       " '9': 72,\n",
       " ':': 13,\n",
       " ';': 75,\n",
       " '<': 105,\n",
       " '=': 94,\n",
       " '>': 106,\n",
       " '?': 56,\n",
       " '@': 81,\n",
       " 'A': 16,\n",
       " 'B': 15,\n",
       " 'C': 4,\n",
       " 'D': 40,\n",
       " 'E': 46,\n",
       " 'F': 33,\n",
       " 'G': 41,\n",
       " 'H': 57,\n",
       " 'I': 22,\n",
       " 'J': 68,\n",
       " 'K': 49,\n",
       " 'L': 37,\n",
       " 'M': 36,\n",
       " 'N': 35,\n",
       " 'O': 30,\n",
       " 'P': 26,\n",
       " 'Q': 78,\n",
       " 'R': 45,\n",
       " 'S': 38,\n",
       " 'T': 9,\n",
       " 'U': 48,\n",
       " 'UNK': 0,\n",
       " 'V': 14,\n",
       " 'W': 50,\n",
       " 'X': 80,\n",
       " 'Y': 47,\n",
       " 'Z': 71,\n",
       " '[': 91,\n",
       " '\\\\': 97,\n",
       " ']': 92,\n",
       " '^': 86,\n",
       " '_': 102,\n",
       " 'a': 6,\n",
       " 'b': 39,\n",
       " 'c': 17,\n",
       " 'd': 18,\n",
       " 'e': 12,\n",
       " 'f': 32,\n",
       " 'g': 42,\n",
       " 'h': 28,\n",
       " 'i': 7,\n",
       " 'j': 23,\n",
       " 'k': 55,\n",
       " 'l': 5,\n",
       " 'm': 8,\n",
       " 'n': 19,\n",
       " 'o': 27,\n",
       " 'p': 11,\n",
       " 'q': 51,\n",
       " 'r': 25,\n",
       " 's': 34,\n",
       " 't': 20,\n",
       " 'u': 24,\n",
       " 'v': 44,\n",
       " 'w': 31,\n",
       " 'x': 59,\n",
       " 'y': 10,\n",
       " 'z': 60,\n",
       " '{': 113,\n",
       " '|': 82,\n",
       " '}': 104,\n",
       " '~': 110,\n",
       " '¢': 121,\n",
       " '£': 118,\n",
       " '¥': 125,\n",
       " '§': 114,\n",
       " '©': 122,\n",
       " '«': 116,\n",
       " '®': 119,\n",
       " '°': 90,\n",
       " '±': 124,\n",
       " '»': 115,\n",
       " 'é': 112,\n",
       " '–': 93,\n",
       " '—': 100,\n",
       " '‘': 109,\n",
       " '’': 64,\n",
       " '“': 107,\n",
       " '”': 89,\n",
       " '•': 84,\n",
       " '€': 117,\n",
       " '■': 123,\n",
       " '●': 87,\n",
       " '◦': 103,\n",
       " '☐': 98,\n",
       " '☑': 101,\n",
       " '☒': 99,\n",
       " '✓': 96,\n",
       " 'ﬁ': 88,\n",
       " 'ﬂ': 108}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char2int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'UNK',\n",
       " 1: ' ',\n",
       " 2: '\\t',\n",
       " 3: '\\n',\n",
       " 4: 'C',\n",
       " 5: 'l',\n",
       " 6: 'a',\n",
       " 7: 'i',\n",
       " 8: 'm',\n",
       " 9: 'T',\n",
       " 10: 'y',\n",
       " 11: 'p',\n",
       " 12: 'e',\n",
       " 13: ':',\n",
       " 14: 'V',\n",
       " 15: 'B',\n",
       " 16: 'A',\n",
       " 17: 'c',\n",
       " 18: 'd',\n",
       " 19: 'n',\n",
       " 20: 't',\n",
       " 21: '-',\n",
       " 22: 'I',\n",
       " 23: 'j',\n",
       " 24: 'u',\n",
       " 25: 'r',\n",
       " 26: 'P',\n",
       " 27: 'o',\n",
       " 28: 'h',\n",
       " 29: '/',\n",
       " 30: 'O',\n",
       " 31: 'w',\n",
       " 32: 'f',\n",
       " 33: 'F',\n",
       " 34: 's',\n",
       " 35: 'N',\n",
       " 36: 'M',\n",
       " 37: 'L',\n",
       " 38: 'S',\n",
       " 39: 'b',\n",
       " 40: 'D',\n",
       " 41: 'G',\n",
       " 42: 'g',\n",
       " 43: '1',\n",
       " 44: 'v',\n",
       " 45: 'R',\n",
       " 46: 'E',\n",
       " 47: 'Y',\n",
       " 48: 'U',\n",
       " 49: 'K',\n",
       " 50: 'W',\n",
       " 51: 'q',\n",
       " 52: '3',\n",
       " 53: '2',\n",
       " 54: '.',\n",
       " 55: 'k',\n",
       " 56: '?',\n",
       " 57: 'H',\n",
       " 58: '8',\n",
       " 59: 'x',\n",
       " 60: 'z',\n",
       " 61: '(',\n",
       " 62: ')',\n",
       " 63: '0',\n",
       " 64: '’',\n",
       " 65: '6',\n",
       " 66: '4',\n",
       " 67: '#',\n",
       " 68: 'J',\n",
       " 69: ',',\n",
       " 70: '7',\n",
       " 71: 'Z',\n",
       " 72: '9',\n",
       " 73: '&',\n",
       " 74: '5',\n",
       " 75: ';',\n",
       " 76: '+',\n",
       " 77: '*',\n",
       " 78: 'Q',\n",
       " 79: '$',\n",
       " 80: 'X',\n",
       " 81: '@',\n",
       " 82: '|',\n",
       " 83: \"'\",\n",
       " 84: '•',\n",
       " 85: '%',\n",
       " 86: '^',\n",
       " 87: '●',\n",
       " 88: 'ﬁ',\n",
       " 89: '”',\n",
       " 90: '°',\n",
       " 91: '[',\n",
       " 92: ']',\n",
       " 93: '–',\n",
       " 94: '=',\n",
       " 95: '\"',\n",
       " 96: '✓',\n",
       " 97: '\\\\',\n",
       " 98: '☐',\n",
       " 99: '☒',\n",
       " 100: '—',\n",
       " 101: '☑',\n",
       " 102: '_',\n",
       " 103: '◦',\n",
       " 104: '}',\n",
       " 105: '<',\n",
       " 106: '>',\n",
       " 107: '“',\n",
       " 108: 'ﬂ',\n",
       " 109: '‘',\n",
       " 110: '~',\n",
       " 111: '!',\n",
       " 112: 'é',\n",
       " 113: '{',\n",
       " 114: '§',\n",
       " 115: '»',\n",
       " 116: '«',\n",
       " 117: '€',\n",
       " 118: '£',\n",
       " 119: '®',\n",
       " 120: '\\x0c',\n",
       " 121: '¢',\n",
       " 122: '©',\n",
       " 123: '■',\n",
       " 124: '±',\n",
       " 125: '¥'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int2char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(int_to_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build char model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 512 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 126)    15876       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) [(None, None, 1024), 2617344     embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, None, 126)    15876       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1024)         0           bidirectional_1[0][1]            \n",
      "                                                                 bidirectional_1[0][3]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 1024)         0           bidirectional_1[0][2]            \n",
      "                                                                 bidirectional_1[0][4]            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, None, 1024), 4714496     embedding_2[0][0]                \n",
      "                                                                 concatenate_1[0][0]              \n",
      "                                                                 concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, None, None)   0           lstm_2[0][0]                     \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention (Activation)          (None, None, None)   0           dot_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dot_2 (Dot)                     (None, None, 1024)   0           attention[0][0]                  \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, None, 2048)   0           dot_2[0][0]                      \n",
      "                                                                 lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 126)    258174      concatenate_3[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 7,621,766\n",
      "Trainable params: 7,590,014\n",
      "Non-trainable params: 31,752\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "(?, ?, 1024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:63: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=[<tf.Tenso...)`\n",
      "/opt/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:66: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"la...)`\n"
     ]
    }
   ],
   "source": [
    "model, encoder_model, decoder_model, encoder_word_embedding_model = build_char_model(latent_dim=latent_dim, num_encoder_tokens=num_encoder_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, None)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, None, 126)         15876     \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection [(None, None, 1024), (Non 2617344   \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 1024)              0         \n",
      "=================================================================\n",
      "Total params: 2,633,220\n",
      "Trainable params: 2,617,344\n",
      "Non-trainable params: 15,876\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_word_embedding_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Hierarichal model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build word vocab (target_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "max_words_seq_len=np.max(words_per_sents_lengths)\n",
    "max_chars_seq_len=np.max(chars_per_words_lengths)\n",
    "print(max_words_seq_len)\n",
    "print(max_chars_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build output vocab on correct words only\n",
    "all_texts = target_texts\n",
    "vocab_to_int, int_to_vocab = build_words_vocab(all_texts)\n",
    "word2int = vocab_to_int\n",
    "int2word = int_to_vocab\n",
    "np.savez('vocab_hier-{}-{}'.format(max_words_seq_len, max_chars_seq_len), char2int=char2int, int2char=int2char, word2int=word2int, int2word=int2word, max_words_seq_len=max_words_seq_len, max_char_seq_len=max_chars_seq_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'UNK': 0,\n",
       " ' ': 1,\n",
       " '\\t': 2,\n",
       " '\\n': 3,\n",
       " 'Claim': 4,\n",
       " 'Type': 5,\n",
       " 'VB': 6,\n",
       " 'Accident': 7,\n",
       " 'Accidental': 8,\n",
       " 'Injury': 9,\n",
       " 'Information': 10,\n",
       " 'First': 11,\n",
       " 'Name': 12,\n",
       " 'Middle': 13,\n",
       " 'Last': 14,\n",
       " 'Social': 15,\n",
       " 'Security': 16,\n",
       " 'Number': 17,\n",
       " 'Birth': 18,\n",
       " 'Date': 19,\n",
       " 'Gender': 20,\n",
       " 'Language': 21,\n",
       " 'Preference': 22,\n",
       " 'Address': 23,\n",
       " 'Line': 24,\n",
       " 'City': 25,\n",
       " 'Postal': 26,\n",
       " 'Code': 27,\n",
       " 'Country': 28,\n",
       " 'Best': 29,\n",
       " 'Phone': 30,\n",
       " 'to': 31,\n",
       " 'be': 32,\n",
       " 'Reached': 33,\n",
       " 'During': 34,\n",
       " 'the': 35,\n",
       " 'Day': 36,\n",
       " 'Email': 37,\n",
       " 'RADIOLOGY': 38,\n",
       " 'REPORT': 39,\n",
       " 'UNKNOWN': 40,\n",
       " 'Technique': 41,\n",
       " 'views': 42,\n",
       " 'left': 43,\n",
       " 'wrist': 44,\n",
       " 'FINDINGS': 45,\n",
       " 'IMPRESSION': 46,\n",
       " 'No': 47,\n",
       " 'acute': 48,\n",
       " 'osseous': 49,\n",
       " 'abnormality': 50,\n",
       " 'identified': 51,\n",
       " 'Daytime': 52,\n",
       " 'Event': 53,\n",
       " 'Stopped': 54,\n",
       " 'Working': 55,\n",
       " 'Yes': 56,\n",
       " 'Physically': 57,\n",
       " 'at': 58,\n",
       " 'Work': 59,\n",
       " 'Hours': 60,\n",
       " 'Worked': 61,\n",
       " 'on': 62,\n",
       " 'Scheduled': 63,\n",
       " 'Missed': 64,\n",
       " 'Returned': 65,\n",
       " 'Related': 66,\n",
       " 'Time': 67,\n",
       " 'of': 68,\n",
       " 'Diagnosis': 69,\n",
       " 'Arthiscopic': 70,\n",
       " 'surgery': 71,\n",
       " 'Surgery': 72,\n",
       " 'Is': 73,\n",
       " 'Required': 74,\n",
       " 'Indicator': 75,\n",
       " 'Outpatient': 76,\n",
       " 'Medical': 77,\n",
       " 'Provider': 78,\n",
       " 'Physician': 79,\n",
       " 'Roles': 80,\n",
       " 'Treating': 81,\n",
       " 'Patrick': 82,\n",
       " 'Emerson': 83,\n",
       " 'Business': 84,\n",
       " 'Telephone': 85,\n",
       " 'Fax': 86,\n",
       " 'Visit': 87,\n",
       " 'Next': 88,\n",
       " 'Hospitalization': 89,\n",
       " 'Hospital': 90,\n",
       " 'Discharge': 91,\n",
       " 'Procedure': 92,\n",
       " 'Left': 93,\n",
       " 'arthiscopic': 94,\n",
       " 'Employment': 95,\n",
       " 'Employer': 96,\n",
       " 'Policy': 97,\n",
       " 'Electronic': 98,\n",
       " 'Submission': 99,\n",
       " 'Identifier': 100,\n",
       " 'Electronically': 101,\n",
       " 'Signed': 102,\n",
       " 'Fraud': 103,\n",
       " 'Statements': 104,\n",
       " 'Reviewed': 105,\n",
       " 'and': 106,\n",
       " 'unum': 107,\n",
       " 'The': 108,\n",
       " 'Benefits': 109,\n",
       " 'Center': 110,\n",
       " 'Not': 111,\n",
       " 'for': 112,\n",
       " 'FMLA': 113,\n",
       " 'Requests': 114,\n",
       " 'Insured': 115,\n",
       " '’': 116,\n",
       " 's': 117,\n",
       " 'Signature': 118,\n",
       " 'Printed': 119,\n",
       " 'Unum': 120,\n",
       " 'Confirmation': 121,\n",
       " 'Coverage': 122,\n",
       " 'Group': 123,\n",
       " 'Customer': 124,\n",
       " 'EE': 125,\n",
       " 'Effective': 126,\n",
       " 'Employee': 127,\n",
       " 'Acc': 128,\n",
       " 'January': 129,\n",
       " 'Wellness': 130,\n",
       " 'Benefit': 131,\n",
       " 'Total': 132,\n",
       " 'Monthly': 133,\n",
       " 'Premium': 134,\n",
       " 'Montly': 135,\n",
       " 'Payroll': 136,\n",
       " 'Deduction': 137,\n",
       " 'MRI': 138,\n",
       " 'OF': 139,\n",
       " 'THE': 140,\n",
       " 'LEFT': 141,\n",
       " 'WRIST': 142,\n",
       " 'WITHOUT': 143,\n",
       " 'CONTRAST': 144,\n",
       " 'COMPARISON': 145,\n",
       " 'None': 146,\n",
       " 'These': 147,\n",
       " 'results': 148,\n",
       " 'were': 149,\n",
       " 'faxed': 150,\n",
       " 'Gelovich': 151,\n",
       " 'signed': 152,\n",
       " 'by': 153,\n",
       " 'Stephen': 154,\n",
       " 'Bravo': 155,\n",
       " 'Dependent': 156,\n",
       " 'Detail': 157,\n",
       " 'Billed': 158,\n",
       " 'Amounts': 159,\n",
       " 'Contract': 160,\n",
       " 'Adjustment': 161,\n",
       " 'Allowed': 162,\n",
       " 'Amount': 163,\n",
       " 'Covered': 164,\n",
       " 'Reason': 165,\n",
       " 'Deductible': 166,\n",
       " 'Other': 167,\n",
       " 'Carrier': 168,\n",
       " 'Paid': 169,\n",
       " 'Patient': 170,\n",
       " 'Responsibility': 171,\n",
       " 'Important': 172,\n",
       " 'about': 173,\n",
       " 'Your': 174,\n",
       " 'Appeal': 175,\n",
       " 'Rights': 176,\n",
       " 'All': 177,\n",
       " 'Languages': 178,\n",
       " 'Contact': 179,\n",
       " 'Did': 180,\n",
       " 'You': 181,\n",
       " 'Know': 182,\n",
       " 'Specialty': 183,\n",
       " 'Orthopedic': 184,\n",
       " 'Surgeon': 185,\n",
       " 'Zachary': 186,\n",
       " 'Jager': 187,\n",
       " 'Unknown': 188,\n",
       " 'Kari': 189,\n",
       " 'Lund': 190,\n",
       " 'Orthopedist': 191,\n",
       " 'Dan': 192,\n",
       " 'Palmer': 193,\n",
       " 'On': 194,\n",
       " '&': 195,\n",
       " 'May': 196,\n",
       " 'Spouse': 197,\n",
       " 'Child': 198,\n",
       " 'BLACK': 199,\n",
       " 'HILLS': 200,\n",
       " 'ORTHOPEDIC': 201,\n",
       " 'CENTER': 202,\n",
       " 'PC': 203,\n",
       " 'LAST': 204,\n",
       " 'PMT': 205,\n",
       " 'AMOUNT': 206,\n",
       " 'DUE': 207,\n",
       " 'DATE': 208,\n",
       " 'PAGE': 209,\n",
       " 'STATEMENT': 210,\n",
       " 'Ins': 211,\n",
       " 'Description': 212,\n",
       " 'E': 213,\n",
       " 'm': 214,\n",
       " 'New': 215,\n",
       " 'Moderat': 216,\n",
       " 'S': 217,\n",
       " 'Clo': 218,\n",
       " 'Tx': 219,\n",
       " 'Phalangealfx': 220,\n",
       " 'Finger': 221,\n",
       " 'Splint': 222,\n",
       " 'Offic': 223,\n",
       " 'Cons': 224,\n",
       " 'Moderate': 225,\n",
       " 'Sever': 226,\n",
       " 'Rad': 227,\n",
       " 'Exam': 228,\n",
       " 'Mini': 229,\n",
       " 'Views': 230,\n",
       " 'Applic': 231,\n",
       " 'Hand': 232,\n",
       " 'Lower': 233,\n",
       " 'Forearm': 234,\n",
       " 'Fiberglass': 235,\n",
       " 'gauntlet': 236,\n",
       " 'Cast': 237,\n",
       " 'Yrs': 238,\n",
       " 'Charge': 239,\n",
       " 'Pmt': 240,\n",
       " 'Pat': 241,\n",
       " 'Adjust': 242,\n",
       " 'Days': 243,\n",
       " 'Balance': 244,\n",
       " 'Pending': 245,\n",
       " 'Now': 246,\n",
       " 'Due': 247,\n",
       " 'Message': 248,\n",
       " 'Account': 249,\n",
       " 'Statement': 250,\n",
       " 'Billing': 251,\n",
       " 'Questions': 252,\n",
       " 'Choice': 253,\n",
       " 'Health': 254,\n",
       " 'Administrators': 255,\n",
       " 'Forwarding': 256,\n",
       " 'Service': 257,\n",
       " 'Requested': 258,\n",
       " 'REGIONAL': 259,\n",
       " 'HEALTH': 260,\n",
       " 'INC': 261,\n",
       " 'Participant': 262,\n",
       " 'ID': 263,\n",
       " 'Original': 264,\n",
       " 'Print': 265,\n",
       " 'Website': 266,\n",
       " 'DEA': 267,\n",
       " 'By': 268,\n",
       " 'Plan': 269,\n",
       " 'DEDUCTIBLE': 270,\n",
       " 'OUT': 271,\n",
       " 'POCKET': 272,\n",
       " 'Regional': 273,\n",
       " 'Inc': 274,\n",
       " 'Sign': 275,\n",
       " 'up': 276,\n",
       " 'paperless': 277,\n",
       " 'Individual': 278,\n",
       " 'Family': 279,\n",
       " 'Out': 280,\n",
       " 'Network': 281,\n",
       " 'Karl': 282,\n",
       " 'Services': 283,\n",
       " 'exam': 284,\n",
       " 'hand': 285,\n",
       " 'Modifiers': 286,\n",
       " 'TC': 287,\n",
       " 'RT': 288,\n",
       " 'Appeals': 289,\n",
       " 'This': 290,\n",
       " 'Qualified': 291,\n",
       " 'sign': 292,\n",
       " 'language': 293,\n",
       " 'interpreters': 294,\n",
       " 'written': 295,\n",
       " 'in': 296,\n",
       " 'other': 297,\n",
       " 'languages': 298,\n",
       " 'Jacquelin': 299,\n",
       " 'Brainard': 300,\n",
       " 'Compliance': 301,\n",
       " 'Officer': 302,\n",
       " 'or': 303,\n",
       " 'mail': 304,\n",
       " 'phone': 305,\n",
       " 'Department': 306,\n",
       " 'Human': 307,\n",
       " 'Complaint': 308,\n",
       " 'forms': 309,\n",
       " 'are': 310,\n",
       " 'available': 311,\n",
       " 'EXPLANATION': 312,\n",
       " 'BENEFITS': 313,\n",
       " 'Retain': 314,\n",
       " 'For': 315,\n",
       " 'Tax': 316,\n",
       " 'Purposes': 317,\n",
       " 'Status': 318,\n",
       " 'Period': 319,\n",
       " 'Totals': 320,\n",
       " 'DAKOTA': 321,\n",
       " 'Automated': 322,\n",
       " 'Attendant': 323,\n",
       " 'hours': 324,\n",
       " 'a': 325,\n",
       " 'day': 326,\n",
       " 'Payments': 327,\n",
       " 'Please': 328,\n",
       " 'Call': 329,\n",
       " 'Upon': 330,\n",
       " 'Receipt': 331,\n",
       " 'Pay': 332,\n",
       " 'Online': 333,\n",
       " '|': 334,\n",
       " 'Update': 335,\n",
       " 'Info': 336,\n",
       " 'See': 337,\n",
       " 'Details': 338,\n",
       " 'Back': 339,\n",
       " 'SHOW': 340,\n",
       " 'PAID': 341,\n",
       " 'HERE': 342,\n",
       " 'MAKE': 343,\n",
       " 'CHECKS': 344,\n",
       " 'TO': 345,\n",
       " 'INSUR': 346,\n",
       " 'PENDING': 347,\n",
       " 'PATIENT': 348,\n",
       " 'BALANCE': 349,\n",
       " 'EXAM': 350,\n",
       " 'HAND': 351,\n",
       " 'THORAC': 352,\n",
       " 'SPINE': 353,\n",
       " 'COMMERCIAL': 354,\n",
       " 'NON': 355,\n",
       " 'ALLOWED': 356,\n",
       " 'CT': 357,\n",
       " 'ABD': 358,\n",
       " 'PELV': 359,\n",
       " 'PAYMENT': 360,\n",
       " 'CHEST': 361,\n",
       " 'VIEWS': 362,\n",
       " 'Summary': 363,\n",
       " 'HARGES': 364,\n",
       " 'Of': 365,\n",
       " 'Today': 366,\n",
       " \"'s\": 367,\n",
       " 'Ethnicity': 368,\n",
       " 'Hispanic': 369,\n",
       " 'Latino': 370,\n",
       " 'Preferred': 371,\n",
       " 'English': 372,\n",
       " 'visit': 373,\n",
       " 'with': 374,\n",
       " 'Suzanne': 375,\n",
       " 'Newsom': 376,\n",
       " 'CNP': 377,\n",
       " '•': 378,\n",
       " 'Lethargy': 379,\n",
       " 'cough': 380,\n",
       " 'Vitals': 381,\n",
       " 'lbs': 382,\n",
       " 'kg': 383,\n",
       " 'Wt': 384,\n",
       " 'Temp': 385,\n",
       " 'F': 386,\n",
       " 'HR': 387,\n",
       " 'Oxygen': 388,\n",
       " 'sat': 389,\n",
       " 'Allergies': 390,\n",
       " 'Amoxicillin': 391,\n",
       " 'rash': 392,\n",
       " 'possible': 393,\n",
       " 'hives': 394,\n",
       " 'Active': 395,\n",
       " 'Diagnoses': 396,\n",
       " 'Include': 397,\n",
       " 'Acute': 398,\n",
       " 'frontal': 399,\n",
       " 'sinusitis': 400,\n",
       " 'unspecified': 401,\n",
       " 'Dizziness': 402,\n",
       " 'giddiness': 403,\n",
       " 'Medication': 404,\n",
       " 'List': 405,\n",
       " 'medications': 406,\n",
       " 'you': 407,\n",
       " 'Taking': 408,\n",
       " 'Zyrtec': 409,\n",
       " 'Childrens': 410,\n",
       " 'Allergy': 411,\n",
       " 'Notes': 412,\n",
       " 'Tests': 413,\n",
       " 'Labs': 414,\n",
       " 'Illumigene': 415,\n",
       " 'MYCO': 416,\n",
       " 'http': 417,\n",
       " 'BASIC': 418,\n",
       " 'METABOLIC': 419,\n",
       " 'SODIUM': 420,\n",
       " 'Range': 421,\n",
       " 'POTASSIUM': 422,\n",
       " 'CHLORIDE': 423,\n",
       " 'GLUCOSE': 424,\n",
       " 'BUN': 425,\n",
       " 'CREATININE': 426,\n",
       " 'CALCIUM': 427,\n",
       " 'CREA': 428,\n",
       " 'RATIO': 429,\n",
       " 'Ratio': 430,\n",
       " 'ANION': 431,\n",
       " 'GAP': 432,\n",
       " 'Calc': 433,\n",
       " 'CBC': 434,\n",
       " 'DIFF': 435,\n",
       " 'WBC': 436,\n",
       " 'RBC': 437,\n",
       " 'HGB': 438,\n",
       " 'HCT': 439,\n",
       " 'MCV': 440,\n",
       " 'fL': 441,\n",
       " 'MCH': 442,\n",
       " 'pg': 443,\n",
       " 'MCHC': 444,\n",
       " 'MPV': 445,\n",
       " 'PLATELETS': 446,\n",
       " 'NEUTROPHILS': 447,\n",
       " 'LYMPHOCYTES': 448,\n",
       " 'MONOCYTES': 449,\n",
       " 'Conditions': 450,\n",
       " 'Problem': 451,\n",
       " 'Idiopathic': 452,\n",
       " 'urticaria': 453,\n",
       " 'document': 454,\n",
       " 'wish': 455,\n",
       " 'keep': 456,\n",
       " 'Policyholder': 457,\n",
       " 'Owner': 458,\n",
       " 'Eastside': 459,\n",
       " 'Jasminder': 460,\n",
       " 'Singh': 461,\n",
       " 'Dev': 462,\n",
       " 'PA': 463,\n",
       " 'EXCUSE': 464,\n",
       " 'east': 465,\n",
       " 'side': 466,\n",
       " 'medical': 467,\n",
       " 'center': 468,\n",
       " 'April': 469,\n",
       " 'Weekly': 470,\n",
       " 'DOB': 471,\n",
       " 'Ph': 472,\n",
       " 'MR': 473,\n",
       " 'Primary': 474,\n",
       " 'Thoracic': 475,\n",
       " 'Strain': 476,\n",
       " 'have': 477,\n",
       " 'strained': 478,\n",
       " 'your': 479,\n",
       " 'thoracic': 480,\n",
       " 'spine': 481,\n",
       " 'Page': 482,\n",
       " 'IF': 483,\n",
       " 'ANY': 484,\n",
       " 'FOLLOWING': 485,\n",
       " 'OCCURS': 486,\n",
       " 'feel': 487,\n",
       " 'weakness': 488,\n",
       " 'arms': 489,\n",
       " 'legs': 490,\n",
       " 'severe': 491,\n",
       " 'increase': 492,\n",
       " 'pain': 493,\n",
       " 'Lumbosacral': 494,\n",
       " 'weak': 495,\n",
       " 'becomes': 496,\n",
       " 'more': 497,\n",
       " 'Follow': 498,\n",
       " 'Up': 499,\n",
       " 'What': 500,\n",
       " 'To': 501,\n",
       " 'Do': 502,\n",
       " 'Take': 503,\n",
       " 'all': 504,\n",
       " 'as': 505,\n",
       " 'directed': 506,\n",
       " 'Additional': 507,\n",
       " 'Prescriptions': 508,\n",
       " 'Written': 509,\n",
       " 'Prescriber': 510,\n",
       " 'Paper': 511,\n",
       " 'Prescription': 512,\n",
       " 'given': 513,\n",
       " 'patient': 514,\n",
       " 'Preventative': 515,\n",
       " 'Instructions': 516,\n",
       " 'knee': 517,\n",
       " 'injury': 518,\n",
       " 'David': 519,\n",
       " 'Bruce': 520,\n",
       " 'Identiﬁer': 521,\n",
       " 'June': 522,\n",
       " 'Explanation': 523,\n",
       " 'Gap': 524,\n",
       " 'no': 525,\n",
       " 'concussion': 526,\n",
       " 'Assistant': 527,\n",
       " 'devin': 528,\n",
       " 'conrad': 529,\n",
       " 'September': 530,\n",
       " 'ACCIDENT': 531,\n",
       " 'CLAIM': 532,\n",
       " 'FORM': 533,\n",
       " 'ATTENDING': 534,\n",
       " 'PHYSICIAN': 535,\n",
       " 'PLEASE': 536,\n",
       " 'PRINT': 537,\n",
       " 'PART': 538,\n",
       " 'I': 539,\n",
       " 'BE': 540,\n",
       " 'COMPLETED': 541,\n",
       " 'BY': 542,\n",
       " 'first': 543,\n",
       " 'unable': 544,\n",
       " 'work': 545,\n",
       " 'Expected': 546,\n",
       " 'Delivery': 547,\n",
       " 'Actual': 548,\n",
       " 'Unable': 549,\n",
       " 'Vaginal': 550,\n",
       " 'per': 551,\n",
       " 'Continued': 552,\n",
       " 'Facility': 553,\n",
       " 'State': 554,\n",
       " 'Zip': 555,\n",
       " 'Performed': 556,\n",
       " 'ICD': 557,\n",
       " 'Attending': 558,\n",
       " 'Degree': 559,\n",
       " 'A': 560,\n",
       " 'check': 561,\n",
       " 'type': 562,\n",
       " 'claim': 563,\n",
       " 'filing': 564,\n",
       " 'B': 565,\n",
       " 'About': 566,\n",
       " 'Suffix': 567,\n",
       " 'MI': 568,\n",
       " 'Spanish': 569,\n",
       " 'Short': 570,\n",
       " 'Term': 571,\n",
       " 'Disability': 572,\n",
       " 'Long': 573,\n",
       " 'Life': 574,\n",
       " 'Insurance': 575,\n",
       " 'Voluntary': 576,\n",
       " 'Was': 577,\n",
       " 'this': 578,\n",
       " 'motor': 579,\n",
       " 'vehicle': 580,\n",
       " 'accident': 581,\n",
       " 'Physicians': 582,\n",
       " 'Hospitals': 583,\n",
       " 'Considerations': 584,\n",
       " 'Male': 585,\n",
       " 'Female': 586,\n",
       " 'Surgical': 587,\n",
       " 'CPT': 588,\n",
       " 'X': 589,\n",
       " 'My': 590,\n",
       " 'Member': 591,\n",
       " 'Relationship': 592,\n",
       " 'person': 593,\n",
       " 'Marital': 594,\n",
       " 'Single': 595,\n",
       " 'Occ': 596,\n",
       " 'Title': 597,\n",
       " 'ResinMixer': 598,\n",
       " 'Hire': 599,\n",
       " 'Termination': 600,\n",
       " 'ATW': 601,\n",
       " 'Limitations': 602,\n",
       " 'Permitted': 603,\n",
       " 'Months': 604,\n",
       " 'Office': 605,\n",
       " 'Crane': 606,\n",
       " 'Composites': 607,\n",
       " 'Florence': 608,\n",
       " 'Earn': 609,\n",
       " 'Change': 610,\n",
       " 'Leave': 611,\n",
       " 'Absence': 612,\n",
       " 'Record': 613,\n",
       " 'Loaded': 614,\n",
       " 'Residence': 615,\n",
       " 'Physical': 616,\n",
       " 'Access': 617,\n",
       " 'Home': 618,\n",
       " 'Supervisor': 619,\n",
       " 'Coverages': 620,\n",
       " 'Product': 621,\n",
       " 'Flex': 622,\n",
       " 'Funding': 623,\n",
       " 'Fully': 624,\n",
       " 'Division': 625,\n",
       " 'PEG': 626,\n",
       " 'Eff': 627,\n",
       " 'Earnings': 628,\n",
       " 'Hourly': 629,\n",
       " 'Mode': 630,\n",
       " 'After': 631,\n",
       " 'Report': 632,\n",
       " 'ASO': 633,\n",
       " 'Self': 634,\n",
       " 'If': 635,\n",
       " 'yes': 636,\n",
       " 'dates': 637,\n",
       " 'admission': 638,\n",
       " 'Treatment': 639,\n",
       " 'Nature': 640,\n",
       " 'estimated': 641,\n",
       " 'duration': 642,\n",
       " 'treatments': 643,\n",
       " 'Job': 644,\n",
       " 'description': 645,\n",
       " 'is': 646,\n",
       " 'attached': 647,\n",
       " 'if': 648,\n",
       " 'checked': 649,\n",
       " 'here': 650,\n",
       " 'address': 651,\n",
       " 'Care': 652,\n",
       " 'DETAILS': 653,\n",
       " 'Dates': 654,\n",
       " 'including': 655,\n",
       " 'Confinement': 656,\n",
       " 'please': 657,\n",
       " 'provide': 658,\n",
       " 'following': 659,\n",
       " 'advice': 660,\n",
       " 'stop': 661,\n",
       " 'working': 662,\n",
       " 'what': 663,\n",
       " 'date': 664,\n",
       " 'Mgmt': 665,\n",
       " 'Svc': 666,\n",
       " 'Applicable': 667,\n",
       " 'Deductions': 668,\n",
       " 'Schedule': 669,\n",
       " 'Per': 670,\n",
       " 'Week': 671,\n",
       " 'Sick': 672,\n",
       " 'Variable': 673,\n",
       " 'Sunday': 674,\n",
       " 'Monday': 675,\n",
       " 'Tuesday': 676,\n",
       " 'Wednesday': 677,\n",
       " 'Thursday': 678,\n",
       " 'Friday': 679,\n",
       " 'Saturday': 680,\n",
       " 'SHORT': 681,\n",
       " 'TERM': 682,\n",
       " 'DISABILITY': 683,\n",
       " 'Hospitalized': 684,\n",
       " 'explain': 685,\n",
       " 'last': 686,\n",
       " 'office': 687,\n",
       " 'next': 688,\n",
       " 'Height': 689,\n",
       " 'Weight': 690,\n",
       " 'Secondary': 691,\n",
       " 'Has': 692,\n",
       " 'been': 693,\n",
       " 'hospitalized': 694,\n",
       " 'performed': 695,\n",
       " 'procedure': 696,\n",
       " 'was': 697,\n",
       " 'Functional': 698,\n",
       " 'Capacity': 699,\n",
       " 'Restrictions': 700,\n",
       " 'ELIZABETH': 701,\n",
       " 'EDGEWOOD': 702,\n",
       " 'FACESHEET': 703,\n",
       " 'MRN': 704,\n",
       " 'Sex': 705,\n",
       " 'Demographics': 706,\n",
       " 'SSN': 707,\n",
       " 'Reg': 708,\n",
       " 'Verified': 709,\n",
       " 'PCP': 710,\n",
       " 'Renew': 711,\n",
       " 'Admission': 712,\n",
       " 'Admitting': 713,\n",
       " 'Larkin': 714,\n",
       " 'John': 715,\n",
       " 'MD': 716,\n",
       " 'Elective': 717,\n",
       " 'Incomplete': 718,\n",
       " 'Area': 719,\n",
       " 'SERVICE': 720,\n",
       " 'AREA': 721,\n",
       " 'EDG': 722,\n",
       " 'SC': 723,\n",
       " 'CRESTVIEW': 724,\n",
       " 'Discharged': 725,\n",
       " 'Confirmed': 726,\n",
       " 'HOSE': 727,\n",
       " 'ital': 728,\n",
       " 'Acct': 729,\n",
       " 'Class': 730,\n",
       " 'Same': 731,\n",
       " 'Guarantor': 732,\n",
       " 'Relation': 733,\n",
       " 'Pt': 734,\n",
       " 'SEH': 735,\n",
       " 'Precert': 736,\n",
       " 'Subscriber': 737,\n",
       " 'Operative': 738,\n",
       " 'Brief': 739,\n",
       " 'Op': 740,\n",
       " 'Note': 741,\n",
       " 'OP': 742,\n",
       " 'Adm': 743,\n",
       " 'continued': 744,\n",
       " 'Author': 745,\n",
       " 'J': 746,\n",
       " 'Filed': 747,\n",
       " 'Editor': 748,\n",
       " 'Elizabeth': 749,\n",
       " 'Healthcare': 750,\n",
       " 'NOTE': 751,\n",
       " 'Body': 752,\n",
       " 'mass': 753,\n",
       " 'index': 754,\n",
       " 'PROCEDURE': 755,\n",
       " 'SURGEON': 756,\n",
       " 'Role': 757,\n",
       " 'ANESTHESIA': 758,\n",
       " 'General': 759,\n",
       " 'SPECIMENS': 760,\n",
       " 'specimens': 761,\n",
       " 'log': 762,\n",
       " 'ESTIMATED': 763,\n",
       " 'BLOOD': 764,\n",
       " 'LOSS': 765,\n",
       " 'PROC': 766,\n",
       " 'COURSE': 767,\n",
       " 'PACU': 768,\n",
       " 'OPERATION': 769,\n",
       " 'Broken': 770,\n",
       " 'big': 771,\n",
       " 'toe': 772,\n",
       " 'foot': 773,\n",
       " 'Todd': 774,\n",
       " 'Francis': 775,\n",
       " 'Podiatrist': 776,\n",
       " 'Ryan': 777,\n",
       " 'Kish': 778,\n",
       " 'Toledo': 779,\n",
       " 'ER': 780,\n",
       " 'Xray': 781,\n",
       " 'July': 782,\n",
       " 'Paramount': 783,\n",
       " 'ProMedica': 784,\n",
       " 'LINE': 785,\n",
       " 'AUTHORIZATION': 786,\n",
       " 'NO': 787,\n",
       " 'CODE': 788,\n",
       " 'MODIFIER': 789,\n",
       " 'DIAGNOSIS': 790,\n",
       " 'EXPLAIN': 791,\n",
       " 'BILLED': 792,\n",
       " 'PARAMOUNT': 793,\n",
       " 'LT': 794,\n",
       " 'Indicates': 795,\n",
       " 'additional': 796,\n",
       " 'information': 797,\n",
       " 'C': 798,\n",
       " 'DPM': 799,\n",
       " 'EMS': 800,\n",
       " 'Christine': 801,\n",
       " 'Nolen': 802,\n",
       " 'Waukesha': 803,\n",
       " 'Memorial': 804,\n",
       " 'Cleaning': 805,\n",
       " 'xray': 806,\n",
       " 'bandage': 807,\n",
       " 'MONTANO': 808,\n",
       " 'CI': 809,\n",
       " 'Web': 810,\n",
       " 'user': 811,\n",
       " 'notes': 812,\n",
       " 'statements': 813,\n",
       " 'PROHEALTH': 814,\n",
       " 'CARE': 815,\n",
       " 'SERVICES': 816,\n",
       " 'GUARANTOR': 817,\n",
       " 'NAME': 818,\n",
       " 'DESCRIPTION': 819,\n",
       " 'PAYMENTS': 820,\n",
       " 'ADJUSTMENTS': 821,\n",
       " 'PATIENTS': 822,\n",
       " 'INVOICE': 823,\n",
       " 'NUMBER': 824,\n",
       " 'Previous': 825,\n",
       " 'CURRENT': 826,\n",
       " 'TOTAL': 827,\n",
       " 'VISIT': 828,\n",
       " 'PAY': 829,\n",
       " 'THIS': 830,\n",
       " 'RETURN': 831,\n",
       " 'PORTION': 832,\n",
       " 'WITH': 833,\n",
       " 'YOUR': 834,\n",
       " 'MASTERCARD': 835,\n",
       " 'DISCOVER': 836,\n",
       " 'VISA': 837,\n",
       " 'Enclosed': 838,\n",
       " 'CHECK': 839,\n",
       " 'PAYABLE': 840,\n",
       " 'EMERGENCY': 841,\n",
       " 'MEDICAL': 842,\n",
       " 'ASSOCIATES': 843,\n",
       " 'PHONE': 844,\n",
       " 'ADDRESSEE': 845,\n",
       " 'INDUSTRIAL': 846,\n",
       " 'LOOP': 847,\n",
       " 'ACCOUNT': 848,\n",
       " \"'S\": 849,\n",
       " 'DEPT': 850,\n",
       " 'PPO': 851,\n",
       " 'ADJ': 852,\n",
       " 'UMR': 853,\n",
       " 'FISERV': 854,\n",
       " 'WI': 855,\n",
       " 'ON': 856,\n",
       " 'OVER': 857,\n",
       " 'DAYS': 858,\n",
       " 'STMT': 859,\n",
       " 'DOCTOR': 860,\n",
       " 'LEGEND': 861,\n",
       " 'NOLEN': 862,\n",
       " 'CHRISTINE': 863,\n",
       " 'D': 864,\n",
       " 'COMMENTS': 865,\n",
       " 'PRIMARY': 866,\n",
       " 'SECONDARY': 867,\n",
       " 'Concussion': 868,\n",
       " 'Souha': 869,\n",
       " 'Hakim': 870,\n",
       " 'MedExpress': 871,\n",
       " 'Codes': 872,\n",
       " 'Urgent': 873,\n",
       " 'Clairn': 874,\n",
       " 'ME': 875,\n",
       " 'Seen': 876,\n",
       " 'Vijay': 877,\n",
       " 'Patel': 878,\n",
       " 'Holder': 879,\n",
       " 'Qty': 880,\n",
       " 'Clinical': 881,\n",
       " 'Chief': 882,\n",
       " 'Penicillins': 883,\n",
       " 'Rash': 884,\n",
       " 'Taken': 885,\n",
       " 'BP': 886,\n",
       " 'mmHg': 887,\n",
       " 'PULSE': 888,\n",
       " 'bpm': 889,\n",
       " 'RESP': 890,\n",
       " 'TEMP': 891,\n",
       " 'WEIGHT': 892,\n",
       " 'lb': 893,\n",
       " 'ft': 894,\n",
       " 'BMI': 895,\n",
       " 'SAT': 896,\n",
       " 'Current': 897,\n",
       " 'Meds': 898,\n",
       " 'ACTIVE': 899,\n",
       " 'albuterol': 900,\n",
       " 'sulfate': 901,\n",
       " 'Encounter': 902,\n",
       " 'Progress': 903,\n",
       " 'none': 904,\n",
       " 'Subjective': 905,\n",
       " 'History': 906,\n",
       " 'provided': 907,\n",
       " 'dad': 908,\n",
       " 'interpreter': 909,\n",
       " 'used': 910,\n",
       " 'presents': 911,\n",
       " 'urgent': 912,\n",
       " 'care': 913,\n",
       " 'Review': 914,\n",
       " 'Systems': 915,\n",
       " 'Cardiovascular': 916,\n",
       " 'Negative': 917,\n",
       " 'chest': 918,\n",
       " 'Skin': 919,\n",
       " 'Neurological': 920,\n",
       " 'headaches': 921,\n",
       " 'Objective': 922,\n",
       " 'HENT': 923,\n",
       " 'Right': 924,\n",
       " 'Ear': 925,\n",
       " 'Tympanic': 926,\n",
       " 'membrane': 927,\n",
       " 'normal': 928,\n",
       " 'Nose': 929,\n",
       " 'Oropharynx': 930,\n",
       " 'clear': 931,\n",
       " 'Eyes': 932,\n",
       " 'Conjunctivae': 933,\n",
       " 'EOM': 934,\n",
       " 'Neck': 935,\n",
       " 'supple': 936,\n",
       " 'rigidity': 937,\n",
       " 'murmur': 938,\n",
       " 'heard': 939,\n",
       " 'Lymphadenopathy': 940,\n",
       " 'He': 941,\n",
       " 'has': 942,\n",
       " 'cervical': 943,\n",
       " 'adenopathy': 944,\n",
       " 'alert': 945,\n",
       " 'warm': 946,\n",
       " 'noted': 947,\n",
       " 'Assessment': 948,\n",
       " 'advise': 949,\n",
       " 'SS': 950,\n",
       " 'Penobscot': 951,\n",
       " 'Community': 952,\n",
       " 'Suite': 953,\n",
       " 'Medicine': 954,\n",
       " 'Mental': 955,\n",
       " 'FAX': 956,\n",
       " 'Transmission': 957,\n",
       " 'Sheet': 958,\n",
       " 'FROM': 959,\n",
       " 'Ext': 960,\n",
       " 'number': 961,\n",
       " 'pages': 962,\n",
       " 'cover': 963,\n",
       " 'page': 964,\n",
       " 'Thank': 965,\n",
       " 'Revised': 966,\n",
       " 'Imaging': 967,\n",
       " 'MaineCare': 968,\n",
       " 'FQHC': 969,\n",
       " 'Low': 970,\n",
       " 'JT': 971,\n",
       " 'Rt': 972,\n",
       " 'Erin': 973,\n",
       " 'Barker': 974,\n",
       " 'Joseph': 975,\n",
       " 'Ordering': 976,\n",
       " 'BARKER': 977,\n",
       " 'ERIN': 978,\n",
       " 'RIGHT': 979,\n",
       " 'KNEE': 980,\n",
       " 'INTERCONDYLAR': 981,\n",
       " 'SPACE': 982,\n",
       " 'ACL': 983,\n",
       " 'PCL': 984,\n",
       " 'intact': 985,\n",
       " 'Unremarkable': 986,\n",
       " 'marrow': 987,\n",
       " 'signal': 988,\n",
       " 'T': 989,\n",
       " 'DICTATION': 990,\n",
       " 'LOCATION': 991,\n",
       " 'MPSYNERNET': 992,\n",
       " 'Reading': 993,\n",
       " 'KASPER': 994,\n",
       " 'JARED': 995,\n",
       " 'Down': 996,\n",
       " 'East': 997,\n",
       " 'Orthopedics': 998,\n",
       " 'Sports': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'UNK',\n",
       " 1: ' ',\n",
       " 2: '\\t',\n",
       " 3: '\\n',\n",
       " 4: 'Claim',\n",
       " 5: 'Type',\n",
       " 6: 'VB',\n",
       " 7: 'Accident',\n",
       " 8: 'Accidental',\n",
       " 9: 'Injury',\n",
       " 10: 'Information',\n",
       " 11: 'First',\n",
       " 12: 'Name',\n",
       " 13: 'Middle',\n",
       " 14: 'Last',\n",
       " 15: 'Social',\n",
       " 16: 'Security',\n",
       " 17: 'Number',\n",
       " 18: 'Birth',\n",
       " 19: 'Date',\n",
       " 20: 'Gender',\n",
       " 21: 'Language',\n",
       " 22: 'Preference',\n",
       " 23: 'Address',\n",
       " 24: 'Line',\n",
       " 25: 'City',\n",
       " 26: 'Postal',\n",
       " 27: 'Code',\n",
       " 28: 'Country',\n",
       " 29: 'Best',\n",
       " 30: 'Phone',\n",
       " 31: 'to',\n",
       " 32: 'be',\n",
       " 33: 'Reached',\n",
       " 34: 'During',\n",
       " 35: 'the',\n",
       " 36: 'Day',\n",
       " 37: 'Email',\n",
       " 38: 'RADIOLOGY',\n",
       " 39: 'REPORT',\n",
       " 40: 'UNKNOWN',\n",
       " 41: 'Technique',\n",
       " 42: 'views',\n",
       " 43: 'left',\n",
       " 44: 'wrist',\n",
       " 45: 'FINDINGS',\n",
       " 46: 'IMPRESSION',\n",
       " 47: 'No',\n",
       " 48: 'acute',\n",
       " 49: 'osseous',\n",
       " 50: 'abnormality',\n",
       " 51: 'identified',\n",
       " 52: 'Daytime',\n",
       " 53: 'Event',\n",
       " 54: 'Stopped',\n",
       " 55: 'Working',\n",
       " 56: 'Yes',\n",
       " 57: 'Physically',\n",
       " 58: 'at',\n",
       " 59: 'Work',\n",
       " 60: 'Hours',\n",
       " 61: 'Worked',\n",
       " 62: 'on',\n",
       " 63: 'Scheduled',\n",
       " 64: 'Missed',\n",
       " 65: 'Returned',\n",
       " 66: 'Related',\n",
       " 67: 'Time',\n",
       " 68: 'of',\n",
       " 69: 'Diagnosis',\n",
       " 70: 'Arthiscopic',\n",
       " 71: 'surgery',\n",
       " 72: 'Surgery',\n",
       " 73: 'Is',\n",
       " 74: 'Required',\n",
       " 75: 'Indicator',\n",
       " 76: 'Outpatient',\n",
       " 77: 'Medical',\n",
       " 78: 'Provider',\n",
       " 79: 'Physician',\n",
       " 80: 'Roles',\n",
       " 81: 'Treating',\n",
       " 82: 'Patrick',\n",
       " 83: 'Emerson',\n",
       " 84: 'Business',\n",
       " 85: 'Telephone',\n",
       " 86: 'Fax',\n",
       " 87: 'Visit',\n",
       " 88: 'Next',\n",
       " 89: 'Hospitalization',\n",
       " 90: 'Hospital',\n",
       " 91: 'Discharge',\n",
       " 92: 'Procedure',\n",
       " 93: 'Left',\n",
       " 94: 'arthiscopic',\n",
       " 95: 'Employment',\n",
       " 96: 'Employer',\n",
       " 97: 'Policy',\n",
       " 98: 'Electronic',\n",
       " 99: 'Submission',\n",
       " 100: 'Identifier',\n",
       " 101: 'Electronically',\n",
       " 102: 'Signed',\n",
       " 103: 'Fraud',\n",
       " 104: 'Statements',\n",
       " 105: 'Reviewed',\n",
       " 106: 'and',\n",
       " 107: 'unum',\n",
       " 108: 'The',\n",
       " 109: 'Benefits',\n",
       " 110: 'Center',\n",
       " 111: 'Not',\n",
       " 112: 'for',\n",
       " 113: 'FMLA',\n",
       " 114: 'Requests',\n",
       " 115: 'Insured',\n",
       " 116: '’',\n",
       " 117: 's',\n",
       " 118: 'Signature',\n",
       " 119: 'Printed',\n",
       " 120: 'Unum',\n",
       " 121: 'Confirmation',\n",
       " 122: 'Coverage',\n",
       " 123: 'Group',\n",
       " 124: 'Customer',\n",
       " 125: 'EE',\n",
       " 126: 'Effective',\n",
       " 127: 'Employee',\n",
       " 128: 'Acc',\n",
       " 129: 'January',\n",
       " 130: 'Wellness',\n",
       " 131: 'Benefit',\n",
       " 132: 'Total',\n",
       " 133: 'Monthly',\n",
       " 134: 'Premium',\n",
       " 135: 'Montly',\n",
       " 136: 'Payroll',\n",
       " 137: 'Deduction',\n",
       " 138: 'MRI',\n",
       " 139: 'OF',\n",
       " 140: 'THE',\n",
       " 141: 'LEFT',\n",
       " 142: 'WRIST',\n",
       " 143: 'WITHOUT',\n",
       " 144: 'CONTRAST',\n",
       " 145: 'COMPARISON',\n",
       " 146: 'None',\n",
       " 147: 'These',\n",
       " 148: 'results',\n",
       " 149: 'were',\n",
       " 150: 'faxed',\n",
       " 151: 'Gelovich',\n",
       " 152: 'signed',\n",
       " 153: 'by',\n",
       " 154: 'Stephen',\n",
       " 155: 'Bravo',\n",
       " 156: 'Dependent',\n",
       " 157: 'Detail',\n",
       " 158: 'Billed',\n",
       " 159: 'Amounts',\n",
       " 160: 'Contract',\n",
       " 161: 'Adjustment',\n",
       " 162: 'Allowed',\n",
       " 163: 'Amount',\n",
       " 164: 'Covered',\n",
       " 165: 'Reason',\n",
       " 166: 'Deductible',\n",
       " 167: 'Other',\n",
       " 168: 'Carrier',\n",
       " 169: 'Paid',\n",
       " 170: 'Patient',\n",
       " 171: 'Responsibility',\n",
       " 172: 'Important',\n",
       " 173: 'about',\n",
       " 174: 'Your',\n",
       " 175: 'Appeal',\n",
       " 176: 'Rights',\n",
       " 177: 'All',\n",
       " 178: 'Languages',\n",
       " 179: 'Contact',\n",
       " 180: 'Did',\n",
       " 181: 'You',\n",
       " 182: 'Know',\n",
       " 183: 'Specialty',\n",
       " 184: 'Orthopedic',\n",
       " 185: 'Surgeon',\n",
       " 186: 'Zachary',\n",
       " 187: 'Jager',\n",
       " 188: 'Unknown',\n",
       " 189: 'Kari',\n",
       " 190: 'Lund',\n",
       " 191: 'Orthopedist',\n",
       " 192: 'Dan',\n",
       " 193: 'Palmer',\n",
       " 194: 'On',\n",
       " 195: '&',\n",
       " 196: 'May',\n",
       " 197: 'Spouse',\n",
       " 198: 'Child',\n",
       " 199: 'BLACK',\n",
       " 200: 'HILLS',\n",
       " 201: 'ORTHOPEDIC',\n",
       " 202: 'CENTER',\n",
       " 203: 'PC',\n",
       " 204: 'LAST',\n",
       " 205: 'PMT',\n",
       " 206: 'AMOUNT',\n",
       " 207: 'DUE',\n",
       " 208: 'DATE',\n",
       " 209: 'PAGE',\n",
       " 210: 'STATEMENT',\n",
       " 211: 'Ins',\n",
       " 212: 'Description',\n",
       " 213: 'E',\n",
       " 214: 'm',\n",
       " 215: 'New',\n",
       " 216: 'Moderat',\n",
       " 217: 'S',\n",
       " 218: 'Clo',\n",
       " 219: 'Tx',\n",
       " 220: 'Phalangealfx',\n",
       " 221: 'Finger',\n",
       " 222: 'Splint',\n",
       " 223: 'Offic',\n",
       " 224: 'Cons',\n",
       " 225: 'Moderate',\n",
       " 226: 'Sever',\n",
       " 227: 'Rad',\n",
       " 228: 'Exam',\n",
       " 229: 'Mini',\n",
       " 230: 'Views',\n",
       " 231: 'Applic',\n",
       " 232: 'Hand',\n",
       " 233: 'Lower',\n",
       " 234: 'Forearm',\n",
       " 235: 'Fiberglass',\n",
       " 236: 'gauntlet',\n",
       " 237: 'Cast',\n",
       " 238: 'Yrs',\n",
       " 239: 'Charge',\n",
       " 240: 'Pmt',\n",
       " 241: 'Pat',\n",
       " 242: 'Adjust',\n",
       " 243: 'Days',\n",
       " 244: 'Balance',\n",
       " 245: 'Pending',\n",
       " 246: 'Now',\n",
       " 247: 'Due',\n",
       " 248: 'Message',\n",
       " 249: 'Account',\n",
       " 250: 'Statement',\n",
       " 251: 'Billing',\n",
       " 252: 'Questions',\n",
       " 253: 'Choice',\n",
       " 254: 'Health',\n",
       " 255: 'Administrators',\n",
       " 256: 'Forwarding',\n",
       " 257: 'Service',\n",
       " 258: 'Requested',\n",
       " 259: 'REGIONAL',\n",
       " 260: 'HEALTH',\n",
       " 261: 'INC',\n",
       " 262: 'Participant',\n",
       " 263: 'ID',\n",
       " 264: 'Original',\n",
       " 265: 'Print',\n",
       " 266: 'Website',\n",
       " 267: 'DEA',\n",
       " 268: 'By',\n",
       " 269: 'Plan',\n",
       " 270: 'DEDUCTIBLE',\n",
       " 271: 'OUT',\n",
       " 272: 'POCKET',\n",
       " 273: 'Regional',\n",
       " 274: 'Inc',\n",
       " 275: 'Sign',\n",
       " 276: 'up',\n",
       " 277: 'paperless',\n",
       " 278: 'Individual',\n",
       " 279: 'Family',\n",
       " 280: 'Out',\n",
       " 281: 'Network',\n",
       " 282: 'Karl',\n",
       " 283: 'Services',\n",
       " 284: 'exam',\n",
       " 285: 'hand',\n",
       " 286: 'Modifiers',\n",
       " 287: 'TC',\n",
       " 288: 'RT',\n",
       " 289: 'Appeals',\n",
       " 290: 'This',\n",
       " 291: 'Qualified',\n",
       " 292: 'sign',\n",
       " 293: 'language',\n",
       " 294: 'interpreters',\n",
       " 295: 'written',\n",
       " 296: 'in',\n",
       " 297: 'other',\n",
       " 298: 'languages',\n",
       " 299: 'Jacquelin',\n",
       " 300: 'Brainard',\n",
       " 301: 'Compliance',\n",
       " 302: 'Officer',\n",
       " 303: 'or',\n",
       " 304: 'mail',\n",
       " 305: 'phone',\n",
       " 306: 'Department',\n",
       " 307: 'Human',\n",
       " 308: 'Complaint',\n",
       " 309: 'forms',\n",
       " 310: 'are',\n",
       " 311: 'available',\n",
       " 312: 'EXPLANATION',\n",
       " 313: 'BENEFITS',\n",
       " 314: 'Retain',\n",
       " 315: 'For',\n",
       " 316: 'Tax',\n",
       " 317: 'Purposes',\n",
       " 318: 'Status',\n",
       " 319: 'Period',\n",
       " 320: 'Totals',\n",
       " 321: 'DAKOTA',\n",
       " 322: 'Automated',\n",
       " 323: 'Attendant',\n",
       " 324: 'hours',\n",
       " 325: 'a',\n",
       " 326: 'day',\n",
       " 327: 'Payments',\n",
       " 328: 'Please',\n",
       " 329: 'Call',\n",
       " 330: 'Upon',\n",
       " 331: 'Receipt',\n",
       " 332: 'Pay',\n",
       " 333: 'Online',\n",
       " 334: '|',\n",
       " 335: 'Update',\n",
       " 336: 'Info',\n",
       " 337: 'See',\n",
       " 338: 'Details',\n",
       " 339: 'Back',\n",
       " 340: 'SHOW',\n",
       " 341: 'PAID',\n",
       " 342: 'HERE',\n",
       " 343: 'MAKE',\n",
       " 344: 'CHECKS',\n",
       " 345: 'TO',\n",
       " 346: 'INSUR',\n",
       " 347: 'PENDING',\n",
       " 348: 'PATIENT',\n",
       " 349: 'BALANCE',\n",
       " 350: 'EXAM',\n",
       " 351: 'HAND',\n",
       " 352: 'THORAC',\n",
       " 353: 'SPINE',\n",
       " 354: 'COMMERCIAL',\n",
       " 355: 'NON',\n",
       " 356: 'ALLOWED',\n",
       " 357: 'CT',\n",
       " 358: 'ABD',\n",
       " 359: 'PELV',\n",
       " 360: 'PAYMENT',\n",
       " 361: 'CHEST',\n",
       " 362: 'VIEWS',\n",
       " 363: 'Summary',\n",
       " 364: 'HARGES',\n",
       " 365: 'Of',\n",
       " 366: 'Today',\n",
       " 367: \"'s\",\n",
       " 368: 'Ethnicity',\n",
       " 369: 'Hispanic',\n",
       " 370: 'Latino',\n",
       " 371: 'Preferred',\n",
       " 372: 'English',\n",
       " 373: 'visit',\n",
       " 374: 'with',\n",
       " 375: 'Suzanne',\n",
       " 376: 'Newsom',\n",
       " 377: 'CNP',\n",
       " 378: '•',\n",
       " 379: 'Lethargy',\n",
       " 380: 'cough',\n",
       " 381: 'Vitals',\n",
       " 382: 'lbs',\n",
       " 383: 'kg',\n",
       " 384: 'Wt',\n",
       " 385: 'Temp',\n",
       " 386: 'F',\n",
       " 387: 'HR',\n",
       " 388: 'Oxygen',\n",
       " 389: 'sat',\n",
       " 390: 'Allergies',\n",
       " 391: 'Amoxicillin',\n",
       " 392: 'rash',\n",
       " 393: 'possible',\n",
       " 394: 'hives',\n",
       " 395: 'Active',\n",
       " 396: 'Diagnoses',\n",
       " 397: 'Include',\n",
       " 398: 'Acute',\n",
       " 399: 'frontal',\n",
       " 400: 'sinusitis',\n",
       " 401: 'unspecified',\n",
       " 402: 'Dizziness',\n",
       " 403: 'giddiness',\n",
       " 404: 'Medication',\n",
       " 405: 'List',\n",
       " 406: 'medications',\n",
       " 407: 'you',\n",
       " 408: 'Taking',\n",
       " 409: 'Zyrtec',\n",
       " 410: 'Childrens',\n",
       " 411: 'Allergy',\n",
       " 412: 'Notes',\n",
       " 413: 'Tests',\n",
       " 414: 'Labs',\n",
       " 415: 'Illumigene',\n",
       " 416: 'MYCO',\n",
       " 417: 'http',\n",
       " 418: 'BASIC',\n",
       " 419: 'METABOLIC',\n",
       " 420: 'SODIUM',\n",
       " 421: 'Range',\n",
       " 422: 'POTASSIUM',\n",
       " 423: 'CHLORIDE',\n",
       " 424: 'GLUCOSE',\n",
       " 425: 'BUN',\n",
       " 426: 'CREATININE',\n",
       " 427: 'CALCIUM',\n",
       " 428: 'CREA',\n",
       " 429: 'RATIO',\n",
       " 430: 'Ratio',\n",
       " 431: 'ANION',\n",
       " 432: 'GAP',\n",
       " 433: 'Calc',\n",
       " 434: 'CBC',\n",
       " 435: 'DIFF',\n",
       " 436: 'WBC',\n",
       " 437: 'RBC',\n",
       " 438: 'HGB',\n",
       " 439: 'HCT',\n",
       " 440: 'MCV',\n",
       " 441: 'fL',\n",
       " 442: 'MCH',\n",
       " 443: 'pg',\n",
       " 444: 'MCHC',\n",
       " 445: 'MPV',\n",
       " 446: 'PLATELETS',\n",
       " 447: 'NEUTROPHILS',\n",
       " 448: 'LYMPHOCYTES',\n",
       " 449: 'MONOCYTES',\n",
       " 450: 'Conditions',\n",
       " 451: 'Problem',\n",
       " 452: 'Idiopathic',\n",
       " 453: 'urticaria',\n",
       " 454: 'document',\n",
       " 455: 'wish',\n",
       " 456: 'keep',\n",
       " 457: 'Policyholder',\n",
       " 458: 'Owner',\n",
       " 459: 'Eastside',\n",
       " 460: 'Jasminder',\n",
       " 461: 'Singh',\n",
       " 462: 'Dev',\n",
       " 463: 'PA',\n",
       " 464: 'EXCUSE',\n",
       " 465: 'east',\n",
       " 466: 'side',\n",
       " 467: 'medical',\n",
       " 468: 'center',\n",
       " 469: 'April',\n",
       " 470: 'Weekly',\n",
       " 471: 'DOB',\n",
       " 472: 'Ph',\n",
       " 473: 'MR',\n",
       " 474: 'Primary',\n",
       " 475: 'Thoracic',\n",
       " 476: 'Strain',\n",
       " 477: 'have',\n",
       " 478: 'strained',\n",
       " 479: 'your',\n",
       " 480: 'thoracic',\n",
       " 481: 'spine',\n",
       " 482: 'Page',\n",
       " 483: 'IF',\n",
       " 484: 'ANY',\n",
       " 485: 'FOLLOWING',\n",
       " 486: 'OCCURS',\n",
       " 487: 'feel',\n",
       " 488: 'weakness',\n",
       " 489: 'arms',\n",
       " 490: 'legs',\n",
       " 491: 'severe',\n",
       " 492: 'increase',\n",
       " 493: 'pain',\n",
       " 494: 'Lumbosacral',\n",
       " 495: 'weak',\n",
       " 496: 'becomes',\n",
       " 497: 'more',\n",
       " 498: 'Follow',\n",
       " 499: 'Up',\n",
       " 500: 'What',\n",
       " 501: 'To',\n",
       " 502: 'Do',\n",
       " 503: 'Take',\n",
       " 504: 'all',\n",
       " 505: 'as',\n",
       " 506: 'directed',\n",
       " 507: 'Additional',\n",
       " 508: 'Prescriptions',\n",
       " 509: 'Written',\n",
       " 510: 'Prescriber',\n",
       " 511: 'Paper',\n",
       " 512: 'Prescription',\n",
       " 513: 'given',\n",
       " 514: 'patient',\n",
       " 515: 'Preventative',\n",
       " 516: 'Instructions',\n",
       " 517: 'knee',\n",
       " 518: 'injury',\n",
       " 519: 'David',\n",
       " 520: 'Bruce',\n",
       " 521: 'Identiﬁer',\n",
       " 522: 'June',\n",
       " 523: 'Explanation',\n",
       " 524: 'Gap',\n",
       " 525: 'no',\n",
       " 526: 'concussion',\n",
       " 527: 'Assistant',\n",
       " 528: 'devin',\n",
       " 529: 'conrad',\n",
       " 530: 'September',\n",
       " 531: 'ACCIDENT',\n",
       " 532: 'CLAIM',\n",
       " 533: 'FORM',\n",
       " 534: 'ATTENDING',\n",
       " 535: 'PHYSICIAN',\n",
       " 536: 'PLEASE',\n",
       " 537: 'PRINT',\n",
       " 538: 'PART',\n",
       " 539: 'I',\n",
       " 540: 'BE',\n",
       " 541: 'COMPLETED',\n",
       " 542: 'BY',\n",
       " 543: 'first',\n",
       " 544: 'unable',\n",
       " 545: 'work',\n",
       " 546: 'Expected',\n",
       " 547: 'Delivery',\n",
       " 548: 'Actual',\n",
       " 549: 'Unable',\n",
       " 550: 'Vaginal',\n",
       " 551: 'per',\n",
       " 552: 'Continued',\n",
       " 553: 'Facility',\n",
       " 554: 'State',\n",
       " 555: 'Zip',\n",
       " 556: 'Performed',\n",
       " 557: 'ICD',\n",
       " 558: 'Attending',\n",
       " 559: 'Degree',\n",
       " 560: 'A',\n",
       " 561: 'check',\n",
       " 562: 'type',\n",
       " 563: 'claim',\n",
       " 564: 'filing',\n",
       " 565: 'B',\n",
       " 566: 'About',\n",
       " 567: 'Suffix',\n",
       " 568: 'MI',\n",
       " 569: 'Spanish',\n",
       " 570: 'Short',\n",
       " 571: 'Term',\n",
       " 572: 'Disability',\n",
       " 573: 'Long',\n",
       " 574: 'Life',\n",
       " 575: 'Insurance',\n",
       " 576: 'Voluntary',\n",
       " 577: 'Was',\n",
       " 578: 'this',\n",
       " 579: 'motor',\n",
       " 580: 'vehicle',\n",
       " 581: 'accident',\n",
       " 582: 'Physicians',\n",
       " 583: 'Hospitals',\n",
       " 584: 'Considerations',\n",
       " 585: 'Male',\n",
       " 586: 'Female',\n",
       " 587: 'Surgical',\n",
       " 588: 'CPT',\n",
       " 589: 'X',\n",
       " 590: 'My',\n",
       " 591: 'Member',\n",
       " 592: 'Relationship',\n",
       " 593: 'person',\n",
       " 594: 'Marital',\n",
       " 595: 'Single',\n",
       " 596: 'Occ',\n",
       " 597: 'Title',\n",
       " 598: 'ResinMixer',\n",
       " 599: 'Hire',\n",
       " 600: 'Termination',\n",
       " 601: 'ATW',\n",
       " 602: 'Limitations',\n",
       " 603: 'Permitted',\n",
       " 604: 'Months',\n",
       " 605: 'Office',\n",
       " 606: 'Crane',\n",
       " 607: 'Composites',\n",
       " 608: 'Florence',\n",
       " 609: 'Earn',\n",
       " 610: 'Change',\n",
       " 611: 'Leave',\n",
       " 612: 'Absence',\n",
       " 613: 'Record',\n",
       " 614: 'Loaded',\n",
       " 615: 'Residence',\n",
       " 616: 'Physical',\n",
       " 617: 'Access',\n",
       " 618: 'Home',\n",
       " 619: 'Supervisor',\n",
       " 620: 'Coverages',\n",
       " 621: 'Product',\n",
       " 622: 'Flex',\n",
       " 623: 'Funding',\n",
       " 624: 'Fully',\n",
       " 625: 'Division',\n",
       " 626: 'PEG',\n",
       " 627: 'Eff',\n",
       " 628: 'Earnings',\n",
       " 629: 'Hourly',\n",
       " 630: 'Mode',\n",
       " 631: 'After',\n",
       " 632: 'Report',\n",
       " 633: 'ASO',\n",
       " 634: 'Self',\n",
       " 635: 'If',\n",
       " 636: 'yes',\n",
       " 637: 'dates',\n",
       " 638: 'admission',\n",
       " 639: 'Treatment',\n",
       " 640: 'Nature',\n",
       " 641: 'estimated',\n",
       " 642: 'duration',\n",
       " 643: 'treatments',\n",
       " 644: 'Job',\n",
       " 645: 'description',\n",
       " 646: 'is',\n",
       " 647: 'attached',\n",
       " 648: 'if',\n",
       " 649: 'checked',\n",
       " 650: 'here',\n",
       " 651: 'address',\n",
       " 652: 'Care',\n",
       " 653: 'DETAILS',\n",
       " 654: 'Dates',\n",
       " 655: 'including',\n",
       " 656: 'Confinement',\n",
       " 657: 'please',\n",
       " 658: 'provide',\n",
       " 659: 'following',\n",
       " 660: 'advice',\n",
       " 661: 'stop',\n",
       " 662: 'working',\n",
       " 663: 'what',\n",
       " 664: 'date',\n",
       " 665: 'Mgmt',\n",
       " 666: 'Svc',\n",
       " 667: 'Applicable',\n",
       " 668: 'Deductions',\n",
       " 669: 'Schedule',\n",
       " 670: 'Per',\n",
       " 671: 'Week',\n",
       " 672: 'Sick',\n",
       " 673: 'Variable',\n",
       " 674: 'Sunday',\n",
       " 675: 'Monday',\n",
       " 676: 'Tuesday',\n",
       " 677: 'Wednesday',\n",
       " 678: 'Thursday',\n",
       " 679: 'Friday',\n",
       " 680: 'Saturday',\n",
       " 681: 'SHORT',\n",
       " 682: 'TERM',\n",
       " 683: 'DISABILITY',\n",
       " 684: 'Hospitalized',\n",
       " 685: 'explain',\n",
       " 686: 'last',\n",
       " 687: 'office',\n",
       " 688: 'next',\n",
       " 689: 'Height',\n",
       " 690: 'Weight',\n",
       " 691: 'Secondary',\n",
       " 692: 'Has',\n",
       " 693: 'been',\n",
       " 694: 'hospitalized',\n",
       " 695: 'performed',\n",
       " 696: 'procedure',\n",
       " 697: 'was',\n",
       " 698: 'Functional',\n",
       " 699: 'Capacity',\n",
       " 700: 'Restrictions',\n",
       " 701: 'ELIZABETH',\n",
       " 702: 'EDGEWOOD',\n",
       " 703: 'FACESHEET',\n",
       " 704: 'MRN',\n",
       " 705: 'Sex',\n",
       " 706: 'Demographics',\n",
       " 707: 'SSN',\n",
       " 708: 'Reg',\n",
       " 709: 'Verified',\n",
       " 710: 'PCP',\n",
       " 711: 'Renew',\n",
       " 712: 'Admission',\n",
       " 713: 'Admitting',\n",
       " 714: 'Larkin',\n",
       " 715: 'John',\n",
       " 716: 'MD',\n",
       " 717: 'Elective',\n",
       " 718: 'Incomplete',\n",
       " 719: 'Area',\n",
       " 720: 'SERVICE',\n",
       " 721: 'AREA',\n",
       " 722: 'EDG',\n",
       " 723: 'SC',\n",
       " 724: 'CRESTVIEW',\n",
       " 725: 'Discharged',\n",
       " 726: 'Confirmed',\n",
       " 727: 'HOSE',\n",
       " 728: 'ital',\n",
       " 729: 'Acct',\n",
       " 730: 'Class',\n",
       " 731: 'Same',\n",
       " 732: 'Guarantor',\n",
       " 733: 'Relation',\n",
       " 734: 'Pt',\n",
       " 735: 'SEH',\n",
       " 736: 'Precert',\n",
       " 737: 'Subscriber',\n",
       " 738: 'Operative',\n",
       " 739: 'Brief',\n",
       " 740: 'Op',\n",
       " 741: 'Note',\n",
       " 742: 'OP',\n",
       " 743: 'Adm',\n",
       " 744: 'continued',\n",
       " 745: 'Author',\n",
       " 746: 'J',\n",
       " 747: 'Filed',\n",
       " 748: 'Editor',\n",
       " 749: 'Elizabeth',\n",
       " 750: 'Healthcare',\n",
       " 751: 'NOTE',\n",
       " 752: 'Body',\n",
       " 753: 'mass',\n",
       " 754: 'index',\n",
       " 755: 'PROCEDURE',\n",
       " 756: 'SURGEON',\n",
       " 757: 'Role',\n",
       " 758: 'ANESTHESIA',\n",
       " 759: 'General',\n",
       " 760: 'SPECIMENS',\n",
       " 761: 'specimens',\n",
       " 762: 'log',\n",
       " 763: 'ESTIMATED',\n",
       " 764: 'BLOOD',\n",
       " 765: 'LOSS',\n",
       " 766: 'PROC',\n",
       " 767: 'COURSE',\n",
       " 768: 'PACU',\n",
       " 769: 'OPERATION',\n",
       " 770: 'Broken',\n",
       " 771: 'big',\n",
       " 772: 'toe',\n",
       " 773: 'foot',\n",
       " 774: 'Todd',\n",
       " 775: 'Francis',\n",
       " 776: 'Podiatrist',\n",
       " 777: 'Ryan',\n",
       " 778: 'Kish',\n",
       " 779: 'Toledo',\n",
       " 780: 'ER',\n",
       " 781: 'Xray',\n",
       " 782: 'July',\n",
       " 783: 'Paramount',\n",
       " 784: 'ProMedica',\n",
       " 785: 'LINE',\n",
       " 786: 'AUTHORIZATION',\n",
       " 787: 'NO',\n",
       " 788: 'CODE',\n",
       " 789: 'MODIFIER',\n",
       " 790: 'DIAGNOSIS',\n",
       " 791: 'EXPLAIN',\n",
       " 792: 'BILLED',\n",
       " 793: 'PARAMOUNT',\n",
       " 794: 'LT',\n",
       " 795: 'Indicates',\n",
       " 796: 'additional',\n",
       " 797: 'information',\n",
       " 798: 'C',\n",
       " 799: 'DPM',\n",
       " 800: 'EMS',\n",
       " 801: 'Christine',\n",
       " 802: 'Nolen',\n",
       " 803: 'Waukesha',\n",
       " 804: 'Memorial',\n",
       " 805: 'Cleaning',\n",
       " 806: 'xray',\n",
       " 807: 'bandage',\n",
       " 808: 'MONTANO',\n",
       " 809: 'CI',\n",
       " 810: 'Web',\n",
       " 811: 'user',\n",
       " 812: 'notes',\n",
       " 813: 'statements',\n",
       " 814: 'PROHEALTH',\n",
       " 815: 'CARE',\n",
       " 816: 'SERVICES',\n",
       " 817: 'GUARANTOR',\n",
       " 818: 'NAME',\n",
       " 819: 'DESCRIPTION',\n",
       " 820: 'PAYMENTS',\n",
       " 821: 'ADJUSTMENTS',\n",
       " 822: 'PATIENTS',\n",
       " 823: 'INVOICE',\n",
       " 824: 'NUMBER',\n",
       " 825: 'Previous',\n",
       " 826: 'CURRENT',\n",
       " 827: 'TOTAL',\n",
       " 828: 'VISIT',\n",
       " 829: 'PAY',\n",
       " 830: 'THIS',\n",
       " 831: 'RETURN',\n",
       " 832: 'PORTION',\n",
       " 833: 'WITH',\n",
       " 834: 'YOUR',\n",
       " 835: 'MASTERCARD',\n",
       " 836: 'DISCOVER',\n",
       " 837: 'VISA',\n",
       " 838: 'Enclosed',\n",
       " 839: 'CHECK',\n",
       " 840: 'PAYABLE',\n",
       " 841: 'EMERGENCY',\n",
       " 842: 'MEDICAL',\n",
       " 843: 'ASSOCIATES',\n",
       " 844: 'PHONE',\n",
       " 845: 'ADDRESSEE',\n",
       " 846: 'INDUSTRIAL',\n",
       " 847: 'LOOP',\n",
       " 848: 'ACCOUNT',\n",
       " 849: \"'S\",\n",
       " 850: 'DEPT',\n",
       " 851: 'PPO',\n",
       " 852: 'ADJ',\n",
       " 853: 'UMR',\n",
       " 854: 'FISERV',\n",
       " 855: 'WI',\n",
       " 856: 'ON',\n",
       " 857: 'OVER',\n",
       " 858: 'DAYS',\n",
       " 859: 'STMT',\n",
       " 860: 'DOCTOR',\n",
       " 861: 'LEGEND',\n",
       " 862: 'NOLEN',\n",
       " 863: 'CHRISTINE',\n",
       " 864: 'D',\n",
       " 865: 'COMMENTS',\n",
       " 866: 'PRIMARY',\n",
       " 867: 'SECONDARY',\n",
       " 868: 'Concussion',\n",
       " 869: 'Souha',\n",
       " 870: 'Hakim',\n",
       " 871: 'MedExpress',\n",
       " 872: 'Codes',\n",
       " 873: 'Urgent',\n",
       " 874: 'Clairn',\n",
       " 875: 'ME',\n",
       " 876: 'Seen',\n",
       " 877: 'Vijay',\n",
       " 878: 'Patel',\n",
       " 879: 'Holder',\n",
       " 880: 'Qty',\n",
       " 881: 'Clinical',\n",
       " 882: 'Chief',\n",
       " 883: 'Penicillins',\n",
       " 884: 'Rash',\n",
       " 885: 'Taken',\n",
       " 886: 'BP',\n",
       " 887: 'mmHg',\n",
       " 888: 'PULSE',\n",
       " 889: 'bpm',\n",
       " 890: 'RESP',\n",
       " 891: 'TEMP',\n",
       " 892: 'WEIGHT',\n",
       " 893: 'lb',\n",
       " 894: 'ft',\n",
       " 895: 'BMI',\n",
       " 896: 'SAT',\n",
       " 897: 'Current',\n",
       " 898: 'Meds',\n",
       " 899: 'ACTIVE',\n",
       " 900: 'albuterol',\n",
       " 901: 'sulfate',\n",
       " 902: 'Encounter',\n",
       " 903: 'Progress',\n",
       " 904: 'none',\n",
       " 905: 'Subjective',\n",
       " 906: 'History',\n",
       " 907: 'provided',\n",
       " 908: 'dad',\n",
       " 909: 'interpreter',\n",
       " 910: 'used',\n",
       " 911: 'presents',\n",
       " 912: 'urgent',\n",
       " 913: 'care',\n",
       " 914: 'Review',\n",
       " 915: 'Systems',\n",
       " 916: 'Cardiovascular',\n",
       " 917: 'Negative',\n",
       " 918: 'chest',\n",
       " 919: 'Skin',\n",
       " 920: 'Neurological',\n",
       " 921: 'headaches',\n",
       " 922: 'Objective',\n",
       " 923: 'HENT',\n",
       " 924: 'Right',\n",
       " 925: 'Ear',\n",
       " 926: 'Tympanic',\n",
       " 927: 'membrane',\n",
       " 928: 'normal',\n",
       " 929: 'Nose',\n",
       " 930: 'Oropharynx',\n",
       " 931: 'clear',\n",
       " 932: 'Eyes',\n",
       " 933: 'Conjunctivae',\n",
       " 934: 'EOM',\n",
       " 935: 'Neck',\n",
       " 936: 'supple',\n",
       " 937: 'rigidity',\n",
       " 938: 'murmur',\n",
       " 939: 'heard',\n",
       " 940: 'Lymphadenopathy',\n",
       " 941: 'He',\n",
       " 942: 'has',\n",
       " 943: 'cervical',\n",
       " 944: 'adenopathy',\n",
       " 945: 'alert',\n",
       " 946: 'warm',\n",
       " 947: 'noted',\n",
       " 948: 'Assessment',\n",
       " 949: 'advise',\n",
       " 950: 'SS',\n",
       " 951: 'Penobscot',\n",
       " 952: 'Community',\n",
       " 953: 'Suite',\n",
       " 954: 'Medicine',\n",
       " 955: 'Mental',\n",
       " 956: 'FAX',\n",
       " 957: 'Transmission',\n",
       " 958: 'Sheet',\n",
       " 959: 'FROM',\n",
       " 960: 'Ext',\n",
       " 961: 'number',\n",
       " 962: 'pages',\n",
       " 963: 'cover',\n",
       " 964: 'page',\n",
       " 965: 'Thank',\n",
       " 966: 'Revised',\n",
       " 967: 'Imaging',\n",
       " 968: 'MaineCare',\n",
       " 969: 'FQHC',\n",
       " 970: 'Low',\n",
       " 971: 'JT',\n",
       " 972: 'Rt',\n",
       " 973: 'Erin',\n",
       " 974: 'Barker',\n",
       " 975: 'Joseph',\n",
       " 976: 'Ordering',\n",
       " 977: 'BARKER',\n",
       " 978: 'ERIN',\n",
       " 979: 'RIGHT',\n",
       " 980: 'KNEE',\n",
       " 981: 'INTERCONDYLAR',\n",
       " 982: 'SPACE',\n",
       " 983: 'ACL',\n",
       " 984: 'PCL',\n",
       " 985: 'intact',\n",
       " 986: 'Unremarkable',\n",
       " 987: 'marrow',\n",
       " 988: 'signal',\n",
       " 989: 'T',\n",
       " 990: 'DICTATION',\n",
       " 991: 'LOCATION',\n",
       " 992: 'MPSYNERNET',\n",
       " 993: 'Reading',\n",
       " 994: 'KASPER',\n",
       " 995: 'JARED',\n",
       " 996: 'Down',\n",
       " 997: 'East',\n",
       " 998: 'Orthopedics',\n",
       " 999: 'Sports',\n",
       " ...}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_word_tokens=len(sorted(list(word2int)))\n",
    "num_char_tokens=len(sorted(list(char2int)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split the data into training and testing sentences\n",
    "input_texts, test_input_texts, target_texts, test_target_texts  = train_test_split(input_texts, target_texts, test_size = 0.15, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vecotrize hierarichal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_char_input_data, decoder_word_input_data, decoder_word_target_data = vectorize_hier_data(input_texts, \n",
    "                                                                                                target_texts, \n",
    "                                                                                                max_words_seq_len, \n",
    "                                                                                                max_chars_seq_len, \n",
    "                                                                                                num_char_tokens, \n",
    "                                                                                                num_word_tokens, \n",
    "                                                                                                word2int, \n",
    "                                                                                                char2int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Hierarichal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, None)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, None, 126)         15876     \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection [(None, None, 1024), (Non 2617344   \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 1024)              0         \n",
      "=================================================================\n",
      "Total params: 2,633,220\n",
      "Trainable params: 2,617,344\n",
      "Non-trainable params: 15,876\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_word_embedding_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder-decoder  model:\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 11, 20)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 11, 1024)     2633220     input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            (None, 11)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) [(None, 11, 1024), ( 6295552     time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 11, 2048)     7182336     input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 1024)         0           bidirectional_2[0][1]            \n",
      "                                                                 bidirectional_2[0][3]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 1024)         0           bidirectional_2[0][2]            \n",
      "                                                                 bidirectional_2[0][4]            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   [(None, 11, 1024), ( 12587008    embedding_3[0][0]                \n",
      "                                                                 concatenate_4[0][0]              \n",
      "                                                                 concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dot_3 (Dot)                     (None, 11, 11)       0           lstm_4[0][0]                     \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 11, 11)       0           dot_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dot_4 (Dot)                     (None, 11, 1024)     0           activation_1[0][0]               \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 11, 2048)     0           dot_4[0][0]                      \n",
      "                                                                 lstm_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 11, 3507)     7185843     concatenate_6[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 35,883,959\n",
      "Trainable params: 35,868,083\n",
      "Non-trainable params: 15,876\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:73: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=[<tf.Tenso...)`\n"
     ]
    }
   ],
   "source": [
    "model, encoder_model, decoder_model = build_hier_model(encoder_word_embedding_model=encoder_word_embedding_model, \n",
    "                              max_words_seq_len=max_words_seq_len,\n",
    "                              max_char_seq_len=max_chars_seq_len,\n",
    "                              num_word_tokens=num_word_tokens,\n",
    "                              num_char_tokens=num_char_tokens, \n",
    "                              latent_dim=latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 11, 20)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 11, 1024)     2633220     input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) [(None, 11, 1024), ( 6295552     time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 1024)         0           bidirectional_2[0][1]            \n",
      "                                                                 bidirectional_2[0][3]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 1024)         0           bidirectional_2[0][2]            \n",
      "                                                                 bidirectional_2[0][4]            \n",
      "==================================================================================================\n",
      "Total params: 8,928,772\n",
      "Trainable params: 8,912,896\n",
      "Non-trainable params: 15,876\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 11)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 11, 2048)     7182336     input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_9 (InputLayer)            (None, 1024)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_10 (InputLayer)           (None, 1024)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   [(None, 11, 1024), ( 12587008    embedding_3[0][0]                \n",
      "                                                                 input_9[0][0]                    \n",
      "                                                                 input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            (None, 11, 1024)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dot_3 (Dot)                     (None, 11, 11)       0           lstm_4[1][0]                     \n",
      "                                                                 input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 11, 11)       0           dot_3[1][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dot_4 (Dot)                     (None, 11, 1024)     0           activation_1[1][0]               \n",
      "                                                                 input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 11, 2048)     0           dot_4[1][0]                      \n",
      "                                                                 lstm_4[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 11, 3507)     7185843     concatenate_6[1][0]              \n",
      "==================================================================================================\n",
      "Total params: 26,955,187\n",
      "Trainable params: 26,955,187\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Hierarichal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6039 samples, validate on 1510 samples\n",
      "Epoch 1/50\n",
      "6039/6039 [==============================] - 20s 3ms/step - loss: 3.2150 - categorical_accuracy: 0.1699 - val_loss: 2.5169 - val_categorical_accuracy: 0.2454\n",
      "\n",
      "Epoch 00001: val_categorical_accuracy improved from -inf to 0.24543, saving model to best_hier_model-11-20.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/keras/engine/network.py:872: UserWarning: Layer lstm_4 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_4/concat:0' shape=(?, 1024) dtype=float32>, <tf.Tensor 'concatenate_5/concat:0' shape=(?, 1024) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50\n",
      "6039/6039 [==============================] - 17s 3ms/step - loss: 1.8730 - categorical_accuracy: 0.3015 - val_loss: 1.9169 - val_categorical_accuracy: 0.3329\n",
      "\n",
      "Epoch 00002: val_categorical_accuracy improved from 0.24543 to 0.33289, saving model to best_hier_model-11-20.hdf5\n",
      "Epoch 3/50\n",
      "6039/6039 [==============================] - 17s 3ms/step - loss: 1.2125 - categorical_accuracy: 0.3744 - val_loss: 1.7871 - val_categorical_accuracy: 0.3670\n",
      "\n",
      "Epoch 00003: val_categorical_accuracy improved from 0.33289 to 0.36703, saving model to best_hier_model-11-20.hdf5\n",
      "Epoch 4/50\n",
      "6039/6039 [==============================] - 17s 3ms/step - loss: 0.8392 - categorical_accuracy: 0.4287 - val_loss: 1.7734 - val_categorical_accuracy: 0.3841\n",
      "\n",
      "Epoch 00004: val_categorical_accuracy improved from 0.36703 to 0.38410, saving model to best_hier_model-11-20.hdf5\n",
      "Epoch 5/50\n",
      "6039/6039 [==============================] - 17s 3ms/step - loss: 0.6042 - categorical_accuracy: 0.4713 - val_loss: 1.7654 - val_categorical_accuracy: 0.3898\n",
      "\n",
      "Epoch 00005: val_categorical_accuracy improved from 0.38410 to 0.38984, saving model to best_hier_model-11-20.hdf5\n",
      "Epoch 6/50\n",
      "6039/6039 [==============================] - 17s 3ms/step - loss: 0.4924 - categorical_accuracy: 0.4939 - val_loss: 1.7433 - val_categorical_accuracy: 0.4063\n",
      "\n",
      "Epoch 00006: val_categorical_accuracy improved from 0.38984 to 0.40635, saving model to best_hier_model-11-20.hdf5\n",
      "Epoch 7/50\n",
      "6039/6039 [==============================] - 17s 3ms/step - loss: 0.3960 - categorical_accuracy: 0.5133 - val_loss: 1.7319 - val_categorical_accuracy: 0.4084\n",
      "\n",
      "Epoch 00007: val_categorical_accuracy improved from 0.40635 to 0.40840, saving model to best_hier_model-11-20.hdf5\n",
      "Epoch 8/50\n",
      "6039/6039 [==============================] - 17s 3ms/step - loss: 0.3415 - categorical_accuracy: 0.5252 - val_loss: 1.7767 - val_categorical_accuracy: 0.4079\n",
      "\n",
      "Epoch 00008: val_categorical_accuracy did not improve from 0.40840\n",
      "Epoch 9/50\n",
      "6039/6039 [==============================] - 17s 3ms/step - loss: 0.3343 - categorical_accuracy: 0.5282 - val_loss: 1.7614 - val_categorical_accuracy: 0.4131\n",
      "\n",
      "Epoch 00009: val_categorical_accuracy improved from 0.40840 to 0.41314, saving model to best_hier_model-11-20.hdf5\n",
      "Epoch 10/50\n",
      "6039/6039 [==============================] - 17s 3ms/step - loss: 0.3018 - categorical_accuracy: 0.5356 - val_loss: 1.8211 - val_categorical_accuracy: 0.4141\n",
      "\n",
      "Epoch 00010: val_categorical_accuracy improved from 0.41314 to 0.41410, saving model to best_hier_model-11-20.hdf5\n",
      "Epoch 11/50\n",
      " 448/6039 [=>............................] - ETA: 14s - loss: 0.2514 - categorical_accuracy: 0.5383"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-86cc20843ace>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m           \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m           \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m           shuffle=True)\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2664\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m                                 session)\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 50\n",
    "lr = 0.01\n",
    "\n",
    "model.compile(optimizer=optimizers.Adam(lr=lr), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "#filepath=\"weights-improvement-{epoch:02d}-{val_categorical_accuracy:.2f}.hdf5\"\n",
    "filepath=\"best_hier_model-{}-{}.hdf5\".format(max_words_seq_len,max_chars_seq_len) # Save only the best model for inference step, as saving the epoch and metric might confuse the inference function which model to use\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_categorical_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "tbCallBack = TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "callbacks_list = [checkpoint, tbCallBack]\n",
    "#callbacks_list = [checkpoint, tbCallBack, lrate]\n",
    "model.fit([encoder_char_input_data, decoder_word_input_data], decoder_word_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=callbacks_list,\n",
    "          validation_split=0.2,\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\t ED Disposition Discharge UNK Yes \\n UNK UNK UNK UNK '"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "decode_gt_sequence(decoder_word_input_data[idx:idx+1], int2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'Ail', 'Other', 'Languages', 'Contact']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(input_texts[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\tAll Other Languages Contact\\n'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_texts[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\t', 'All', 'Other', 'Languages', 'Contact', '\\n']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_lst =  word_tokenize(target_texts[idx])\n",
    "words_lst.insert(0, '\\t')\n",
    "words_lst.append('\\n')\n",
    "words_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ED Disposition Discharge UNK Yes \\n UNK UNK UNK UNK UNK '"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_gt_sequence(np.argmax(decoder_word_target_data[idx:idx+1], axis=-1), int2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ED systems Diagnosis Contact \\n \\n \\n \\n \\n \\n \\n '"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = model.predict([encoder_char_input_data[idx:idx+1], decoder_word_input_data[idx-1:idx]])\n",
    "y.shape\n",
    "#sampled_token_index = np.argmax(y[0, -1, :])\n",
    "d = np.argmax(y, axis=-1)\n",
    "d.shape\n",
    "decode_gt_sequence(d, int2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ED Departure  \n"
     ]
    }
   ],
   "source": [
    "input_seq = encoder_char_input_data[idx:idx+1]\n",
    "decoded_sentence, _ = decode_sequence(input_seq, encoder_model, decoder_model, max_words_seq_len, num_word_tokens, int2word)\n",
    "print(decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: follow up with where when\n",
      "GT sentence: Follow Up With: Where: When:\n",
      "Decoded sentence: Follow Up With  \n",
      "-\n",
      "Input sentence: Trace number:\n",
      "GT sentence: Trace number:\n",
      "Decoded sentence: Trace number  \n",
      "-\n",
      "Input sentence: Last Nama Sufix First Name MI_\n",
      "GT sentence: Last Name Suffix First Name MI\n",
      "Decoded sentence: Last Name Suffix First Name MI  \n",
      "-\n",
      "Input sentence: ☐ Long Tarn Disabllity\n",
      "GT sentence: ☐ Long Term Disability\n",
      "Decoded sentence: ☐ Short Term Disability  \n",
      "-\n",
      "Input sentence: ATTENDING PHYSICIAN STATEMENT (Continued)\n",
      "GT sentence: ATTENDING PHYSICIAN STATEMENT (Continued)\n",
      "Decoded sentence: ATTENDING PHYSICIAN STATEMENT  \n",
      "-\n",
      "Input sentence: fraud statements reviewed and electronically\n",
      "GT sentence: Fraud Statements Reviewed and Electronically\n",
      "Decoded sentence: Fraud Statements Reviewed  \n",
      "-\n",
      "Input sentence: medexpress\n",
      "GT sentence: MedExpress\n",
      "Decoded sentence: Medications  \n",
      "-\n",
      "Input sentence: Account #\n",
      "GT sentence: Account #\n",
      "Decoded sentence: Precert Policy  \n",
      "-\n",
      "Input sentence: None available.\n",
      "GT sentence: None available.\n",
      "Decoded sentence: None available County Hospital  \n",
      "-\n",
      "Input sentence: H B (53 years).\n",
      "GT sentence: DOB (53 years)\n",
      "Decoded sentence: DOB DOB  \n",
      "-\n",
      "Input sentence: c. Signature of Attending Physician\n",
      "GT sentence: C. Signature of Attending Physician\n",
      "Decoded sentence: B Attending Regional Hospital  \n",
      "-\n",
      "Input sentence: — [Result _ Daté Time |\n",
      "GT sentence: Result Date Time\n",
      "Decoded sentence: Result Date Time  \n",
      "-\n",
      "Input sentence: north huntingdon, pa, \n",
      "GT sentence: NORTH HUNINGTON,PA,\n",
      "Decoded sentence: NORTH HUNINGTON  \n",
      "-\n",
      "Input sentence: Admitting. Srowder Larkin. John J. MD\n",
      "GT sentence: Admitting Provider Larkin. John J. MD\n",
      "Decoded sentence: Admitting NY  \n",
      "-\n",
      "Input sentence: Diagnosis\n",
      "GT sentence: Diagnosis\n",
      "Decoded sentence: Diagnosis  \n",
      "-\n",
      "Input sentence: Diabetes Mellitus, Typa T1\n",
      "GT sentence: Diabetes Mellitus, Type II\n",
      "Decoded sentence: Diabetes Mellitus  \n",
      "-\n",
      "Input sentence: Date of Birlh (mmuddryy)\n",
      "GT sentence: Date of Birth (mm/dd/yy)\n",
      "Decoded sentence: Date of Birth Date  \n",
      "-\n",
      "Input sentence: G. Tax Conslderations\n",
      "GT sentence: G. Tax Considerations\n",
      "Decoded sentence: ☐ Considerations  \n",
      "-\n",
      "Input sentence: postal code 26330\n",
      "GT sentence: Postal Code: 26330\n",
      "Decoded sentence: Postal Code PAT  \n",
      "-\n",
      "Input sentence: 2 19 Part-time hours per day\n",
      "GT sentence: Part-time hours per day\n",
      "Decoded sentence: ☒ day 's Signature  \n",
      "-\n",
      "Input sentence: Employee Health Medical Category [FML] — blood\n",
      "GT sentence: Employee Health Medical Category [FML] - blood\n",
      "Decoded sentence: Employee Health Medical Category  \n",
      "-\n",
      "Input sentence: Address Line 2:\n",
      "GT sentence: Address Line 2:\n",
      "Decoded sentence: Address Line  \n",
      "-\n",
      "Input sentence: Group Policy #:\n",
      "GT sentence: Group Policy #:\n",
      "Decoded sentence: Group Policy Rd  \n",
      "-\n",
      "Input sentence: Physician tnformation\n",
      "GT sentence: Physician Information\n",
      "Decoded sentence: Physician 's signature  \n",
      "-\n",
      "Input sentence: Medical Provider Information , Physician\n",
      "GT sentence: Medical Provider Information - Physician\n",
      "Decoded sentence: Medical Provider Information  \n",
      "-\n",
      "Input sentence: Cellular Telephone Number\n",
      "GT sentence: Cellular Telephone Number\n",
      "Decoded sentence: Cellular Telephone Number  \n",
      "-\n",
      "Input sentence: Initial Vitals & Measurements:\n",
      "GT sentence: Initial Vitals & Measurements:\n",
      "Decoded sentence: Initial Vitals & Measurements & Coverage  \n",
      "-\n",
      "Input sentence: MRI Yes El No Date: (mmlddlyy) I 11?, \n",
      "GT sentence: MRI Yes No Date: (mm/dd/yy)\n",
      "Decoded sentence: Health  \n",
      "-\n",
      "Input sentence: status Complete\n",
      "GT sentence: Status Complete\n",
      "Decoded sentence: Arrival Date and Thursday  \n",
      "-\n",
      "Input sentence: Arrival Date: Thursday, March 01, 2018\n",
      "GT sentence: Arrival Date Thursday, March 01, 2018\n",
      "Decoded sentence: through  \n",
      "-\n",
      "Input sentence: through\n",
      "GT sentence: through\n",
      "Decoded sentence: Date of LEAVES  \n",
      "-\n",
      "Input sentence: Date of Birth _\n",
      "GT sentence: Date of Birth\n",
      "Decoded sentence: Total Amount Billed by  \n",
      "-\n",
      "Input sentence: TWIN CITIES ORTHOPEDICS PA \n",
      "GT sentence: TWIN CITIES ORTHOPEDICS, PA\n",
      "Decoded sentence: OPERATION RECORD  \n",
      "-\n",
      "Input sentence: ! [3 1/18/2018\n",
      "GT sentence: 1/18/2018\n",
      "Decoded sentence: 's ☒ Employee  \n",
      "-\n",
      "Input sentence: (Name / Relationship)\n",
      "GT sentence: (Name / Relationship)\n",
      "Decoded sentence: Date of service  \n",
      "-\n",
      "Input sentence: Date: 02/05/2018\n",
      "GT sentence: Date: 02/05/2018\n",
      "Decoded sentence: Current Medications  \n",
      "-\n",
      "Input sentence: General Appearance:\n",
      "GT sentence: General Appearance:\n",
      "Decoded sentence: ☐ Life Insurance  \n",
      "-\n",
      "Input sentence: ☐ Life Insurance\n",
      "GT sentence: ☐ Life Insurance\n",
      "Decoded sentence: Expected  \n",
      "-\n",
      "Input sentence: (FLULAVAL)\n",
      "GT sentence: (FLULAVAL)\n",
      "Decoded sentence: Active Problems  \n",
      "-\n",
      "Input sentence: Active Problems\n",
      "GT sentence: Active Problems\n",
      "Decoded sentence: Fax Number  \n",
      "-\n",
      "Input sentence: Fax Number Len BBD _rowsy ho Joo\n",
      "GT sentence: Fax Number 330-480-3522\n",
      "Decoded sentence: Insured 's Signature  \n",
      "-\n",
      "Input sentence: 241 Robert ☒. Wilson Drive\n",
      "GT sentence: 241 Robert K. Wilson Drive\n",
      "Decoded sentence: • CHLORIDE is new  \n",
      "-\n",
      "Input sentence: RDW~CV — 12.70 Range: 11504.5 .. %\n",
      "GT sentence: • RDW-CV - 12.70 Range: 11.5-04.5 - %\n",
      "Decoded sentence: Paid Amount  \n",
      "-\n",
      "Input sentence: myWalieHealth\n",
      "GT sentence: myWakeHealth\n",
      "Decoded sentence: Westbury  \n",
      "-\n",
      "Input sentence: Westbury, NY 11590\n",
      "GT sentence: Westbury, NY 11590\n",
      "Decoded sentence: First Name MI  \n",
      "-\n",
      "Input sentence: First Name:\n",
      "GT sentence: First Name:\n",
      "Decoded sentence: Employee Wellness Benefit January  \n",
      "-\n",
      "Input sentence: Employee Wellness Benefi January 1, 2016\n",
      "GT sentence: Employee Wellness Benefit January 1, 2016\n",
      "Decoded sentence: Electronic Submission  \n",
      "-\n",
      "Input sentence: Electronic Submission\n",
      "GT sentence: Electronic Submission\n",
      "Decoded sentence: Executive Director  \n",
      "-\n",
      "Input sentence: Executive Director\n",
      "GT sentence: Executive Director\n",
      "Decoded sentence: Add another doctor  \n",
      "-\n",
      "Input sentence: Add doctors details — yes\n",
      "GT sentence: Add doctors details - yes\n",
      "Decoded sentence: All Other Languages Contact  \n",
      "-\n",
      "Input sentence: i Ail Other Languages Contact\n",
      "GT sentence: All Other Languages Contact\n",
      "Decoded sentence: ED Departure  \n",
      "-\n",
      "Input sentence: ED Disposition Discharge .' Yes\n",
      "GT sentence: ED Disposition Discharge: Yes\n",
      "Decoded sentence: Physical Therapist  \n",
      "-\n",
      "Input sentence: Physical Therapist\n",
      "GT sentence: Physical Therapist\n",
      "Decoded sentence: policy or policies  \n",
      "-\n",
      "Input sentence: pulicy or policies.\n",
      "GT sentence: policy or policies.\n",
      "Decoded sentence: Claimant Name  \n",
      "-\n",
      "Input sentence: Claimant Name: Andrea B Patterson-Fyffe\n",
      "GT sentence: Claimant Name: Andrea B Patterson-Fyffe\n",
      "Decoded sentence: First Documented  \n",
      "-\n",
      "Input sentence: First Documenterd:\n",
      "GT sentence: First Documented:\n",
      "Decoded sentence: LADY LAKE and  \n",
      "-\n",
      "Input sentence: LADY LAKE, FL 32150 . Boo ;\n",
      "GT sentence: LADY LAKE, FL 32159\n",
      "Decoded sentence: myWakeHealth  \n",
      "-\n",
      "Input sentence: jacksonville\n",
      "GT sentence: Jacksonville\n",
      "Decoded sentence: Fred of the  \n",
      "-\n",
      "Input sentence: Fred W, Ortmann, MD, FAAOS\n",
      "GT sentence: Fred W. Ortmann, MD, FAAOS\n",
      "Decoded sentence: Did you advise the patient to stop working  \n",
      "-\n",
      "Input sentence: Did you advise the patient to stop working?\n",
      "GT sentence: Did you advise the patient to stop working?\n",
      "Decoded sentence: All Other Languages Contact  \n",
      "-\n",
      "Input sentence: All Other Languages Contact\n",
      "GT sentence: All Other Languages Contact\n",
      "Decoded sentence: Estimated Patient Responsibility  \n",
      "-\n",
      "Input sentence: estimated patlent responsibility\n",
      "GT sentence: Estimated Patient Responsibility\n",
      "Decoded sentence: Duration less Health  \n",
      "-\n",
      "Input sentence: Duration less than 24 hours\n",
      "GT sentence: Duration less than 24 hours\n",
      "Decoded sentence: Ali E  \n",
      "-\n",
      "Input sentence: Al BE. Guy, M.D.\n",
      "GT sentence: Ali E. Guy, M.D.\n",
      "Decoded sentence: Pulse Clinic  \n",
      "-\n",
      "Input sentence: Pulse ' | BY't2/20 1026\n",
      "GT sentence: Pulse 89 02/20 1026\n",
      "Decoded sentence: Provider Last Name  \n",
      "-\n",
      "Input sentence: Procedure *ER Hand Right 3 Views\n",
      "GT sentence: Procedure *XR Hand Right 3 Views\n",
      "Decoded sentence: Responsible Provider Specialty  \n",
      "-\n",
      "Input sentence: Responsible Provider: D. Thompson McGuire MD\n",
      "GT sentence: Responsible Provider: D. Thompson McGuire MD\n",
      "Decoded sentence: Date of Birth Date  \n",
      "-\n",
      "Input sentence: Dﬂlﬂ of Birth (mﬂﬂdti'yy)\n",
      "GT sentence: Date of Birth (mm/dd/yy)\n",
      "Decoded sentence: Ordering Provider Information  \n",
      "-\n",
      "Input sentence: Ordering Provider: BARKER, ERIN\n",
      "GT sentence: Ordering Provider: BARKER, ERIN\n",
      "Decoded sentence: Participant ID  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: Participant ID:\n",
      "GT sentence: Participant ID:\n",
      "Decoded sentence: Printed Name  \n",
      "-\n",
      "Input sentence: Printed Name\n",
      "GT sentence: Printed Name\n",
      "Decoded sentence: Accountability Act of treatment  \n",
      "-\n",
      "Input sentence: Accountability Act (HIPAA) Privacy Rule.\n",
      "GT sentence: Accountability Act (HIPAA) Privacy Rule.\n",
      "Decoded sentence: CONTACT  \n",
      "-\n",
      "Input sentence: CONTACT\n",
      "GT sentence: CONTACT\n",
      "Decoded sentence: PHONE  \n",
      "-\n",
      "Input sentence: Policy #:\n",
      "GT sentence: Policy #:\n",
      "Decoded sentence: Provider Last Name  \n",
      "-\n",
      "Input sentence: - Cimhosis of the liver or Hepatitis B & C\n",
      "GT sentence: - Cirrhosis of the liver or Hepatitis B & C\n",
      "Decoded sentence: Medical Center  \n",
      "-\n",
      "Input sentence: Provider Last Name: Guy\n",
      "GT sentence: Provider Last Name: Guy\n",
      "Decoded sentence: policy or policies  \n",
      "-\n",
      "Input sentence: Myelogram: Cervical Lumbar\n",
      "GT sentence: [ ] Myelogram: Cervical Lumbar\n",
      "Decoded sentence: Jason R Bearden TABLET  \n",
      "-\n",
      "Input sentence: palicy or policies.\n",
      "GT sentence: policy or policies.\n",
      "Decoded sentence: ATTENDING PHYSICIAN STATEMENT  \n",
      "-\n",
      "Input sentence: Jason P, Rogers, MD {Board Eligible)\n",
      "GT sentence: Jason P. Rogers, MD (Board Eligible)\n",
      "Decoded sentence: F  \n",
      "-\n",
      "Input sentence: ATTENDING PHYSICIAN STATEMENT (PLEASE PRINT)\n",
      "GT sentence: ATTENDING PHYSICIAN STATEMENT (PLEASE PRINT)\n",
      "Decoded sentence: Medical Provider Information  \n",
      "-\n",
      "Input sentence: F (352) 350-2077\n",
      "GT sentence: F (352) 350-2077\n",
      "Decoded sentence: SHOW floor to HOSPITAL  \n",
      "-\n",
      "Input sentence: Medical Proxitler Information — Physician\n",
      "GT sentence: Medical Provider Information - Physician\n",
      "Decoded sentence: RADIOLOGY Clear Clear  \n",
      "-\n",
      "Input sentence: 401 West Decatur Street,\n",
      "GT sentence: 401 West Decatur Street\n",
      "Decoded sentence: State where hired  \n",
      "-\n",
      "Input sentence: RADIOLOGY REPORT - FINAL\n",
      "GT sentence: RADIOLOGY REPORT - FINAL\n",
      "Decoded sentence: ATTENDING PHYSICIAN STATEMENT  \n",
      "-\n",
      "Input sentence: e where superviead hic\n",
      "GT sentence: State where hired\n",
      "Decoded sentence: Date of Birth Date  \n",
      "-\n",
      "Input sentence: ATTENDING PHYSICIAN STATEMENT\n",
      "GT sentence: ATTENDING PHYSICIAN STATEMENT\n",
      "Decoded sentence: Total Monthly Premium  \n",
      "-\n",
      "Input sentence: Date of Birth (mm/ddiyyyy)\n",
      "GT sentence: Date of Birth (mm/dd/yyyy)\n",
      "Decoded sentence: Denial of any any any  \n",
      "-\n",
      "Input sentence: Total Monthly Premium:\n",
      "GT sentence: Total Monthly Premium:\n",
      "Decoded sentence: Total Payment next visit  \n",
      "-\n",
      "Input sentence: Denial of any significant medical history,\n",
      "GT sentence: Denial of any significant medical history,\n",
      "Decoded sentence: Insured Coverage Type Coverage Effective Date  \n",
      "-\n",
      "Input sentence: B/P Mean | | 93.0 02/20 1026\n",
      "GT sentence: B/P Mean 93.0 02/20 1026\n",
      "Decoded sentence: Not for FMLA Requests  \n",
      "-\n",
      "Input sentence: 241 Robert 2. Wilson Drive\n",
      "GT sentence: 241 Robert K. Wilson Drive\n",
      "Decoded sentence: Last Name Suffix First Name MI  \n",
      "-\n",
      "Input sentence: (Not for FMLA Requests)\n",
      "GT sentence: (Not for FMLA Requests)\n",
      "Decoded sentence: SEQ YOU  \n",
      "-\n",
      "Input sentence: Last Name ——. Suffix First Nama MI\n",
      "GT sentence: Last Name Suffix First Name MI\n",
      "Decoded sentence: Date of Service Next  \n",
      "-\n",
      "Input sentence: Patient's ability to perform: {Please Chaek)\n",
      "GT sentence: Patient's ability to perform: (Please Check)\n",
      "Decoded sentence: Telephone Number  \n",
      "-\n",
      "Input sentence: SEQ # * \n",
      "GT sentence: SEQ #: 4\n",
      "Decoded sentence: Total Monthly Premium  \n",
      "-\n",
      "Input sentence: (Name) ._ (Telephone Number)\n",
      "GT sentence: (Name)  (Telephone Number)\n",
      "Decoded sentence: Fax Number  \n",
      "-\n",
      "Input sentence: 1 {330) 729.2970 Tro\n",
      "GT sentence: (330) 729-2929\n",
      "Decoded sentence: Electronically Signed By Insured 's Signature  \n",
      "-\n",
      "Input sentence: Total Monthly Premium:\n",
      "GT sentence: Total Monthly Premium:\n",
      "Decoded sentence: Transaction reference number  \n",
      "-\n",
      "Input sentence: Fax: J36) 545-5020\n",
      "GT sentence: Fax: (336) 545-5020\n",
      "Decoded sentence: Fax  \n",
      "-\n",
      "Input sentence: Bectronlcally Ja ned By:\n",
      "GT sentence: Electronically Signed By:\n",
      "Decoded sentence: Spouse Wellness Benefit August  \n"
     ]
    }
   ],
   "source": [
    "# Sample output from train data\n",
    "decoded_sentences = []\n",
    "target_texts_ =  []\n",
    "for seq_index in range(100):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "\n",
    "    input_seq = encoder_char_input_data[seq_index: seq_index + 1]\n",
    "    target_seq = np.argmax(decoder_word_target_data[seq_index: seq_index + 1], axis=-1)\n",
    "    #print(target_seq)\n",
    "    \n",
    "    decoded_sentence, _ = decode_sequence(input_seq, encoder_model, decoder_model, max_words_seq_len, num_word_tokens, int2word)\n",
    "    target_text = target_texts[seq_index][1:-1]\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('GT sentence:', target_text)\n",
    "    print('Decoded sentence:', decoded_sentence)   \n",
    "    decoded_sentences.append(decoded_sentence)\n",
    "    target_texts_.append(target_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_char_input_data, decoder_word_input_data, decoder_word_target_data = vectorize_hier_data(test_input_texts, \n",
    "                                                                                                test_target_texts, \n",
    "                                                                                                max_words_seq_len, \n",
    "                                                                                                max_chars_seq_len, \n",
    "                                                                                                num_char_tokens, \n",
    "                                                                                                num_word_tokens, \n",
    "                                                                                                word2int, \n",
    "                                                                                                char2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: follow up with where when\n",
      "GT sentence: Follow Up With: Where: When:\n",
      "Decoded sentence: of a communicable or  \n",
      "-\n",
      "Input sentence: Trace number:\n",
      "GT sentence: Trace number:\n",
      "Decoded sentence: Commercial Insurance Adjustment  \n",
      "-\n",
      "Input sentence: Last Nama Sufix First Name MI_\n",
      "GT sentence: Last Name Suffix First Name MI\n",
      "Decoded sentence: Height  \n",
      "-\n",
      "Input sentence: ☐ Long Tarn Disabllity\n",
      "GT sentence: ☐ Long Term Disability\n",
      "Decoded sentence: Printed by a Hospital  \n",
      "-\n",
      "Input sentence: ATTENDING PHYSICIAN STATEMENT (Continued)\n",
      "GT sentence: ATTENDING PHYSICIAN STATEMENT (Continued)\n",
      "Decoded sentence: Diplomate of the  \n",
      "-\n",
      "Input sentence: fraud statements reviewed and electronically\n",
      "GT sentence: Fraud Statements Reviewed and Electronically\n",
      "Decoded sentence: Gender  \n",
      "-\n",
      "Input sentence: medexpress\n",
      "GT sentence: MedExpress\n",
      "Decoded sentence: City State Zip  \n",
      "-\n",
      "Input sentence: Account #\n",
      "GT sentence: Account #\n",
      "Decoded sentence: PATIENT  \n",
      "-\n",
      "Input sentence: None available.\n",
      "GT sentence: None available.\n",
      "Decoded sentence: Address Line  \n",
      "-\n",
      "Input sentence: H B (53 years).\n",
      "GT sentence: DOB (53 years)\n",
      "Decoded sentence: Date  \n",
      "-\n",
      "Input sentence: c. Signature of Attending Physician\n",
      "GT sentence: C. Signature of Attending Physician\n",
      "Decoded sentence: Patient Name  \n",
      "-\n",
      "Input sentence: — [Result _ Daté Time |\n",
      "GT sentence: Result Date Time\n",
      "Decoded sentence: ACCOUNT ACCOUNT ACCOUNT ACCOUNT ACCOUNT ACCOUNT ACCOUNT ACCOUNT ACCOUNT ACCOUNT ACCOUNT ACCOUNT  \n",
      "-\n",
      "Input sentence: north huntingdon, pa, \n",
      "GT sentence: NORTH HUNINGTON,PA,\n",
      "Decoded sentence: ☒ Long Term Disability  \n",
      "-\n",
      "Input sentence: Admitting. Srowder Larkin. John J. MD\n",
      "GT sentence: Admitting Provider Larkin. John J. MD\n",
      "Decoded sentence: AMOUNT DUE  \n",
      "-\n",
      "Input sentence: Diagnosis\n",
      "GT sentence: Diagnosis\n",
      "Decoded sentence: Health Carrier ID  \n",
      "-\n",
      "Input sentence: Diabetes Mellitus, Typa T1\n",
      "GT sentence: Diabetes Mellitus, Type II\n",
      "Decoded sentence: OPERATIVE REPORT  \n",
      "-\n",
      "Input sentence: Date of Birlh (mmuddryy)\n",
      "GT sentence: Date of Birth (mm/dd/yy)\n",
      "Decoded sentence: Telephone Number  \n",
      "-\n",
      "Input sentence: G. Tax Conslderations\n",
      "GT sentence: G. Tax Considerations\n",
      "Decoded sentence: through  \n",
      "-\n",
      "Input sentence: postal code 26330\n",
      "GT sentence: Postal Code: 26330\n",
      "Decoded sentence: Monday Complaint  \n",
      "-\n",
      "Input sentence: 2 19 Part-time hours per day\n",
      "GT sentence: Part-time hours per day\n",
      "Decoded sentence: Procedures  \n",
      "-\n",
      "Input sentence: Employee Health Medical Category [FML] — blood\n",
      "GT sentence: Employee Health Medical Category [FML] - blood\n",
      "Decoded sentence: Patient ID  \n",
      "-\n",
      "Input sentence: Address Line 2:\n",
      "GT sentence: Address Line 2:\n",
      "Decoded sentence: What To Do  \n",
      "-\n",
      "Input sentence: Group Policy #:\n",
      "GT sentence: Group Policy #:\n",
      "Decoded sentence: ED Departure  \n",
      "-\n",
      "Input sentence: Physician tnformation\n",
      "GT sentence: Physician Information\n",
      "Decoded sentence: unum  \n",
      "-\n",
      "Input sentence: Medical Provider Information , Physician\n",
      "GT sentence: Medical Provider Information - Physician\n",
      "Decoded sentence: Address Line  \n",
      "-\n",
      "Input sentence: Cellular Telephone Number\n",
      "GT sentence: Cellular Telephone Number\n",
      "Decoded sentence: Appeals Details Insured 's Hospital  \n",
      "-\n",
      "Input sentence: Initial Vitals & Measurements:\n",
      "GT sentence: Initial Vitals & Measurements:\n",
      "Decoded sentence: policy or policies  \n",
      "-\n",
      "Input sentence: MRI Yes El No Date: (mmlddlyy) I 11?, \n",
      "GT sentence: MRI Yes No Date: (mm/dd/yy)\n",
      "Decoded sentence: Shoulder Surgery  \n",
      "-\n",
      "Input sentence: status Complete\n",
      "GT sentence: Status Complete\n",
      "Decoded sentence: DOB  \n",
      "-\n",
      "Input sentence: Arrival Date: Thursday, March 01, 2018\n",
      "GT sentence: Arrival Date Thursday, March 01, 2018\n",
      "Decoded sentence: MD  \n",
      "-\n",
      "Input sentence: through\n",
      "GT sentence: through\n",
      "Decoded sentence: Country number  \n",
      "-\n",
      "Input sentence: Date of Birth _\n",
      "GT sentence: Date of Birth\n",
      "Decoded sentence: Skin warm & REHABILITATION  \n",
      "-\n",
      "Input sentence: TWIN CITIES ORTHOPEDICS PA \n",
      "GT sentence: TWIN CITIES ORTHOPEDICS, PA\n",
      "Decoded sentence: Physician 's signature  \n",
      "-\n",
      "Input sentence: ! [3 1/18/2018\n",
      "GT sentence: 1/18/2018\n",
      "Decoded sentence: Electronically Signed Insured 's Signature  \n",
      "-\n",
      "Input sentence: (Name / Relationship)\n",
      "GT sentence: (Name / Relationship)\n",
      "Decoded sentence: ☐ Other ☐  \n",
      "-\n",
      "Input sentence: Date: 02/05/2018\n",
      "GT sentence: Date: 02/05/2018\n",
      "Decoded sentence: Date of Service Encounter  \n",
      "-\n",
      "Input sentence: General Appearance:\n",
      "GT sentence: General Appearance:\n",
      "Decoded sentence: TRANSCRIBED BY  \n",
      "-\n",
      "Input sentence: ☐ Life Insurance\n",
      "GT sentence: ☐ Life Insurance\n",
      "Decoded sentence: EE Name  \n",
      "-\n",
      "Input sentence: (FLULAVAL)\n",
      "GT sentence: (FLULAVAL)\n",
      "Decoded sentence: What To last Day  \n",
      "-\n",
      "Input sentence: Active Problems\n",
      "GT sentence: Active Problems\n",
      "Decoded sentence: ROS Statements  \n",
      "-\n",
      "Input sentence: Fax Number Len BBD _rowsy ho Joo\n",
      "GT sentence: Fax Number 330-480-3522\n",
      "Decoded sentence: Confirmation of Coverage  \n",
      "-\n",
      "Input sentence: 241 Robert ☒. Wilson Drive\n",
      "GT sentence: 241 Robert K. Wilson Drive\n",
      "Decoded sentence: This Plan  \n",
      "-\n",
      "Input sentence: RDW~CV — 12.70 Range: 11504.5 .. %\n",
      "GT sentence: • RDW-CV - 12.70 Range: 11.5-04.5 - %\n",
      "Decoded sentence: Insured Social Security Number  \n",
      "-\n",
      "Input sentence: myWalieHealth\n",
      "GT sentence: myWakeHealth\n",
      "Decoded sentence: Patient Health and title  \n",
      "-\n",
      "Input sentence: Westbury, NY 11590\n",
      "GT sentence: Westbury, NY 11590\n",
      "Decoded sentence: see Negative  \n",
      "-\n",
      "Input sentence: First Name:\n",
      "GT sentence: First Name:\n",
      "Decoded sentence: Follow  \n",
      "-\n",
      "Input sentence: Employee Wellness Benefi January 1, 2016\n",
      "GT sentence: Employee Wellness Benefit January 1, 2016\n",
      "Decoded sentence: PICKENS COUNTY MEDICAL CENTER  \n",
      "-\n",
      "Input sentence: Electronic Submission\n",
      "GT sentence: Electronic Submission\n",
      "Decoded sentence: Dispense As Written  \n",
      "-\n",
      "Input sentence: Executive Director\n",
      "GT sentence: Executive Director\n",
      "Decoded sentence: Patient Gender ☒ Male ☐ Female  \n",
      "-\n",
      "Input sentence: Add doctors details — yes\n",
      "GT sentence: Add doctors details - yes\n",
      "Decoded sentence: ATTENDING PHYSICIAN STATEMENT  \n",
      "-\n",
      "Input sentence: i Ail Other Languages Contact\n",
      "GT sentence: All Other Languages Contact\n",
      "Decoded sentence: Height with missing  \n",
      "-\n",
      "Input sentence: ED Disposition Discharge .' Yes\n",
      "GT sentence: ED Disposition Discharge: Yes\n",
      "Decoded sentence: City  \n",
      "-\n",
      "Input sentence: Physical Therapist\n",
      "GT sentence: Physical Therapist\n",
      "Decoded sentence: Name of the  \n",
      "-\n",
      "Input sentence: pulicy or policies.\n",
      "GT sentence: policy or policies.\n",
      "Decoded sentence: Date of Disposition  \n",
      "-\n",
      "Input sentence: Claimant Name: Andrea B Patterson-Fyffe\n",
      "GT sentence: Claimant Name: Andrea B Patterson-Fyffe\n",
      "Decoded sentence: OPERATION RECORD  \n",
      "-\n",
      "Input sentence: First Documenterd:\n",
      "GT sentence: First Documented:\n",
      "Decoded sentence: Employee Wellness Benefit July  \n",
      "-\n",
      "Input sentence: LADY LAKE, FL 32150 . Boo ;\n",
      "GT sentence: LADY LAKE, FL 32159\n",
      "Decoded sentence: Hospital Name  \n",
      "-\n",
      "Input sentence: jacksonville\n",
      "GT sentence: Jacksonville\n",
      "Decoded sentence: Country  \n",
      "-\n",
      "Input sentence: Fred W, Ortmann, MD, FAAOS\n",
      "GT sentence: Fred W. Ortmann, MD, FAAOS\n",
      "Decoded sentence: Physician 's signature  \n",
      "-\n",
      "Input sentence: Did you advise the patient to stop working?\n",
      "GT sentence: Did you advise the patient to stop working?\n",
      "Decoded sentence: Printed Name Social Security Number  \n",
      "-\n",
      "Input sentence: All Other Languages Contact\n",
      "GT sentence: All Other Languages Contact\n",
      "Decoded sentence: ☐ Voluntary Benefits  \n",
      "-\n",
      "Input sentence: estimated patlent responsibility\n",
      "GT sentence: Estimated Patient Responsibility\n",
      "Decoded sentence: DT DT  \n",
      "-\n",
      "Input sentence: Duration less than 24 hours\n",
      "GT sentence: Duration less than 24 hours\n",
      "Decoded sentence: CMDOC  \n",
      "-\n",
      "Input sentence: Al BE. Guy, M.D.\n",
      "GT sentence: Ali E. Guy, M.D.\n",
      "Decoded sentence: Patient Name  \n",
      "-\n",
      "Input sentence: Pulse ' | BY't2/20 1026\n",
      "GT sentence: Pulse 89 02/20 1026\n",
      "Decoded sentence: Follow up in  \n",
      "-\n",
      "Input sentence: Procedure *ER Hand Right 3 Views\n",
      "GT sentence: Procedure *XR Hand Right 3 Views\n",
      "Decoded sentence: Skelaxin  \n",
      "-\n",
      "Input sentence: Responsible Provider: D. Thompson McGuire MD\n",
      "GT sentence: Responsible Provider: D. Thompson McGuire MD\n",
      "Decoded sentence: Assessment  \n",
      "-\n",
      "Input sentence: Dﬂlﬂ of Birth (mﬂﬂdti'yy)\n",
      "GT sentence: Date of Birth (mm/dd/yy)\n",
      "Decoded sentence: Age Date  \n",
      "-\n",
      "Input sentence: Ordering Provider: BARKER, ERIN\n",
      "GT sentence: Ordering Provider: BARKER, ERIN\n",
      "Decoded sentence: ATTENDING PHYSICIAN STATEMENT  \n",
      "-\n",
      "Input sentence: Participant ID:\n",
      "GT sentence: Participant ID:\n",
      "Decoded sentence: Employee Wellness Benefit January  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: Printed Name\n",
      "GT sentence: Printed Name\n",
      "Decoded sentence: Reviewed history from and and FOLLOWING  \n",
      "-\n",
      "Input sentence: Accountability Act (HIPAA) Privacy Rule.\n",
      "GT sentence: Accountability Act (HIPAA) Privacy Rule.\n",
      "Decoded sentence: Main Clinic  \n",
      "-\n",
      "Input sentence: CONTACT\n",
      "GT sentence: CONTACT\n",
      "Decoded sentence: Total Monthly Premium  \n",
      "-\n",
      "Input sentence: Policy #:\n",
      "GT sentence: Policy #:\n",
      "Decoded sentence: Jeffrey Ericksen by  \n",
      "-\n",
      "Input sentence: - Cimhosis of the liver or Hepatitis B & C\n",
      "GT sentence: - Cirrhosis of the liver or Hepatitis B & C\n",
      "Decoded sentence: Patient 's Signature  \n",
      "-\n",
      "Input sentence: Provider Last Name: Guy\n",
      "GT sentence: Provider Last Name: Guy\n",
      "Decoded sentence: Date of First Visit  \n",
      "-\n",
      "Input sentence: Myelogram: Cervical Lumbar\n",
      "GT sentence: [ ] Myelogram: Cervical Lumbar\n",
      "Decoded sentence: State  \n",
      "-\n",
      "Input sentence: palicy or policies.\n",
      "GT sentence: policy or policies.\n",
      "Decoded sentence: Group Policy Rd  \n",
      "-\n",
      "Input sentence: Jason P, Rogers, MD {Board Eligible)\n",
      "GT sentence: Jason P. Rogers, MD (Board Eligible)\n",
      "Decoded sentence: Signature of service  \n",
      "-\n",
      "Input sentence: ATTENDING PHYSICIAN STATEMENT (PLEASE PRINT)\n",
      "GT sentence: ATTENDING PHYSICIAN STATEMENT (PLEASE PRINT)\n",
      "Decoded sentence: PCP PARKER  \n",
      "-\n",
      "Input sentence: F (352) 350-2077\n",
      "GT sentence: F (352) 350-2077\n",
      "Decoded sentence: Insured 's Signature Electronically Signed  \n",
      "-\n",
      "Input sentence: Medical Proxitler Information — Physician\n",
      "GT sentence: Medical Provider Information - Physician\n",
      "Decoded sentence: Diagnosis Description  \n",
      "-\n",
      "Input sentence: 401 West Decatur Street,\n",
      "GT sentence: 401 West Decatur Street\n",
      "Decoded sentence: Voluntary Benefits MedSupport Insurance  \n",
      "-\n",
      "Input sentence: RADIOLOGY REPORT - FINAL\n",
      "GT sentence: RADIOLOGY REPORT - FINAL\n",
      "Decoded sentence: December December  \n",
      "-\n",
      "Input sentence: e where superviead hic\n",
      "GT sentence: State where hired\n",
      "Decoded sentence: Birth XR  \n",
      "-\n",
      "Input sentence: ATTENDING PHYSICIAN STATEMENT\n",
      "GT sentence: ATTENDING PHYSICIAN STATEMENT\n",
      "Decoded sentence: Signature  \n",
      "-\n",
      "Input sentence: Date of Birth (mm/ddiyyyy)\n",
      "GT sentence: Date of Birth (mm/dd/yyyy)\n",
      "Decoded sentence: Insured Coverage Type Coverage Effective Date  \n",
      "-\n",
      "Input sentence: Total Monthly Premium:\n",
      "GT sentence: Total Monthly Premium:\n",
      "Decoded sentence: and medications as heeded and Human  \n",
      "-\n",
      "Input sentence: Denial of any significant medical history,\n",
      "GT sentence: Denial of any significant medical history,\n",
      "Decoded sentence: Claim  \n",
      "-\n",
      "Input sentence: B/P Mean | | 93.0 02/20 1026\n",
      "GT sentence: B/P Mean 93.0 02/20 1026\n",
      "Decoded sentence: CPT Code  \n",
      "-\n",
      "Input sentence: 241 Robert 2. Wilson Drive\n",
      "GT sentence: 241 Robert K. Wilson Drive\n",
      "Decoded sentence: Name Specialty  \n",
      "-\n",
      "Input sentence: (Not for FMLA Requests)\n",
      "GT sentence: (Not for FMLA Requests)\n",
      "Decoded sentence: Worker 's  \n",
      "-\n",
      "Input sentence: Last Name ——. Suffix First Nama MI\n",
      "GT sentence: Last Name Suffix First Name MI\n",
      "Decoded sentence: NOTE  \n",
      "-\n",
      "Input sentence: Patient's ability to perform: {Please Chaek)\n",
      "GT sentence: Patient's ability to perform: (Please Check)\n",
      "Decoded sentence: Language Preference English  \n",
      "-\n",
      "Input sentence: SEQ # * \n",
      "GT sentence: SEQ #: 4\n",
      "Decoded sentence: TIER to Know  \n",
      "-\n",
      "Input sentence: (Name) ._ (Telephone Number)\n",
      "GT sentence: (Name)  (Telephone Number)\n",
      "Decoded sentence: ☒ Employee selected benefits  \n",
      "-\n",
      "Input sentence: 1 {330) 729.2970 Tro\n",
      "GT sentence: (330) 729-2929\n",
      "Decoded sentence: Was this may  \n",
      "-\n",
      "Input sentence: Total Monthly Premium:\n",
      "GT sentence: Total Monthly Premium:\n",
      "Decoded sentence: Claim Event Information  \n",
      "-\n",
      "Input sentence: Fax: J36) 545-5020\n",
      "GT sentence: Fax: (336) 545-5020\n",
      "Decoded sentence: How BY  \n",
      "-\n",
      "Input sentence: Bectronlcally Ja ned By:\n",
      "GT sentence: Electronically Signed By:\n",
      "Decoded sentence: Electronically Signed Insured Date  \n"
     ]
    }
   ],
   "source": [
    "# Sample output from val data\n",
    "decoded_sentences = []\n",
    "target_texts_ =  []\n",
    "for seq_index in range(100):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "\n",
    "    input_seq = encoder_char_input_data[seq_index: seq_index + 1]\n",
    "    target_seq = np.argmax(decoder_word_target_data[seq_index: seq_index + 1], axis=-1)\n",
    "    #print(target_seq)\n",
    "    \n",
    "    decoded_sentence, _ = decode_sequence(input_seq, encoder_model, decoder_model, max_words_seq_len, num_word_tokens, int2word)\n",
    "    target_text = target_texts[seq_index][1:-1]\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('GT sentence:', target_text)\n",
    "    print('Decoded sentence:', decoded_sentence)   \n",
    "    decoded_sentences.append(decoded_sentence)\n",
    "    target_texts_.append(target_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
