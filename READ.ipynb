{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c7-HTtj36-uK"
   },
   "source": [
    "<h1><center>READ: Recurrent Encoder Neural Language Model with Hierarichal Attention Decoder Fine tuning for Text Classification</center></h1>\n",
    "\n",
    "This work is inspired by two recent advances in NLP:\n",
    "\n",
    "1- ULMFiT: Transfer learning from pre-trained model for LM, fined tuned on NLP task\n",
    "\n",
    "2- HATT: Hierarichal Attention Classifier\n",
    "\n",
    "__What's in READ not in ULMFiT__\n",
    "- Hierarichy: which is good for sentiment prediction\n",
    "- Attention\n",
    "\n",
    "\n",
    "__What's in ULMFiT not in READ__\n",
    "- AWD-LSTM\n",
    "- Pre-trained LM on wikitext, then IMDB\n",
    "- LRFind\n",
    "- Bidirectional\n",
    "\n",
    "__References__\n",
    "https://github.com/fastai/fastai/blob/master/courses/dl2/imdb.ipynb\n",
    "https://arxiv.org/abs/1801.06146\n",
    "https://www.cs.cmu.edu/~hovy/papers/16HLT-hierarchical-attention-networks.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "YNhYpEEEa-8N",
    "outputId": "60ad49e2-aafd-4e23-e0a0-b91fb8ba83af"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmad/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import array\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import string\n",
    "os.environ['KERAS_BACKEND']='tensorflow'\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout, LSTM, GRU, Bidirectional, TimeDistributed\n",
    "from keras.models import Model, Sequential\n",
    "from keras.callbacks import ModelCheckpoint, Callback\n",
    "from keras.models import load_model\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from pickle import dump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uFJaE2bIUlK3"
   },
   "source": [
    "# IMDb data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "foFUAI1ZUxlk"
   },
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "CrGwHCNf66rG",
    "outputId": "63969b65-888c-47af-9b17-f3594ced0e4e"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "DATA_PATH=Path('./dat/')\n",
    "DATA_PATH.mkdir(exist_ok=True)\n",
    "#if not os.path.exists('./dat/aclImdb_v1.tar.gz'):\n",
    "if not os.path.exists('./dat/aclImdb'):\n",
    "    !curl -O http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz \n",
    "    !tar -xf aclImdb_v1.tar.gz -C {DATA_PATH}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Czebjx8TDH4c"
   },
   "outputs": [],
   "source": [
    "BOS = 'xbos'  # beginning-of-sentence tag\n",
    "FLD = 'xfld'  # data field tag\n",
    "UNK_TOK = '_UNK_'\n",
    "UNK_ID = 0 # 0 index is reserved for the UNK in both Keras Tokenizer and Embedding\n",
    "\n",
    "PATH=Path('./dat/aclImdb/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2aMAxTD7DM1F"
   },
   "outputs": [],
   "source": [
    "CLAS_PATH=Path('./dat/imdb_clas/')\n",
    "CLAS_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "LM_PATH=Path('./dat/imdb_lm/')\n",
    "LM_PATH.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zLieidwnEbAB"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "re1 = re.compile(r'  +')\n",
    "import html\n",
    "\n",
    "def fixup(x):\n",
    "    x = x.replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n",
    "        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n",
    "        '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>','u_n').replace(' @.@ ','.').replace(\n",
    "        ' @-@ ','-').replace('\\\\', ' \\\\ ')\n",
    "    return re1.sub(' ', html.unescape(x)) # Do not lower() so that capitalized words still hold a sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P1d7ncB_DPr3"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "CLASSES = ['neg', 'pos']#, 'unsup']\n",
    "\n",
    "def get_texts(path):\n",
    "    texts,labels = [],[]\n",
    "    for idx,label in enumerate(CLASSES):\n",
    "        for fname in (path/label).glob('*.*'):\n",
    "            texts.append(fixup(fname.open('r', encoding='utf-8').read()))\n",
    "            labels.append(idx)\n",
    "    return np.array(texts),np.array(labels)\n",
    "    #return texts, labels\n",
    "\n",
    "trn_texts,trn_labels = get_texts(PATH/'train')\n",
    "val_texts,val_labels = get_texts(PATH/'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "1Hm96jRZDeiN",
    "outputId": "7da86578-a39a-44f9-a706-d198113e411c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 25000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trn_texts),len(val_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1201
    },
    "colab_type": "code",
    "id": "KQVcGwZhDgJg",
    "outputId": "815d4f06-1e37-49e4-94b0-2847b4cf1614"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the autobiographical coming-of-age tale \"Romulus, My Father,\" Eric Bana, of \"Munich\" fame, plays an impoverished German émigré struggling to raise his son, Raymond (Kodi Smit-McPhee), in rural 1960's Australia. The major obstacle to the family's stability and happiness is his wife, Christina (Franka Potente), who flagrantly violates her wedding vows by shamelessly shacking up with other men. Despite her highly unconventional behavior, Romulus refuses to grant her a divorce, masochistically torturing himself in the vain hope that she will one day return to him. It is, unfortunately, the good-hearted and good-natured Raimond who must bear witness to all this marital turmoil - and it is his memoir that serves as the basis for the movie (Raimond Gaita would later grow up to be an author).\n",
      "\n",
      "Even though I admire \"Romulus, My Father\" for what it is trying to do, I can't honestly say I enjoyed it, for while the film has some fine performances and serious intentions going for it, these simply aren't enough to counteract the dour storyline and funereal pacing, which leave the audience as despairing and depressed as the people on screen. A serious slice-of-life drama is one thing, but this unremittingly downbeat wallow in adultery, insanity and multiple suicides (let alone attempted suicides) is something else again.\n",
      "\n",
      "\n",
      "Practically the only other actor who would be less likely to play Evel Knieval than Hamilton is Anthony Perkins, yet somehow Hamilton manages to turn in a reasonably effective portrayal (and as producer of the film, he wasn't likely to be fired or told he wasn't right for the part!) The early life of the daredevil motorcyclist is recounted here in multiple flashbacks. The film opens with a rather silly prologue with Hamilton in his white-leather, star-spangled gear spouting the world according to Knieval as if to say, \"Don't worry. This film is about my youth, but I'll be back in my familiar costume by the end of the picture.\" Hamilton is preparing for a huge jump, yet is still licking his wounds from the previous one as devoted wife Lyon both supports and derides him. He recalls various vignettes of his childhood and delinquent teenage years along with his early days as a stunt rider and blossoming celebrity. This flip-flop approach is pretty abrupt and sometimes disjointed, but it does prevent the movie from sticking to one of its inexpensive sets for too long a time or from getting into a rut with the fairly pedestrian characters. Hamilton, usually a suave and debonair persona, does a very fine job of enacting the tiny details of his subject's mannerisms and demeanor including his walk. His hair is a shade lighter and longer and he works hard to give the right inflections in his speaking. (He even pays minor tribute to Knieval's many injuries by appearing in a skimpy towel while his shoulders are covered in \"scars\" from the multitudinous accidents.) Facially, he looks nothing like the real cyclist, but he does suggest him in his physical performance. Lyon is excellent at playing the young girl he loves and then the more worldly wife, though her 3-pack a day voice does threaten to give her away at any given moment. She and Hamilton strike up an easy chemistry which goes a long way in putting the film over. Other nice supporting turns are given by Freed as his jaded doctor, Cameron as an early influence and Taylor as a flea-bitten sideshow barker. The film was made on a low budget, but the story is a rather low rent one anyway, so that doesn't affect it too badly. The makers wisely used actual Butte, Montana locations to give the film a proper small town ambiance. Several of Hamilton's antics are amusing, though the character is certainly reckless and inconsiderate of other people's property! Some of the real Knieval's completed and failed stunts are included in some blurry footage, one of which features a mind-boggling \"splatter\" in which the man is rolled up and snapped around like a rag doll. Hamilton's then-wife (Stewart) appears briefly as a nurse.\n",
      "\n",
      "\n",
      "This mini-series is actually more entertaining than some others with much bigger budgets and grander aspirations. SOTD falls somewhere between \"Kung-Fu\" and \"H R Pufnstuff\" on the entertainment spectrum. If it weren't so long (nearly 3 hours) I think that kids would like it quite a bit. It's got adventure, action, \"cliffhanger scenes\", and not too much romance or other \"icky\" stuff. When you're young, you're not too critical of flexing rubber swords, campy acting, and scenes that are repeated. (At least two scenes are repeated identically in the movie, just as was done in old-time serials in order to bring the audience up to speed.) Finally, kids are usually more accepting of American English dialogue coming out of the mouths of Asian actors. (Not to mention the fact that several of the leading roles are played by non-Asian actors.) \n",
      "\n",
      "I was going to give this movie three stars, but I felt like the director, producers, and cast deserved some extra credit for at least carrying through on the project. This movie is not art, but, like painting your house, it actually took some time, effort, and discipline to get it made.\n",
      "\n",
      "Overall, not a recommended use for your time, but it might keep the kids entertained while traveling in the mini-van.\n",
      "\n",
      "Oh, yeah...hey, IMDb! \"Dialogue\" is the preferred and traditional spelling. Your spell-checking seems to think that \"dialog\" is the proper spelling. While \"dialog\" is acceptable, both Webster's and the OED consider it an alternative form.\n",
      "\n",
      "\n",
      "This movie started out with some semblance of a plot, then abandoned it for an endless series of random characters and encounters that have nothing to do with moving the story forward. It was impossible to remain engaged with this film. This movie is a very cynical pile of garbage made by some people with animation skills but totally lacking in creativity or storytelling ability. It is a shockingly bad effort coming from a major studio. Clearly there are morale and motivation problems at Disney, not to mention a complete lack of oversight and quality control. That management allowed this movie to see the light of day speaks volumes about their incompetence and desperation. This movie joins my very short \"worst movies of all time\" list.\n",
      "\n",
      "\n",
      "This movie had a IMDB rating of 8.1 so I expected much more from it. It starts out funny and endearing with an energy that feels spontaneous. But before the movie is half-way through, it begins to drag and everything becomes sickingly predictable. The characters in the office were delightful in the first third of the movie, but we get to know them a little too well; they become caricatures, not real people at all. This is the same story I've seen hundreds of times, only told here with slightly different circumstances. The thing is, I could stomach another predictable love story if only the dialog weren't so stale!\n",
      "\n",
      "The only thing that could be worse is if the characters had inconsistent and unbelievable motivations, and unfortunately that was also the case with Dead Letter Office. Hopefully this movie will end up in the Dead Movie Office soon.\n",
      "\n",
      "\n",
      "I know I've already added a comment but I just wanted to clarify something...\n",
      "\n",
      "I'm not some old fogey from the Baby Boom generation that grew up glued to a flickering b/w picture of Phil Silvers, Jackie Gleason etc.\n",
      "\n",
      "Bilko was already 20 years old before I was born but I had the pleasure of discovering Phil Silver's Bilko courtesy of BBC2. I wonder if I would have enjoyed Steve Martin's travesty if I hadn't seen or heard of Phil Silvers - I don't know - maybe I would have.\n",
      "\n",
      "Some of the other reviewers who think this movie is worthy of a '10' admit that they haven't seen the original. I can only urge you to spend 21 minutes of your life watching a single episode. If after watching the original Ernie, Colonel Hall, Ritzig & Emma, Duane Doberman, Henshaw, Dino, Flashman, Zimmerman, Mullin et al you still think that Steve Martin's film is woth anything above a '2' - I'll stand you a pint....\n",
      "\n",
      "\n",
      "Never saw the original movie in the series...I only hope it was a much better movie than this or the sequel made in the 1980's as if it is not how were these two terrible sequels even justified. This movie had a really good lead in when they were advertising it to be shown on one of those old independent stations that are a thing of the past now. Anyways it looked like it would be a pretty good scary movie. It was, however, a movie that would make some Walt Disney movies look dark. Really, this movie was just a bunch of light fluff with virtually no boggy creek creature to be seen. The only real sighting is near the end when you see its shape during a very heavy rainstorm, other than that there is virtually no sign of the creature which was really disappointing as a kid. The story is basically the old evil hunters must kill anything they see and are after the boggy creek creature and kids are out to help it or just some random hairy guy in the woods that likes to pull random boats through the water. Not really worth watching I would however like to see the original, granted the maker of that would make the also bad boggy creature of the 80's, but he also made a very good slasher movie in the 70's \"The Town the Dreaded Sundown\".\n",
      "\n",
      "\n",
      "A pale shadow of a great musical, this movie suffers from the fact that the director, Richard Attenborough, completely misses the point of the musical, needlessly \"opens\" it up, and muddies the thrust of the play. The show is about a group of dancers auditioning for a job in a B'way musical and examines their drive & desire to work in this demanding and not-always-rewarding line of work. Attenborough gives us a fresh-faced cast of hopefuls, assuming that they are trying to get their \"big break\" in show business, rather than presenting the grittier mix of characters created on stage as a group of working \"gypsies\" living show to show, along with a couple of newcomers. The film has one advantage over the play and that is the opening scene, showing the size of the original audition and the true scale of shrinkage down to the 16/17 on the line (depending on how you count Cassie, who is stupidly kept out of the line in the movie). Anyone who can catch a local civic light opera production of the play will have a much richer experience than seeing this poorly-conceived film.\n",
      "\n",
      "\n",
      "1914 was an amazing year for Charlie Chaplin. It was his first year in films and he appeared in more than 30 films! While most of these films weren't particularly good, they did give him a chance to slowly evolve his screen persona. However, by this film, the familiar \"Little Tramp\" character was still in development. Sure Charlie looked the part, but his character still lacked the sweetness and decency that he later developed. Instead, Chaplin often hit, kicked or did other nasty things to people for seemingly no reason at all.\n",
      "\n",
      "As for this very slight film, it is interesting to watch for the cast. While they are not familiar today, Chaplin stars along with Mabel Normand, Chester Conklin and Mack Swain--all exceptionally popular stars with Keystone Films. The problem with this film is that while it has a few nice scenes, the plot seems very vague and improperly developed. Chester and Mabel got to the race track (a very common theme in Keystone productions--it must have been located near a race track). Charlie and Mack show up and sneak in. Mack is chased by the police for doing this while Charlie slaps Chester around and steals his girl. In the end, for no apparent reason, the cops take Chester and Mack away--leaving Charlie with Mabel (who, oddly, didn't seem put off by Charlie's boorish behaviors).\n",
      "\n",
      "Unless you are a huge silent comedy buff or film historian, this is a very forgettable film that is only important in the evolution of Chaplin. What he and the other actors actually do on stage, while not unusual for a Keystone film, isn't particularly funny when seen today.\n",
      "\n",
      "\n",
      "So, American Pie: Beta House is the 6th American Pie movie in the series. Although, it really has nothing to do with the original three American Pie movies except some of the characters are supposed to be related to the characters in the original trilogy and Eugene Levy is in it (can't that guy get better gigs?).\n",
      "\n",
      "There is very little to compliment this movie on. There aren't any funny jokes. The acting is painful to watch, especially the girl with the \"southern\" accent which sounds more like a Canadian's impersonation of a British woman pretending to be a hillbilly by using the word \"ya'll.\" This movie makes me feel like such an idiot. Why didn't I apply to a college where nobody goes to class (but everybody gets good grades), girls consistently take their clothes off in public, everybody has promiscuous unprotected sex without the burden of babies and STIs, and you can ejaculate all over a girl's family photos without her minding? Really, this series has lowered itself to the standards of softcore porn. Maybe for the next one, they'll finally break down and hire Ron Jeremy as the lead. I'm sure they can just tie it in to the series by making his character Stifler's 3rd uncle once removed or something like that.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for t in trn_texts[:10]:\n",
    "  print(t)\n",
    "  print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M0os8Xo9D6AP"
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "trn_idx = np.random.permutation(len(trn_texts))\n",
    "val_idx = np.random.permutation(len(val_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xw2NtVqoD713"
   },
   "outputs": [],
   "source": [
    "trn_texts = trn_texts[trn_idx]\n",
    "val_texts = val_texts[val_idx]\n",
    "\n",
    "trn_labels = trn_labels[trn_idx]\n",
    "val_labels = val_labels[val_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d6jgtmqjUsEk"
   },
   "source": [
    "## Fit tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "colab_type": "code",
    "id": "7kk0LpBi_4vn",
    "outputId": "c6b701ed-7711-40b4-ab32-a633d437a0af"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmad/anaconda3/lib/python3.6/site-packages/keras_preprocessing/text.py:177: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "VOCAB_SIZE = 60000\n",
    "tokenizer = Tokenizer(nb_words=VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(np.concatenate([trn_texts, val_texts]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z-n31GUQFuzR"
   },
   "outputs": [],
   "source": [
    "# Insert UNK\n",
    "tokenizer.word_index[UNK_TOK] = UNK_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SpKWDqVrTQIi"
   },
   "outputs": [],
   "source": [
    "str2int = tokenizer.word_index\n",
    "int2str = dict([(value, key) for (key, value) in str2int.items()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kUClZfs6oL6Q"
   },
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328
    },
    "colab_type": "code",
    "id": "pPr2l1y-oKyN",
    "outputId": "224810ea-18b3-4cee-e7de-4c2c31ee1384"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-03-31 19:06:43--  https://github.com/ahmadelsallab/HierarichalAttentionClassifier_HATT_Sentiment/raw/master/data/glove/glove.6B.100d.txt\n",
      "Resolving github.com (github.com)... 192.30.253.113, 192.30.253.112\n",
      "Connecting to github.com (github.com)|192.30.253.113|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/ahmadelsallab/HierarichalAttentionClassifier_HATT_Sentiment/master/data/glove/glove.6B.100d.txt [following]\n",
      "--2019-03-31 19:06:45--  https://raw.githubusercontent.com/ahmadelsallab/HierarichalAttentionClassifier_HATT_Sentiment/master/data/glove/glove.6B.100d.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.240.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.240.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 6339063 (6.0M) [text/plain]\n",
      "Saving to: ‘./dat/glove/glove.6B.100d.txt.1’\n",
      "\n",
      "glove.6B.100d.txt.1 100%[===================>]   6.04M   120KB/s    in 52s     \n",
      "\n",
      "2019-03-31 19:07:37 (120 KB/s) - ‘./dat/glove/glove.6B.100d.txt.1’ saved [6339063/6339063]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "GLOVE_DIR = \"./dat/glove\"\n",
    "try:\n",
    "    os.mkdir(os.path.join('./dat', 'glove'))\n",
    "except:\n",
    "    pass\n",
    "!wget -P {GLOVE_DIR} https://github.com/ahmadelsallab/HierarichalAttentionClassifier_HATT_Sentiment/raw/master/data/glove/glove.6B.100d.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Ty7hGkboRKT"
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_embeddings(embeddings_file):\n",
    "    embeddings_index = {}\n",
    "    f = open(embeddings_file)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "    f.close()\n",
    "\n",
    "    print('Total %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "    embedding_matrix = np.random.random((VOCAB_SIZE+1, EMBEDDING_DIM))\n",
    "    for word, i in str2int.items():\n",
    "        if i < VOCAB_SIZE:\n",
    "          embedding_vector = embeddings_index.get(word)\n",
    "          if embedding_vector is not None:\n",
    "              # words not found in embedding index will be all-zeros.\n",
    "              embedding_matrix[i] = embedding_vector    \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "lPv1e8taoZy9",
    "outputId": "7ace7c87-7dad-4416-ea9f-e4174c2b20c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ahmad/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t0Dsi-7zohZs"
   },
   "source": [
    "# Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ExuuPUonog58"
   },
   "outputs": [],
   "source": [
    "LM_DATA_SIZE = 200000\n",
    "LM_SEQ_LEN = 50\n",
    "VOCAB_SIZE = 60000\n",
    "MAX_SENT_LENGTH = LM_SEQ_LEN\n",
    "MAX_SENTS = 15\n",
    "MAX_NB_WORDS = VOCAB_SIZE\n",
    "EMBEDDING_DIM = 100\n",
    "#GLOVE_DIR = \"./dat/glove\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IlNB6J5eAaeQ"
   },
   "source": [
    "# NLM\n",
    "\n",
    "https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l0b6Y693AghF"
   },
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nBZl_YytAZ-P"
   },
   "outputs": [],
   "source": [
    "def prepare_lm_data(in_texts, seq_len, size):\n",
    "    \n",
    "    # organize into sequences of tokens\n",
    "    length = seq_len + 1\n",
    "    sequences = list()\n",
    "    for i in range(length, len(in_texts)):\n",
    "      if i < size:\n",
    "        # select sequence of tokens\n",
    "        seq = in_texts[i-length:i]\n",
    "        if(len(seq) != 51):\n",
    "          print(len(seq))\n",
    "        # convert into a line\n",
    "        #line = ' '.join(seq)\n",
    "        # store\n",
    "\n",
    "        sequences.append(seq)\n",
    "        '''\n",
    "        l = len(line.split())#len(tokenizer.texts_to_sequences(line)) \n",
    "        if  l!= 51:\n",
    "          print(l)\n",
    "        '''\n",
    "        #print(line)\n",
    "      else:\n",
    "        break\n",
    "        \n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6U7RQtZKAin5"
   },
   "outputs": [],
   "source": [
    "def binarize_lm_data(in_texts, tokenizer):\n",
    "    \n",
    "    sequences = []\n",
    "    for t in in_texts:\n",
    "      words_idx = []\n",
    "      for w in t:\n",
    "        if w in tokenizer.word_index:\n",
    "          idx = tokenizer.word_index[w]\n",
    "          if idx < VOCAB_SIZE:\n",
    "            words_idx.append(idx)\n",
    "          else:\n",
    "            words_idx.append(UNK_ID)\n",
    "        else:\n",
    "          words_idx.append(UNK_ID)\n",
    "          \n",
    "      sequences.append(words_idx) \n",
    "    \n",
    "    #sequences = [[tokenizer.word_index[w] for w in t] for t in in_texts]\n",
    "    #return np.array(tokenizer.texts_to_sequences(in_texts))\n",
    "    return np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xRlK-9WBAkE0"
   },
   "outputs": [],
   "source": [
    "\n",
    "texts = text_to_word_sequence(' '.join(list(trn_texts)))#list(trn_texts)# np.concatenate([trn_texts, val_texts])\n",
    "\n",
    "\n",
    "text_sequences = prepare_lm_data(texts, LM_SEQ_LEN, size=LM_DATA_SIZE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 219
    },
    "colab_type": "code",
    "id": "pJylz-9o-L_Y",
    "outputId": "3476e08d-03b6-46c1-b88d-3a4767cb0086"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'man', 'brings', 'his', 'new', 'wife', 'to', 'his', 'home', 'where', 'his', 'former', 'wife', 'died', 'of', 'an', 'accident', 'his', 'new', 'wife', 'has', 'just', 'been', 'released', 'from', 'an', 'institution', 'and', 'is', 'also', 'very', 'rich', 'all', 'of', 'the', 'sudden', 'she', 'starts', 'hearing', 'noises', 'and', 'seeing', 'skulls', 'all', 'over', 'the', 'place', 'is', 'she', 'going', 'crazy']\n",
      "['man', 'brings', 'his', 'new', 'wife', 'to', 'his', 'home', 'where', 'his', 'former', 'wife', 'died', 'of', 'an', 'accident', 'his', 'new', 'wife', 'has', 'just', 'been', 'released', 'from', 'an', 'institution', 'and', 'is', 'also', 'very', 'rich', 'all', 'of', 'the', 'sudden', 'she', 'starts', 'hearing', 'noises', 'and', 'seeing', 'skulls', 'all', 'over', 'the', 'place', 'is', 'she', 'going', 'crazy', 'again']\n",
      "['brings', 'his', 'new', 'wife', 'to', 'his', 'home', 'where', 'his', 'former', 'wife', 'died', 'of', 'an', 'accident', 'his', 'new', 'wife', 'has', 'just', 'been', 'released', 'from', 'an', 'institution', 'and', 'is', 'also', 'very', 'rich', 'all', 'of', 'the', 'sudden', 'she', 'starts', 'hearing', 'noises', 'and', 'seeing', 'skulls', 'all', 'over', 'the', 'place', 'is', 'she', 'going', 'crazy', 'again', 'or']\n",
      "['his', 'new', 'wife', 'to', 'his', 'home', 'where', 'his', 'former', 'wife', 'died', 'of', 'an', 'accident', 'his', 'new', 'wife', 'has', 'just', 'been', 'released', 'from', 'an', 'institution', 'and', 'is', 'also', 'very', 'rich', 'all', 'of', 'the', 'sudden', 'she', 'starts', 'hearing', 'noises', 'and', 'seeing', 'skulls', 'all', 'over', 'the', 'place', 'is', 'she', 'going', 'crazy', 'again', 'or', 'is']\n",
      "['new', 'wife', 'to', 'his', 'home', 'where', 'his', 'former', 'wife', 'died', 'of', 'an', 'accident', 'his', 'new', 'wife', 'has', 'just', 'been', 'released', 'from', 'an', 'institution', 'and', 'is', 'also', 'very', 'rich', 'all', 'of', 'the', 'sudden', 'she', 'starts', 'hearing', 'noises', 'and', 'seeing', 'skulls', 'all', 'over', 'the', 'place', 'is', 'she', 'going', 'crazy', 'again', 'or', 'is', 'the']\n",
      "['wife', 'to', 'his', 'home', 'where', 'his', 'former', 'wife', 'died', 'of', 'an', 'accident', 'his', 'new', 'wife', 'has', 'just', 'been', 'released', 'from', 'an', 'institution', 'and', 'is', 'also', 'very', 'rich', 'all', 'of', 'the', 'sudden', 'she', 'starts', 'hearing', 'noises', 'and', 'seeing', 'skulls', 'all', 'over', 'the', 'place', 'is', 'she', 'going', 'crazy', 'again', 'or', 'is', 'the', 'first']\n",
      "['to', 'his', 'home', 'where', 'his', 'former', 'wife', 'died', 'of', 'an', 'accident', 'his', 'new', 'wife', 'has', 'just', 'been', 'released', 'from', 'an', 'institution', 'and', 'is', 'also', 'very', 'rich', 'all', 'of', 'the', 'sudden', 'she', 'starts', 'hearing', 'noises', 'and', 'seeing', 'skulls', 'all', 'over', 'the', 'place', 'is', 'she', 'going', 'crazy', 'again', 'or', 'is', 'the', 'first', 'wife']\n",
      "['his', 'home', 'where', 'his', 'former', 'wife', 'died', 'of', 'an', 'accident', 'his', 'new', 'wife', 'has', 'just', 'been', 'released', 'from', 'an', 'institution', 'and', 'is', 'also', 'very', 'rich', 'all', 'of', 'the', 'sudden', 'she', 'starts', 'hearing', 'noises', 'and', 'seeing', 'skulls', 'all', 'over', 'the', 'place', 'is', 'she', 'going', 'crazy', 'again', 'or', 'is', 'the', 'first', 'wife', 'coming']\n",
      "['home', 'where', 'his', 'former', 'wife', 'died', 'of', 'an', 'accident', 'his', 'new', 'wife', 'has', 'just', 'been', 'released', 'from', 'an', 'institution', 'and', 'is', 'also', 'very', 'rich', 'all', 'of', 'the', 'sudden', 'she', 'starts', 'hearing', 'noises', 'and', 'seeing', 'skulls', 'all', 'over', 'the', 'place', 'is', 'she', 'going', 'crazy', 'again', 'or', 'is', 'the', 'first', 'wife', 'coming', 'back']\n",
      "['where', 'his', 'former', 'wife', 'died', 'of', 'an', 'accident', 'his', 'new', 'wife', 'has', 'just', 'been', 'released', 'from', 'an', 'institution', 'and', 'is', 'also', 'very', 'rich', 'all', 'of', 'the', 'sudden', 'she', 'starts', 'hearing', 'noises', 'and', 'seeing', 'skulls', 'all', 'over', 'the', 'place', 'is', 'she', 'going', 'crazy', 'again', 'or', 'is', 'the', 'first', 'wife', 'coming', 'back', 'from']\n"
     ]
    }
   ],
   "source": [
    "for t in text_sequences[:10]:\n",
    "  print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mb6apC9a-K_j"
   },
   "outputs": [],
   "source": [
    "sequences = binarize_lm_data(text_sequences, tokenizer)\n",
    "\n",
    "sz_limit = LM_DATA_SIZE# len(sequences)\n",
    "\n",
    "# separate into input and output\n",
    "sequences = array(sequences[:sz_limit])\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "\n",
    "#y = to_categorical(y, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "M-VBHoEe-iby",
    "outputId": "124e9b1b-2fa7-4847-e3ce-22f1208ea0d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(199949, 50)\n",
      "(199949,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CWRQOjjfAvIc"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 474
    },
    "colab_type": "code",
    "id": "6uPUMNULA0wI",
    "outputId": "32f18667-8f8a-4e63-c7f4-b676af2b104a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 7396 word vectors.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 50, 100)           6000100   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 50, 200)           120600    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 50, 200)           40200     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               120400    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 60001)             6060101   \n",
      "=================================================================\n",
      "Total params: 12,341,401\n",
      "Trainable params: 12,341,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "#model = Sequential()\n",
    "#model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
    "#model.add(LSTM(100, return_sequences=True))\n",
    "#model.add(LSTM(100))\n",
    "\n",
    "#GLOVE_DIR = \"./dat/glove\"\n",
    "\n",
    "embeddings_file = os.path.join(GLOVE_DIR, 'glove.6B.100d.txt')\n",
    "embedding_matrix = load_embeddings(embeddings_file)\n",
    "        \n",
    "  \n",
    "embedding_layer = Embedding(VOCAB_SIZE+1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=LM_SEQ_LEN,\n",
    "                            trainable=True)\n",
    "sentence_input = Input(shape=(LM_SEQ_LEN,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sentence_input)\n",
    "l_lstm = Bidirectional(GRU(100, return_sequences=True))(embedded_sequences)\n",
    "l_word_enc = TimeDistributed(Dense(200))(l_lstm)\n",
    "l_lstm_2 = LSTM(100)(l_word_enc)\n",
    "#model.add(Dense(100, activation='relu'))\n",
    "l_dense = Dense(100, activation='relu')(l_word_enc)\n",
    "#model.add(Dense(vocab_size, activation='softmax'))\n",
    "output = Dense(VOCAB_SIZE+1, activation='softmax')(l_lstm_2)\n",
    "model = Model(sentence_input, output)\n",
    "print(model.summary())\n",
    "word_enc = Model(sentence_input, l_word_enc)\n",
    "\n",
    "# compile model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "5lXjMnYwKRmE",
    "outputId": "6bbef1c1-4980-4dbd-88aa-0cfe0aff7325"
   },
   "outputs": [],
   "source": [
    "# Mount GDrive\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/gdrive')\n",
    "#gdrive_path = 'gdrive/My Drive'\n",
    "gdrive_path = './dat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qruxe3-fJ--1"
   },
   "outputs": [],
   "source": [
    "# save the model to file\n",
    "lm_model_file_name = 'lm_model.h5'\n",
    "word_enc_model_file_name = 'word_enc_model.h5'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 110
    },
    "colab_type": "code",
    "id": "YBWrzWtI_jkj",
    "outputId": "40b56a57-9d7b-45f1-b092-0ce1d8a6e373"
   },
   "outputs": [],
   "source": [
    "load_prev_model = True\n",
    "filepath = os.path.join(gdrive_path, lm_model_file_name)\n",
    "if load_prev_model and os.path.exists(filepath):\n",
    "  model = load_model(filepath)  \n",
    "  #word_enc = load_model(os.path.join(gdrive_path, word_enc_model_file_name))  \n",
    "  print('Existing LM loaded')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-uhrWzGaJmey"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "class SaveWordEncoder(Callback):\n",
    "    '''\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.acc = []\n",
    "    '''\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "      word_enc.save(os.path.join(gdrive_path, word_enc_model_file_name))\n",
    "        \n",
    "\n",
    "word_enc_model_cbk = SaveWordEncoder()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZT-OqKCj_eSE"
   },
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(filepath, monitor='acc', verbose=1, save_best_only=True, mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CqLYjsAJALAV"
   },
   "outputs": [],
   "source": [
    "callbacks_lst = [word_enc_model_cbk, checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "RqINBIZUPMOy",
    "outputId": "9212a68d-7cb0-423e-886e-7288033112ba"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "import keras.backend as K\n",
    "K.clear_session()\n",
    "model = load_model(filepath)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "8kwtJgBeA6KM",
    "outputId": "b0aa377d-5c54-4200-ff0d-3ee913ce3111"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " 38784/199949 [====>.........................] - ETA: 4:39 - loss: 7.4898 - acc: 0.0547"
     ]
    }
   ],
   "source": [
    "\n",
    "# fit model\n",
    "model.fit(X, y, batch_size=128, epochs=100, callbacks=callbacks_lst)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4MCo2QuBKB9R"
   },
   "outputs": [],
   "source": [
    "word_enc.save(os.path.join(gdrive_path, word_enc_model_file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YYFCfBaPSZgP"
   },
   "source": [
    "# HATT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sXEuwCHEUe7D"
   },
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6dqlE0HwSbyS"
   },
   "outputs": [],
   "source": [
    "from nltk import tokenize\n",
    "def prepare_hier_data(in_texts, in_labels):\n",
    "    \n",
    "    reviews = []\n",
    "    labels = []\n",
    "    texts = []\n",
    "    \n",
    "    for idx in range(len(in_texts)):\n",
    "        text = in_texts[idx]\n",
    "        label = in_labels[idx]\n",
    "        if label != 2:\n",
    "          #print('Parsing review ' + str(idx))\n",
    "          texts.append(text)\n",
    "          sentences = tokenize.sent_tokenize(text)\n",
    "          reviews.append(sentences)       \n",
    "          labels.append(label)\n",
    "    return reviews, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J4thy4I0UNGH"
   },
   "outputs": [],
   "source": [
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "def binarize_hier_data(reviews, labels, tokenizer):\n",
    "    data_lst = []\n",
    "    labels_lst = []\n",
    "    for i, sentences in enumerate(reviews):\n",
    "        data = UNK_ID * np.ones((MAX_SENTS, MAX_SENT_LENGTH), dtype='int32') # Init all as UNK\n",
    "        for j, sent in enumerate(sentences):\n",
    "            if j< MAX_SENTS:\n",
    "                wordTokens = text_to_word_sequence(sent)\n",
    "                k=0\n",
    "                for _, word in enumerate(wordTokens):\n",
    "                    if word in tokenizer.word_index:\n",
    "                      if k<MAX_SENT_LENGTH and tokenizer.word_index[word]<MAX_NB_WORDS:\n",
    "                          data[j,k] = tokenizer.word_index[word]\n",
    "                          k=k+1\n",
    "        data_lst.append(data)\n",
    "        labels_lst.append(labels[i])\n",
    "    data = np.array(data_lst)\n",
    "    targets = np.array(labels_lst) \n",
    "    targets = to_categorical(np.asarray(targets))\n",
    "    return data, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LOrdvgToVZML"
   },
   "outputs": [],
   "source": [
    "train_texts_, train_labels_ = prepare_hier_data(trn_texts, trn_labels)\n",
    "train_data, train_targets = binarize_hier_data(train_texts_, train_labels_, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8eZdCDnkZZpP"
   },
   "outputs": [],
   "source": [
    "# 25k only are training out of 75k, becasue 50k are unsup --> label = 2\n",
    "print('Shape of data tensor:', train_data.shape)\n",
    "print('Shape of label tensor:', train_targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QASX84GaagR-"
   },
   "source": [
    "## Split train/val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OjzhWOSjZwu_"
   },
   "outputs": [],
   "source": [
    "\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "indices = np.arange(train_data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "train_data = train_data[indices]\n",
    "train_targets = train_targets[indices]\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * train_data.shape[0])\n",
    "\n",
    "x_train = train_data[:-nb_validation_samples]\n",
    "y_train = train_targets[:-nb_validation_samples]\n",
    "x_val = train_data[-nb_validation_samples:]\n",
    "y_val = train_targets[-nb_validation_samples:]\n",
    "\n",
    "print('Number of positive and negative reviews in traing and validation set')\n",
    "print(y_train.sum(axis=0))\n",
    "print(y_val.sum(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "82YW-4Tkat1w"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jKu5d4fQB4Fs"
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "def load_word_enc_model(word_enc_model_file_name):\n",
    "    word_enc_model = load_model(word_enc_model_file_name)\n",
    "\n",
    "    '''\n",
    "    embeddings_file = os.path.join(GLOVE_DIR, 'glove.6B.100d.txt')\n",
    "    embedding_matrix = load_embeddings(embeddings_file)\n",
    "\n",
    "\n",
    "    embedding_layer = Embedding(VOCAB_SIZE+1,\n",
    "                              EMBEDDING_DIM,\n",
    "                              weights=[embedding_matrix],\n",
    "                              input_length=MAX_SENT_LENGTH,\n",
    "                              trainable=True)\n",
    "    sentence_input = Input(shape=(LM_SEQ_LEN,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sentence_input)\n",
    "    l_lstm = Bidirectional(GRU(100, return_sequences=True))(embedded_sequences)\n",
    "    l_word_enc = TimeDistributed(Dense(200))(l_lstm)\n",
    "\n",
    "\n",
    "    word_enc_model = Model(sentence_input, l_word_enc)  \n",
    "    '''\n",
    "    print(word_enc_model.summary())\n",
    "    return word_enc_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vaHckN9PBacV"
   },
   "outputs": [],
   "source": [
    "\n",
    "word_enc_model = load_word_enc_model(os.path.join(gdrive_path, word_enc_model_file_name))#model.load(word_enc_model_file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zzxO4B8IasVR"
   },
   "outputs": [],
   "source": [
    "\n",
    "embeddings_file_name = os.path.join(GLOVE_DIR, 'glove.6B.100d.txt')\n",
    "\n",
    "# building Hierachical Attention network\n",
    "embedding_matrix = load_embeddings(embeddings_file_name)\n",
    "\n",
    "        \n",
    "embedding_layer = Embedding(VOCAB_SIZE,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SENT_LENGTH,\n",
    "                            trainable=True)\n",
    "\n",
    "class AttLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.init = initializers.he_normal()\n",
    "        super(AttLayer, self).__init__(**kwargs)\n",
    "    '''\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape)==3\n",
    "        self.W = self.init((input_shape[-1],1))\n",
    "        self.trainable_weights = [self.W]\n",
    "        super(AttLayer, self).build(input_shape)  # be sure you call this somewhere!\n",
    "    '''\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape)==3\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.W = self.add_weight(name='kernel', \n",
    "                                      shape=(input_shape[-1], 1),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        super(AttLayer, self).build(input_shape)  # Be sure to call this at the end\n",
    "    \n",
    "    def call(self, x, mask=None):\n",
    "        eij = K.tanh(K.dot(x, self.W))\n",
    "        \n",
    "        ai = K.exp(eij)\n",
    "        weights = ai/tf.expand_dims(K.sum(ai, axis=1), 1)\n",
    "        \n",
    "        weighted_input = x*weights\n",
    "        return tf.reduce_sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "\n",
    "sentence_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\n",
    "'''\n",
    "embedded_sequences = embedding_layer(sentence_input)\n",
    "l_lstm = Bidirectional(GRU(100, return_sequences=True))(embedded_sequences)\n",
    "l_dense = TimeDistributed(Dense(200))(l_lstm)\n",
    "'''\n",
    "l_dense = word_enc_model(sentence_input)\n",
    "l_att = AttLayer()(l_dense)\n",
    "sentEncoder = Model(sentence_input, l_att)\n",
    "\n",
    "review_input = Input(shape=(MAX_SENTS,MAX_SENT_LENGTH), dtype='int32')\n",
    "review_encoder = TimeDistributed(sentEncoder)(review_input)\n",
    "l_lstm_sent = Bidirectional(GRU(100, return_sequences=True))(review_encoder)\n",
    "l_dense_sent = TimeDistributed(Dense(200))(l_lstm_sent)\n",
    "l_att_sent = AttLayer()(l_dense_sent)\n",
    "preds = Dense(2, activation='softmax')(l_att_sent)\n",
    "model = Model(review_input, preds)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1YwHgN2BZZn_"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GEOcjyCjAYmc"
   },
   "outputs": [],
   "source": [
    "hatt_model = 'hatt_model.h5'\n",
    "load_prev_model = False\n",
    "filepath = os.path.join(gdrive_path, hatt_model)\n",
    "if load_prev_model and os.path.exists(filepath):\n",
    "    model = load_model(filepath) \n",
    "    print('HATT model loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RPJMtx9yAd88"
   },
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g5G_WMCuAm05"
   },
   "outputs": [],
   "source": [
    "callbacks_lst = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_enc_model.trainable = False\n",
    "model.summary()\n",
    "# Must call compile for trainable to take effect\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JQUZsTvmgxH9"
   },
   "outputs": [],
   "source": [
    "\n",
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 50\n",
    "print(\"model fitting - Hierachical attention network\")\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val), nb_epoch=NUM_EPOCHS, batch_size=BATCH_SIZE, callbacks=callbacks_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_enc_model.trainable = True\n",
    "model.summary()\n",
    "# Must call compile for trainable to take effect\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 50\n",
    "print(\"model fitting - Hierachical attention network\")\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val), nb_epoch=NUM_EPOCHS, batch_size=BATCH_SIZE, callbacks=callbacks_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CIUs79avhOIx"
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FUzWac-3ZZmi"
   },
   "outputs": [],
   "source": [
    "test_texts_, test_labels_ = prepare_hier_data(val_texts, val_labels)\n",
    "test_data, test_targets = binarize_hier_data(test_texts_, test_labels_, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wcabJk6gGea8"
   },
   "outputs": [],
   "source": [
    "\n",
    "print('Shape of data tensor:', test_data.shape)\n",
    "print('Shape of label tensor:', test_targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nY8VkCYvhUc6"
   },
   "outputs": [],
   "source": [
    "for i, rev in enumerate(test_texts_):\n",
    "    print(rev)\n",
    "    test_input = test_data[i].copy()\n",
    "    test_input = np.reshape(test_input, (1,test_input.shape[0], test_input.shape[1]))\n",
    "    prediction = model.predict(test_input)\n",
    "    print('Prediction: ', prediction)\n",
    "    sentiment = np.argmax(prediction)\n",
    "    print('Sentiment: ' + str(sentiment))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "READ.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
