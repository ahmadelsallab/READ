{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c7-HTtj36-uK"
   },
   "source": [
    "<h1><center>READ: Recurrent Encoder Neural Language Model with Hierarichal Attention Decoder Fine tuning for Text Classification</center></h1>\n",
    "\n",
    "This work is inspired by two recent advances in NLP:\n",
    "\n",
    "1- ULMFiT: Transfer learning from pre-trained model for LM, fined tuned on NLP task\n",
    "\n",
    "2- HATT: Hierarichal Attention Classifier\n",
    "\n",
    "__What's in READ not in ULMFiT__\n",
    "- Hierarichy: which is good for sentiment prediction\n",
    "- Attention\n",
    "\n",
    "\n",
    "__What's in ULMFiT not in READ__\n",
    "- AWD-LSTM\n",
    "- Pre-trained LM on wikitext, then IMDB\n",
    "- LRFind\n",
    "- Bidirectional\n",
    "\n",
    "__References__\n",
    "https://github.com/fastai/fastai/blob/master/courses/dl2/imdb.ipynb\n",
    "https://arxiv.org/abs/1801.06146\n",
    "https://www.cs.cmu.edu/~hovy/papers/16HLT-hierarchical-attention-networks.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "YNhYpEEEa-8N",
    "outputId": "60ad49e2-aafd-4e23-e0a0-b91fb8ba83af"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmad/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import array\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import string\n",
    "os.environ['KERAS_BACKEND']='tensorflow'\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout, LSTM, GRU, Bidirectional, TimeDistributed\n",
    "from keras.models import Model, Sequential\n",
    "from keras.callbacks import ModelCheckpoint, Callback\n",
    "from keras.models import load_model\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from pickle import dump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uFJaE2bIUlK3"
   },
   "source": [
    "# IMDb data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "foFUAI1ZUxlk"
   },
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "CrGwHCNf66rG",
    "outputId": "63969b65-888c-47af-9b17-f3594ced0e4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 80.2M  100 80.2M    0     0    98k      0  0:13:52  0:13:52 --:--:--  142k17118  0  74715      0  0:18:45  0:08:41  0:10:04  131k59.5M    0     0  90219      0  0:15:32  0:11:32  0:04:00 39405\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "DATA_PATH=Path('../dat/')\n",
    "DATA_PATH.mkdir(exist_ok=True)\n",
    "!curl -O http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz \n",
    "!tar -xf aclImdb_v1.tar.gz -C {DATA_PATH}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Czebjx8TDH4c"
   },
   "outputs": [],
   "source": [
    "BOS = 'xbos'  # beginning-of-sentence tag\n",
    "FLD = 'xfld'  # data field tag\n",
    "UNK_TOK = '_UNK_'\n",
    "UNK_ID = 0 # 0 index is reserved for the UNK in both Keras Tokenizer and Embedding\n",
    "\n",
    "PATH=Path('../dat/aclImdb/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2aMAxTD7DM1F"
   },
   "outputs": [],
   "source": [
    "CLAS_PATH=Path('../dat/imdb_clas/')\n",
    "CLAS_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "LM_PATH=Path('../dat/imdb_lm/')\n",
    "LM_PATH.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zLieidwnEbAB"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "re1 = re.compile(r'  +')\n",
    "import html\n",
    "\n",
    "def fixup(x):\n",
    "    x = x.replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n",
    "        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n",
    "        '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>','u_n').replace(' @.@ ','.').replace(\n",
    "        ' @-@ ','-').replace('\\\\', ' \\\\ ')\n",
    "    return re1.sub(' ', html.unescape(x)) # Do not lower() so that capitalized words still hold a sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P1d7ncB_DPr3"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "CLASSES = ['neg', 'pos']#, 'unsup']\n",
    "\n",
    "def get_texts(path):\n",
    "    texts,labels = [],[]\n",
    "    for idx,label in enumerate(CLASSES):\n",
    "        for fname in (path/label).glob('*.*'):\n",
    "            texts.append(fixup(fname.open('r', encoding='utf-8').read()))\n",
    "            labels.append(idx)\n",
    "    return np.array(texts),np.array(labels)\n",
    "    #return texts, labels\n",
    "\n",
    "trn_texts,trn_labels = get_texts(PATH/'train')\n",
    "val_texts,val_labels = get_texts(PATH/'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "1Hm96jRZDeiN",
    "outputId": "7da86578-a39a-44f9-a706-d198113e411c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 25000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trn_texts),len(val_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1201
    },
    "colab_type": "code",
    "id": "KQVcGwZhDgJg",
    "outputId": "815d4f06-1e37-49e4-94b0-2847b4cf1614"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the autobiographical coming-of-age tale \"Romulus, My Father,\" Eric Bana, of \"Munich\" fame, plays an impoverished German émigré struggling to raise his son, Raymond (Kodi Smit-McPhee), in rural 1960's Australia. The major obstacle to the family's stability and happiness is his wife, Christina (Franka Potente), who flagrantly violates her wedding vows by shamelessly shacking up with other men. Despite her highly unconventional behavior, Romulus refuses to grant her a divorce, masochistically torturing himself in the vain hope that she will one day return to him. It is, unfortunately, the good-hearted and good-natured Raimond who must bear witness to all this marital turmoil - and it is his memoir that serves as the basis for the movie (Raimond Gaita would later grow up to be an author).\n",
      "\n",
      "Even though I admire \"Romulus, My Father\" for what it is trying to do, I can't honestly say I enjoyed it, for while the film has some fine performances and serious intentions going for it, these simply aren't enough to counteract the dour storyline and funereal pacing, which leave the audience as despairing and depressed as the people on screen. A serious slice-of-life drama is one thing, but this unremittingly downbeat wallow in adultery, insanity and multiple suicides (let alone attempted suicides) is something else again.\n",
      "\n",
      "\n",
      "Practically the only other actor who would be less likely to play Evel Knieval than Hamilton is Anthony Perkins, yet somehow Hamilton manages to turn in a reasonably effective portrayal (and as producer of the film, he wasn't likely to be fired or told he wasn't right for the part!) The early life of the daredevil motorcyclist is recounted here in multiple flashbacks. The film opens with a rather silly prologue with Hamilton in his white-leather, star-spangled gear spouting the world according to Knieval as if to say, \"Don't worry. This film is about my youth, but I'll be back in my familiar costume by the end of the picture.\" Hamilton is preparing for a huge jump, yet is still licking his wounds from the previous one as devoted wife Lyon both supports and derides him. He recalls various vignettes of his childhood and delinquent teenage years along with his early days as a stunt rider and blossoming celebrity. This flip-flop approach is pretty abrupt and sometimes disjointed, but it does prevent the movie from sticking to one of its inexpensive sets for too long a time or from getting into a rut with the fairly pedestrian characters. Hamilton, usually a suave and debonair persona, does a very fine job of enacting the tiny details of his subject's mannerisms and demeanor including his walk. His hair is a shade lighter and longer and he works hard to give the right inflections in his speaking. (He even pays minor tribute to Knieval's many injuries by appearing in a skimpy towel while his shoulders are covered in \"scars\" from the multitudinous accidents.) Facially, he looks nothing like the real cyclist, but he does suggest him in his physical performance. Lyon is excellent at playing the young girl he loves and then the more worldly wife, though her 3-pack a day voice does threaten to give her away at any given moment. She and Hamilton strike up an easy chemistry which goes a long way in putting the film over. Other nice supporting turns are given by Freed as his jaded doctor, Cameron as an early influence and Taylor as a flea-bitten sideshow barker. The film was made on a low budget, but the story is a rather low rent one anyway, so that doesn't affect it too badly. The makers wisely used actual Butte, Montana locations to give the film a proper small town ambiance. Several of Hamilton's antics are amusing, though the character is certainly reckless and inconsiderate of other people's property! Some of the real Knieval's completed and failed stunts are included in some blurry footage, one of which features a mind-boggling \"splatter\" in which the man is rolled up and snapped around like a rag doll. Hamilton's then-wife (Stewart) appears briefly as a nurse.\n",
      "\n",
      "\n",
      "This mini-series is actually more entertaining than some others with much bigger budgets and grander aspirations. SOTD falls somewhere between \"Kung-Fu\" and \"H R Pufnstuff\" on the entertainment spectrum. If it weren't so long (nearly 3 hours) I think that kids would like it quite a bit. It's got adventure, action, \"cliffhanger scenes\", and not too much romance or other \"icky\" stuff. When you're young, you're not too critical of flexing rubber swords, campy acting, and scenes that are repeated. (At least two scenes are repeated identically in the movie, just as was done in old-time serials in order to bring the audience up to speed.) Finally, kids are usually more accepting of American English dialogue coming out of the mouths of Asian actors. (Not to mention the fact that several of the leading roles are played by non-Asian actors.) \n",
      "\n",
      "I was going to give this movie three stars, but I felt like the director, producers, and cast deserved some extra credit for at least carrying through on the project. This movie is not art, but, like painting your house, it actually took some time, effort, and discipline to get it made.\n",
      "\n",
      "Overall, not a recommended use for your time, but it might keep the kids entertained while traveling in the mini-van.\n",
      "\n",
      "Oh, yeah...hey, IMDb! \"Dialogue\" is the preferred and traditional spelling. Your spell-checking seems to think that \"dialog\" is the proper spelling. While \"dialog\" is acceptable, both Webster's and the OED consider it an alternative form.\n",
      "\n",
      "\n",
      "This movie started out with some semblance of a plot, then abandoned it for an endless series of random characters and encounters that have nothing to do with moving the story forward. It was impossible to remain engaged with this film. This movie is a very cynical pile of garbage made by some people with animation skills but totally lacking in creativity or storytelling ability. It is a shockingly bad effort coming from a major studio. Clearly there are morale and motivation problems at Disney, not to mention a complete lack of oversight and quality control. That management allowed this movie to see the light of day speaks volumes about their incompetence and desperation. This movie joins my very short \"worst movies of all time\" list.\n",
      "\n",
      "\n",
      "This movie had a IMDB rating of 8.1 so I expected much more from it. It starts out funny and endearing with an energy that feels spontaneous. But before the movie is half-way through, it begins to drag and everything becomes sickingly predictable. The characters in the office were delightful in the first third of the movie, but we get to know them a little too well; they become caricatures, not real people at all. This is the same story I've seen hundreds of times, only told here with slightly different circumstances. The thing is, I could stomach another predictable love story if only the dialog weren't so stale!\n",
      "\n",
      "The only thing that could be worse is if the characters had inconsistent and unbelievable motivations, and unfortunately that was also the case with Dead Letter Office. Hopefully this movie will end up in the Dead Movie Office soon.\n",
      "\n",
      "\n",
      "I know I've already added a comment but I just wanted to clarify something...\n",
      "\n",
      "I'm not some old fogey from the Baby Boom generation that grew up glued to a flickering b/w picture of Phil Silvers, Jackie Gleason etc.\n",
      "\n",
      "Bilko was already 20 years old before I was born but I had the pleasure of discovering Phil Silver's Bilko courtesy of BBC2. I wonder if I would have enjoyed Steve Martin's travesty if I hadn't seen or heard of Phil Silvers - I don't know - maybe I would have.\n",
      "\n",
      "Some of the other reviewers who think this movie is worthy of a '10' admit that they haven't seen the original. I can only urge you to spend 21 minutes of your life watching a single episode. If after watching the original Ernie, Colonel Hall, Ritzig & Emma, Duane Doberman, Henshaw, Dino, Flashman, Zimmerman, Mullin et al you still think that Steve Martin's film is woth anything above a '2' - I'll stand you a pint....\n",
      "\n",
      "\n",
      "Never saw the original movie in the series...I only hope it was a much better movie than this or the sequel made in the 1980's as if it is not how were these two terrible sequels even justified. This movie had a really good lead in when they were advertising it to be shown on one of those old independent stations that are a thing of the past now. Anyways it looked like it would be a pretty good scary movie. It was, however, a movie that would make some Walt Disney movies look dark. Really, this movie was just a bunch of light fluff with virtually no boggy creek creature to be seen. The only real sighting is near the end when you see its shape during a very heavy rainstorm, other than that there is virtually no sign of the creature which was really disappointing as a kid. The story is basically the old evil hunters must kill anything they see and are after the boggy creek creature and kids are out to help it or just some random hairy guy in the woods that likes to pull random boats through the water. Not really worth watching I would however like to see the original, granted the maker of that would make the also bad boggy creature of the 80's, but he also made a very good slasher movie in the 70's \"The Town the Dreaded Sundown\".\n",
      "\n",
      "\n",
      "A pale shadow of a great musical, this movie suffers from the fact that the director, Richard Attenborough, completely misses the point of the musical, needlessly \"opens\" it up, and muddies the thrust of the play. The show is about a group of dancers auditioning for a job in a B'way musical and examines their drive & desire to work in this demanding and not-always-rewarding line of work. Attenborough gives us a fresh-faced cast of hopefuls, assuming that they are trying to get their \"big break\" in show business, rather than presenting the grittier mix of characters created on stage as a group of working \"gypsies\" living show to show, along with a couple of newcomers. The film has one advantage over the play and that is the opening scene, showing the size of the original audition and the true scale of shrinkage down to the 16/17 on the line (depending on how you count Cassie, who is stupidly kept out of the line in the movie). Anyone who can catch a local civic light opera production of the play will have a much richer experience than seeing this poorly-conceived film.\n",
      "\n",
      "\n",
      "1914 was an amazing year for Charlie Chaplin. It was his first year in films and he appeared in more than 30 films! While most of these films weren't particularly good, they did give him a chance to slowly evolve his screen persona. However, by this film, the familiar \"Little Tramp\" character was still in development. Sure Charlie looked the part, but his character still lacked the sweetness and decency that he later developed. Instead, Chaplin often hit, kicked or did other nasty things to people for seemingly no reason at all.\n",
      "\n",
      "As for this very slight film, it is interesting to watch for the cast. While they are not familiar today, Chaplin stars along with Mabel Normand, Chester Conklin and Mack Swain--all exceptionally popular stars with Keystone Films. The problem with this film is that while it has a few nice scenes, the plot seems very vague and improperly developed. Chester and Mabel got to the race track (a very common theme in Keystone productions--it must have been located near a race track). Charlie and Mack show up and sneak in. Mack is chased by the police for doing this while Charlie slaps Chester around and steals his girl. In the end, for no apparent reason, the cops take Chester and Mack away--leaving Charlie with Mabel (who, oddly, didn't seem put off by Charlie's boorish behaviors).\n",
      "\n",
      "Unless you are a huge silent comedy buff or film historian, this is a very forgettable film that is only important in the evolution of Chaplin. What he and the other actors actually do on stage, while not unusual for a Keystone film, isn't particularly funny when seen today.\n",
      "\n",
      "\n",
      "So, American Pie: Beta House is the 6th American Pie movie in the series. Although, it really has nothing to do with the original three American Pie movies except some of the characters are supposed to be related to the characters in the original trilogy and Eugene Levy is in it (can't that guy get better gigs?).\n",
      "\n",
      "There is very little to compliment this movie on. There aren't any funny jokes. The acting is painful to watch, especially the girl with the \"southern\" accent which sounds more like a Canadian's impersonation of a British woman pretending to be a hillbilly by using the word \"ya'll.\" This movie makes me feel like such an idiot. Why didn't I apply to a college where nobody goes to class (but everybody gets good grades), girls consistently take their clothes off in public, everybody has promiscuous unprotected sex without the burden of babies and STIs, and you can ejaculate all over a girl's family photos without her minding? Really, this series has lowered itself to the standards of softcore porn. Maybe for the next one, they'll finally break down and hire Ron Jeremy as the lead. I'm sure they can just tie it in to the series by making his character Stifler's 3rd uncle once removed or something like that.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for t in trn_texts[:10]:\n",
    "  print(t)\n",
    "  print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M0os8Xo9D6AP"
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "trn_idx = np.random.permutation(len(trn_texts))\n",
    "val_idx = np.random.permutation(len(val_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xw2NtVqoD713"
   },
   "outputs": [],
   "source": [
    "trn_texts = trn_texts[trn_idx]\n",
    "val_texts = val_texts[val_idx]\n",
    "\n",
    "trn_labels = trn_labels[trn_idx]\n",
    "val_labels = val_labels[val_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d6jgtmqjUsEk"
   },
   "source": [
    "## Fit tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "colab_type": "code",
    "id": "7kk0LpBi_4vn",
    "outputId": "c6b701ed-7711-40b4-ab32-a633d437a0af"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmad/anaconda3/lib/python3.6/site-packages/keras_preprocessing/text.py:177: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "VOCAB_SIZE = 60000\n",
    "tokenizer = Tokenizer(nb_words=VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(np.concatenate([trn_texts, val_texts]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z-n31GUQFuzR"
   },
   "outputs": [],
   "source": [
    "# Insert UNK\n",
    "tokenizer.word_index[UNK_TOK] = UNK_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SpKWDqVrTQIi"
   },
   "outputs": [],
   "source": [
    "str2int = tokenizer.word_index\n",
    "int2str = dict([(value, key) for (key, value) in str2int.items()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kUClZfs6oL6Q"
   },
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328
    },
    "colab_type": "code",
    "id": "pPr2l1y-oKyN",
    "outputId": "224810ea-18b3-4cee-e7de-4c2c31ee1384"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-03-19 16:54:12--  https://github.com/ahmadelsallab/HierarichalAttentionClassifier_HATT_Sentiment/raw/master/data/glove/glove.6B.100d.txt\n",
      "Resolving github.com (github.com)... 140.82.118.3, 140.82.118.4\n",
      "Connecting to github.com (github.com)|140.82.118.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/ahmadelsallab/HierarichalAttentionClassifier_HATT_Sentiment/master/data/glove/glove.6B.100d.txt [following]\n",
      "--2019-03-19 16:54:16--  https://raw.githubusercontent.com/ahmadelsallab/HierarichalAttentionClassifier_HATT_Sentiment/master/data/glove/glove.6B.100d.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.240.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.240.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 6339063 (6.0M) [text/plain]\n",
      "Saving to: ‘../dat/glove/glove.6B.100d.txt.1’\n",
      "\n",
      "glove.6B.100d.txt.1 100%[===================>]   6.04M  45.6KB/s    in 32s     \n",
      "\n",
      "2019-03-19 16:54:49 (192 KB/s) - ‘../dat/glove/glove.6B.100d.txt.1’ saved [6339063/6339063]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "GLOVE_DIR = \"../dat/glove\"\n",
    "try:\n",
    "    os.mkdir(os.path.join('../dat', 'glove'))\n",
    "except:\n",
    "    pass\n",
    "!wget -P {GLOVE_DIR} https://github.com/ahmadelsallab/HierarichalAttentionClassifier_HATT_Sentiment/raw/master/data/glove/glove.6B.100d.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Ty7hGkboRKT"
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_embeddings(embeddings_file):\n",
    "    embeddings_index = {}\n",
    "    f = open(embeddings_file)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "    f.close()\n",
    "\n",
    "    print('Total %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "    embedding_matrix = np.random.random((VOCAB_SIZE+1, EMBEDDING_DIM))\n",
    "    for word, i in str2int.items():\n",
    "        if i < VOCAB_SIZE:\n",
    "          embedding_vector = embeddings_index.get(word)\n",
    "          if embedding_vector is not None:\n",
    "              # words not found in embedding index will be all-zeros.\n",
    "              embedding_matrix[i] = embedding_vector    \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "lPv1e8taoZy9",
    "outputId": "7ace7c87-7dad-4416-ea9f-e4174c2b20c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ahmad/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t0Dsi-7zohZs"
   },
   "source": [
    "# Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ExuuPUonog58"
   },
   "outputs": [],
   "source": [
    "LM_DATA_SIZE = 200000\n",
    "LM_SEQ_LEN = 50\n",
    "VOCAB_SIZE = 60000\n",
    "MAX_SENT_LENGTH = LM_SEQ_LEN\n",
    "MAX_SENTS = 15\n",
    "MAX_NB_WORDS = VOCAB_SIZE\n",
    "EMBEDDING_DIM = 100\n",
    "GLOVE_DIR = \"./dat/glove\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IlNB6J5eAaeQ"
   },
   "source": [
    "# NLM\n",
    "\n",
    "https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l0b6Y693AghF"
   },
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nBZl_YytAZ-P"
   },
   "outputs": [],
   "source": [
    "def prepare_lm_data(in_texts, seq_len, size):\n",
    "    \n",
    "    # organize into sequences of tokens\n",
    "    length = seq_len + 1\n",
    "    sequences = list()\n",
    "    for i in range(length, len(in_texts)):\n",
    "      if i < size:\n",
    "        # select sequence of tokens\n",
    "        seq = in_texts[i-length:i]\n",
    "        if(len(seq) != 51):\n",
    "          print(len(seq))\n",
    "        # convert into a line\n",
    "        #line = ' '.join(seq)\n",
    "        # store\n",
    "\n",
    "        sequences.append(seq)\n",
    "        '''\n",
    "        l = len(line.split())#len(tokenizer.texts_to_sequences(line)) \n",
    "        if  l!= 51:\n",
    "          print(l)\n",
    "        '''\n",
    "        #print(line)\n",
    "      else:\n",
    "        break\n",
    "        \n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6U7RQtZKAin5"
   },
   "outputs": [],
   "source": [
    "def binarize_lm_data(in_texts, tokenizer):\n",
    "    \n",
    "    sequences = []\n",
    "    for t in in_texts:\n",
    "      words_idx = []\n",
    "      for w in t:\n",
    "        if w in tokenizer.word_index:\n",
    "          idx = tokenizer.word_index[w]\n",
    "          if idx < VOCAB_SIZE:\n",
    "            words_idx.append(idx)\n",
    "          else:\n",
    "            words_idx.append(UNK_ID)\n",
    "        else:\n",
    "          words_idx.append(UNK_ID)\n",
    "          \n",
    "      sequences.append(words_idx) \n",
    "    \n",
    "    #sequences = [[tokenizer.word_index[w] for w in t] for t in in_texts]\n",
    "    #return np.array(tokenizer.texts_to_sequences(in_texts))\n",
    "    return np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xRlK-9WBAkE0"
   },
   "outputs": [],
   "source": [
    "\n",
    "texts = text_to_word_sequence(' '.join(list(trn_texts)))#list(trn_texts)# np.concatenate([trn_texts, val_texts])\n",
    "\n",
    "\n",
    "text_sequences = prepare_lm_data(texts, LM_SEQ_LEN, size=LM_DATA_SIZE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 219
    },
    "colab_type": "code",
    "id": "pJylz-9o-L_Y",
    "outputId": "3476e08d-03b6-46c1-b88d-3a4767cb0086"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'man', 'brings', 'his', 'new', 'wife', 'to', 'his', 'home', 'where', 'his', 'former', 'wife', 'died', 'of', 'an', 'accident', 'his', 'new', 'wife', 'has', 'just', 'been', 'released', 'from', 'an', 'institution', 'and', 'is', 'also', 'very', 'rich', 'all', 'of', 'the', 'sudden', 'she', 'starts', 'hearing', 'noises', 'and', 'seeing', 'skulls', 'all', 'over', 'the', 'place', 'is', 'she', 'going', 'crazy']\n",
      "['man', 'brings', 'his', 'new', 'wife', 'to', 'his', 'home', 'where', 'his', 'former', 'wife', 'died', 'of', 'an', 'accident', 'his', 'new', 'wife', 'has', 'just', 'been', 'released', 'from', 'an', 'institution', 'and', 'is', 'also', 'very', 'rich', 'all', 'of', 'the', 'sudden', 'she', 'starts', 'hearing', 'noises', 'and', 'seeing', 'skulls', 'all', 'over', 'the', 'place', 'is', 'she', 'going', 'crazy', 'again']\n",
      "['brings', 'his', 'new', 'wife', 'to', 'his', 'home', 'where', 'his', 'former', 'wife', 'died', 'of', 'an', 'accident', 'his', 'new', 'wife', 'has', 'just', 'been', 'released', 'from', 'an', 'institution', 'and', 'is', 'also', 'very', 'rich', 'all', 'of', 'the', 'sudden', 'she', 'starts', 'hearing', 'noises', 'and', 'seeing', 'skulls', 'all', 'over', 'the', 'place', 'is', 'she', 'going', 'crazy', 'again', 'or']\n",
      "['his', 'new', 'wife', 'to', 'his', 'home', 'where', 'his', 'former', 'wife', 'died', 'of', 'an', 'accident', 'his', 'new', 'wife', 'has', 'just', 'been', 'released', 'from', 'an', 'institution', 'and', 'is', 'also', 'very', 'rich', 'all', 'of', 'the', 'sudden', 'she', 'starts', 'hearing', 'noises', 'and', 'seeing', 'skulls', 'all', 'over', 'the', 'place', 'is', 'she', 'going', 'crazy', 'again', 'or', 'is']\n",
      "['new', 'wife', 'to', 'his', 'home', 'where', 'his', 'former', 'wife', 'died', 'of', 'an', 'accident', 'his', 'new', 'wife', 'has', 'just', 'been', 'released', 'from', 'an', 'institution', 'and', 'is', 'also', 'very', 'rich', 'all', 'of', 'the', 'sudden', 'she', 'starts', 'hearing', 'noises', 'and', 'seeing', 'skulls', 'all', 'over', 'the', 'place', 'is', 'she', 'going', 'crazy', 'again', 'or', 'is', 'the']\n",
      "['wife', 'to', 'his', 'home', 'where', 'his', 'former', 'wife', 'died', 'of', 'an', 'accident', 'his', 'new', 'wife', 'has', 'just', 'been', 'released', 'from', 'an', 'institution', 'and', 'is', 'also', 'very', 'rich', 'all', 'of', 'the', 'sudden', 'she', 'starts', 'hearing', 'noises', 'and', 'seeing', 'skulls', 'all', 'over', 'the', 'place', 'is', 'she', 'going', 'crazy', 'again', 'or', 'is', 'the', 'first']\n",
      "['to', 'his', 'home', 'where', 'his', 'former', 'wife', 'died', 'of', 'an', 'accident', 'his', 'new', 'wife', 'has', 'just', 'been', 'released', 'from', 'an', 'institution', 'and', 'is', 'also', 'very', 'rich', 'all', 'of', 'the', 'sudden', 'she', 'starts', 'hearing', 'noises', 'and', 'seeing', 'skulls', 'all', 'over', 'the', 'place', 'is', 'she', 'going', 'crazy', 'again', 'or', 'is', 'the', 'first', 'wife']\n",
      "['his', 'home', 'where', 'his', 'former', 'wife', 'died', 'of', 'an', 'accident', 'his', 'new', 'wife', 'has', 'just', 'been', 'released', 'from', 'an', 'institution', 'and', 'is', 'also', 'very', 'rich', 'all', 'of', 'the', 'sudden', 'she', 'starts', 'hearing', 'noises', 'and', 'seeing', 'skulls', 'all', 'over', 'the', 'place', 'is', 'she', 'going', 'crazy', 'again', 'or', 'is', 'the', 'first', 'wife', 'coming']\n",
      "['home', 'where', 'his', 'former', 'wife', 'died', 'of', 'an', 'accident', 'his', 'new', 'wife', 'has', 'just', 'been', 'released', 'from', 'an', 'institution', 'and', 'is', 'also', 'very', 'rich', 'all', 'of', 'the', 'sudden', 'she', 'starts', 'hearing', 'noises', 'and', 'seeing', 'skulls', 'all', 'over', 'the', 'place', 'is', 'she', 'going', 'crazy', 'again', 'or', 'is', 'the', 'first', 'wife', 'coming', 'back']\n",
      "['where', 'his', 'former', 'wife', 'died', 'of', 'an', 'accident', 'his', 'new', 'wife', 'has', 'just', 'been', 'released', 'from', 'an', 'institution', 'and', 'is', 'also', 'very', 'rich', 'all', 'of', 'the', 'sudden', 'she', 'starts', 'hearing', 'noises', 'and', 'seeing', 'skulls', 'all', 'over', 'the', 'place', 'is', 'she', 'going', 'crazy', 'again', 'or', 'is', 'the', 'first', 'wife', 'coming', 'back', 'from']\n"
     ]
    }
   ],
   "source": [
    "for t in text_sequences[:10]:\n",
    "  print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mb6apC9a-K_j"
   },
   "outputs": [],
   "source": [
    "sequences = binarize_lm_data(text_sequences, tokenizer)\n",
    "\n",
    "sz_limit = LM_DATA_SIZE# len(sequences)\n",
    "\n",
    "# separate into input and output\n",
    "sequences = array(sequences[:sz_limit])\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "\n",
    "#y = to_categorical(y, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "M-VBHoEe-iby",
    "outputId": "124e9b1b-2fa7-4847-e3ce-22f1208ea0d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(199949, 50)\n",
      "(199949,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CWRQOjjfAvIc"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 474
    },
    "colab_type": "code",
    "id": "6uPUMNULA0wI",
    "outputId": "32f18667-8f8a-4e63-c7f4-b676af2b104a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 7396 word vectors.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 50, 100)           6000100   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 50, 200)           120600    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 50, 200)           40200     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               120400    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 60001)             6060101   \n",
      "=================================================================\n",
      "Total params: 12,341,401\n",
      "Trainable params: 12,341,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "#model = Sequential()\n",
    "#model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
    "#model.add(LSTM(100, return_sequences=True))\n",
    "#model.add(LSTM(100))\n",
    "\n",
    "GLOVE_DIR = \"../dat/glove\"\n",
    "\n",
    "embeddings_file = os.path.join(GLOVE_DIR, 'glove.6B.100d.txt')\n",
    "embedding_matrix = load_embeddings(embeddings_file)\n",
    "        \n",
    "  \n",
    "embedding_layer = Embedding(VOCAB_SIZE+1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=LM_SEQ_LEN,\n",
    "                            trainable=True)\n",
    "sentence_input = Input(shape=(LM_SEQ_LEN,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sentence_input)\n",
    "l_lstm = Bidirectional(GRU(100, return_sequences=True))(embedded_sequences)\n",
    "l_word_enc = TimeDistributed(Dense(200))(l_lstm)\n",
    "l_lstm_2 = LSTM(100)(l_word_enc)\n",
    "#model.add(Dense(100, activation='relu'))\n",
    "l_dense = Dense(100, activation='relu')(l_word_enc)\n",
    "#model.add(Dense(vocab_size, activation='softmax'))\n",
    "output = Dense(VOCAB_SIZE+1, activation='softmax')(l_lstm_2)\n",
    "model = Model(sentence_input, output)\n",
    "print(model.summary())\n",
    "word_enc = Model(sentence_input, l_word_enc)\n",
    "\n",
    "# compile model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "5lXjMnYwKRmE",
    "outputId": "6bbef1c1-4980-4dbd-88aa-0cfe0aff7325"
   },
   "outputs": [],
   "source": [
    "# Mount GDrive\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/gdrive')\n",
    "#gdrive_path = 'gdrive/My Drive'\n",
    "gdrive_path = '../dat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qruxe3-fJ--1"
   },
   "outputs": [],
   "source": [
    "# save the model to file\n",
    "lm_model_file_name = 'lm_model.h5'\n",
    "word_enc_model_file_name = 'word_enc_model.h5'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 110
    },
    "colab_type": "code",
    "id": "YBWrzWtI_jkj",
    "outputId": "40b56a57-9d7b-45f1-b092-0ce1d8a6e373"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing LM loaded\n"
     ]
    }
   ],
   "source": [
    "load_prev_model = True\n",
    "filepath = os.path.join(gdrive_path, lm_model_file_name)\n",
    "if load_prev_model and os.path.exists(filepath):\n",
    "  model = load_model(filepath)  \n",
    "  #word_enc = load_model(os.path.join(gdrive_path, word_enc_model_file_name))  \n",
    "  print('Existing LM loaded')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-uhrWzGaJmey"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "class SaveWordEncoder(Callback):\n",
    "    '''\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.acc = []\n",
    "    '''\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "      word_enc.save(os.path.join(gdrive_path, word_enc_model_file_name))\n",
    "        \n",
    "\n",
    "word_enc_model_cbk = SaveWordEncoder()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZT-OqKCj_eSE"
   },
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(filepath, monitor='acc', verbose=1, save_best_only=True, mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CqLYjsAJALAV"
   },
   "outputs": [],
   "source": [
    "callbacks_lst = [word_enc_model_cbk, checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "RqINBIZUPMOy",
    "outputId": "9212a68d-7cb0-423e-886e-7288033112ba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport keras.backend as K\\nK.clear_session()\\nmodel = load_model(filepath)\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import keras.backend as K\n",
    "K.clear_session()\n",
    "model = load_model(filepath)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "8kwtJgBeA6KM",
    "outputId": "b0aa377d-5c54-4200-ff0d-3ee913ce3111"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "199949/199949 [==============================] - 315s 2ms/step - loss: 5.8001 - acc: 0.2549\n",
      "\n",
      "Epoch 00001: acc improved from -inf to 0.25485, saving model to ../dat/lm_model.h5\n",
      "Epoch 2/100\n",
      "199949/199949 [==============================] - 312s 2ms/step - loss: 5.7826 - acc: 0.2598\n",
      "\n",
      "Epoch 00002: acc improved from 0.25485 to 0.25983, saving model to ../dat/lm_model.h5\n",
      "Epoch 3/100\n",
      "199949/199949 [==============================] - 313s 2ms/step - loss: 5.7660 - acc: 0.2641\n",
      "\n",
      "Epoch 00003: acc improved from 0.25983 to 0.26414, saving model to ../dat/lm_model.h5\n",
      "Epoch 4/100\n",
      "199949/199949 [==============================] - 313s 2ms/step - loss: 5.7569 - acc: 0.2683\n",
      "\n",
      "Epoch 00004: acc improved from 0.26414 to 0.26828, saving model to ../dat/lm_model.h5\n",
      "Epoch 5/100\n",
      "199949/199949 [==============================] - 313s 2ms/step - loss: 5.7325 - acc: 0.2725\n",
      "\n",
      "Epoch 00005: acc improved from 0.26828 to 0.27248, saving model to ../dat/lm_model.h5\n",
      "Epoch 6/100\n",
      "199949/199949 [==============================] - 313s 2ms/step - loss: 5.6771 - acc: 0.2779\n",
      "\n",
      "Epoch 00006: acc improved from 0.27248 to 0.27787, saving model to ../dat/lm_model.h5\n",
      "Epoch 7/100\n",
      "199949/199949 [==============================] - 313s 2ms/step - loss: 5.6642 - acc: 0.2827\n",
      "\n",
      "Epoch 00007: acc improved from 0.27787 to 0.28275, saving model to ../dat/lm_model.h5\n",
      "Epoch 8/100\n",
      "199949/199949 [==============================] - 313s 2ms/step - loss: 5.6432 - acc: 0.2861\n",
      "\n",
      "Epoch 00008: acc improved from 0.28275 to 0.28610, saving model to ../dat/lm_model.h5\n",
      "Epoch 9/100\n",
      "199949/199949 [==============================] - 313s 2ms/step - loss: 5.6213 - acc: 0.2897\n",
      "\n",
      "Epoch 00009: acc improved from 0.28610 to 0.28974, saving model to ../dat/lm_model.h5\n",
      "Epoch 10/100\n",
      "199949/199949 [==============================] - 313s 2ms/step - loss: 5.6021 - acc: 0.2908\n",
      "\n",
      "Epoch 00010: acc improved from 0.28974 to 0.29076, saving model to ../dat/lm_model.h5\n",
      "Epoch 11/100\n",
      "199949/199949 [==============================] - 313s 2ms/step - loss: 5.5964 - acc: 0.2944\n",
      "\n",
      "Epoch 00011: acc improved from 0.29076 to 0.29437, saving model to ../dat/lm_model.h5\n",
      "Epoch 12/100\n",
      "199949/199949 [==============================] - 313s 2ms/step - loss: 5.5959 - acc: 0.2958\n",
      "\n",
      "Epoch 00012: acc improved from 0.29437 to 0.29584, saving model to ../dat/lm_model.h5\n",
      "Epoch 13/100\n",
      "199949/199949 [==============================] - 314s 2ms/step - loss: 5.5778 - acc: 0.2993\n",
      "\n",
      "Epoch 00013: acc improved from 0.29584 to 0.29934, saving model to ../dat/lm_model.h5\n",
      "Epoch 14/100\n",
      "199949/199949 [==============================] - 314s 2ms/step - loss: 5.5847 - acc: 0.3006\n",
      "\n",
      "Epoch 00014: acc improved from 0.29934 to 0.30064, saving model to ../dat/lm_model.h5\n",
      "Epoch 15/100\n",
      "199949/199949 [==============================] - 313s 2ms/step - loss: 5.5669 - acc: 0.3042\n",
      "\n",
      "Epoch 00015: acc improved from 0.30064 to 0.30416, saving model to ../dat/lm_model.h5\n",
      "Epoch 16/100\n",
      "199949/199949 [==============================] - 314s 2ms/step - loss: 5.5559 - acc: 0.3079\n",
      "\n",
      "Epoch 00016: acc improved from 0.30416 to 0.30787, saving model to ../dat/lm_model.h5\n",
      "Epoch 17/100\n",
      "199949/199949 [==============================] - 314s 2ms/step - loss: 5.5378 - acc: 0.3117\n",
      "\n",
      "Epoch 00017: acc improved from 0.30787 to 0.31174, saving model to ../dat/lm_model.h5\n",
      "Epoch 18/100\n",
      "199949/199949 [==============================] - 314s 2ms/step - loss: 5.5269 - acc: 0.3133\n",
      "\n",
      "Epoch 00018: acc improved from 0.31174 to 0.31333, saving model to ../dat/lm_model.h5\n",
      "Epoch 19/100\n",
      "199949/199949 [==============================] - 313s 2ms/step - loss: 5.5250 - acc: 0.3181\n",
      "\n",
      "Epoch 00019: acc improved from 0.31333 to 0.31815, saving model to ../dat/lm_model.h5\n",
      "Epoch 20/100\n",
      "199949/199949 [==============================] - 314s 2ms/step - loss: 5.5376 - acc: 0.3192\n",
      "\n",
      "Epoch 00020: acc improved from 0.31815 to 0.31916, saving model to ../dat/lm_model.h5\n",
      "Epoch 21/100\n",
      "199949/199949 [==============================] - 314s 2ms/step - loss: 5.4994 - acc: 0.3219\n",
      "\n",
      "Epoch 00021: acc improved from 0.31916 to 0.32194, saving model to ../dat/lm_model.h5\n",
      "Epoch 22/100\n",
      "199949/199949 [==============================] - 314s 2ms/step - loss: 5.4677 - acc: 0.3285\n",
      "\n",
      "Epoch 00022: acc improved from 0.32194 to 0.32850, saving model to ../dat/lm_model.h5\n",
      "Epoch 23/100\n",
      "199949/199949 [==============================] - 314s 2ms/step - loss: 5.4695 - acc: 0.3294\n",
      "\n",
      "Epoch 00023: acc improved from 0.32850 to 0.32941, saving model to ../dat/lm_model.h5\n",
      "Epoch 24/100\n",
      "199949/199949 [==============================] - 314s 2ms/step - loss: 5.4592 - acc: 0.3320\n",
      "\n",
      "Epoch 00024: acc improved from 0.32941 to 0.33197, saving model to ../dat/lm_model.h5\n",
      "Epoch 25/100\n",
      "199949/199949 [==============================] - 325s 2ms/step - loss: 5.4385 - acc: 0.3368\n",
      "\n",
      "Epoch 00025: acc improved from 0.33197 to 0.33684, saving model to ../dat/lm_model.h5\n",
      "Epoch 26/100\n",
      "199949/199949 [==============================] - 314s 2ms/step - loss: 5.4372 - acc: 0.3398\n",
      "\n",
      "Epoch 00026: acc improved from 0.33684 to 0.33980, saving model to ../dat/lm_model.h5\n",
      "Epoch 27/100\n",
      "199949/199949 [==============================] - 314s 2ms/step - loss: 5.4220 - acc: 0.3417\n",
      "\n",
      "Epoch 00027: acc improved from 0.33980 to 0.34168, saving model to ../dat/lm_model.h5\n",
      "Epoch 28/100\n",
      "199949/199949 [==============================] - 314s 2ms/step - loss: 5.4068 - acc: 0.3419\n",
      "\n",
      "Epoch 00028: acc improved from 0.34168 to 0.34194, saving model to ../dat/lm_model.h5\n",
      "Epoch 29/100\n",
      "199949/199949 [==============================] - 314s 2ms/step - loss: 5.3919 - acc: 0.3427\n",
      "\n",
      "Epoch 00029: acc improved from 0.34194 to 0.34265, saving model to ../dat/lm_model.h5\n",
      "Epoch 30/100\n",
      "199949/199949 [==============================] - 314s 2ms/step - loss: 5.3658 - acc: 0.3465\n",
      "\n",
      "Epoch 00030: acc improved from 0.34265 to 0.34647, saving model to ../dat/lm_model.h5\n",
      "Epoch 31/100\n",
      "199949/199949 [==============================] - 314s 2ms/step - loss: 5.3686 - acc: 0.3480\n",
      "\n",
      "Epoch 00031: acc improved from 0.34647 to 0.34799, saving model to ../dat/lm_model.h5\n",
      "Epoch 32/100\n",
      "199949/199949 [==============================] - 314s 2ms/step - loss: 5.3847 - acc: 0.3470\n",
      "\n",
      "Epoch 00032: acc did not improve from 0.34799\n",
      "Epoch 33/100\n",
      "199949/199949 [==============================] - 314s 2ms/step - loss: 5.3785 - acc: 0.3477\n",
      "\n",
      "Epoch 00033: acc did not improve from 0.34799\n",
      "Epoch 34/100\n",
      "199949/199949 [==============================] - 316s 2ms/step - loss: 5.3754 - acc: 0.3479\n",
      "\n",
      "Epoch 00034: acc did not improve from 0.34799\n",
      "Epoch 35/100\n",
      "199949/199949 [==============================] - 323s 2ms/step - loss: 5.3451 - acc: 0.3522\n",
      "\n",
      "Epoch 00035: acc improved from 0.34799 to 0.35224, saving model to ../dat/lm_model.h5\n",
      "Epoch 36/100\n",
      "199949/199949 [==============================] - 315s 2ms/step - loss: 5.3442 - acc: 0.3554\n",
      "\n",
      "Epoch 00036: acc improved from 0.35224 to 0.35539, saving model to ../dat/lm_model.h5\n",
      "Epoch 37/100\n",
      "199949/199949 [==============================] - 313s 2ms/step - loss: 5.3391 - acc: 0.3543\n",
      "\n",
      "Epoch 00037: acc did not improve from 0.35539\n",
      "Epoch 38/100\n",
      "199949/199949 [==============================] - 313s 2ms/step - loss: 5.3036 - acc: 0.3582\n",
      "\n",
      "Epoch 00038: acc improved from 0.35539 to 0.35815, saving model to ../dat/lm_model.h5\n",
      "Epoch 39/100\n",
      "199949/199949 [==============================] - 314s 2ms/step - loss: 5.3305 - acc: 0.3567\n",
      "\n",
      "Epoch 00039: acc did not improve from 0.35815\n",
      "Epoch 40/100\n",
      "199949/199949 [==============================] - 314s 2ms/step - loss: 5.3191 - acc: 0.3589\n",
      "\n",
      "Epoch 00040: acc improved from 0.35815 to 0.35888, saving model to ../dat/lm_model.h5\n",
      "Epoch 41/100\n",
      "199949/199949 [==============================] - 314s 2ms/step - loss: 5.3353 - acc: 0.3569\n",
      "\n",
      "Epoch 00041: acc did not improve from 0.35888\n",
      "Epoch 42/100\n",
      "199949/199949 [==============================] - 314s 2ms/step - loss: 5.2820 - acc: 0.3612\n",
      "\n",
      "Epoch 00042: acc improved from 0.35888 to 0.36119, saving model to ../dat/lm_model.h5\n",
      "Epoch 43/100\n",
      "199949/199949 [==============================] - 313s 2ms/step - loss: 5.2883 - acc: 0.3629\n",
      "\n",
      "Epoch 00043: acc improved from 0.36119 to 0.36287, saving model to ../dat/lm_model.h5\n",
      "Epoch 44/100\n",
      "199949/199949 [==============================] - 314s 2ms/step - loss: 5.3101 - acc: 0.3603\n",
      "\n",
      "Epoch 00044: acc did not improve from 0.36287\n",
      "Epoch 45/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199949/199949 [==============================] - 313s 2ms/step - loss: 5.3247 - acc: 0.3614\n",
      "\n",
      "Epoch 00045: acc did not improve from 0.36287\n",
      "Epoch 46/100\n",
      "199949/199949 [==============================] - 313s 2ms/step - loss: 5.3254 - acc: 0.3617\n",
      "\n",
      "Epoch 00046: acc did not improve from 0.36287\n",
      "Epoch 47/100\n",
      "199949/199949 [==============================] - 313s 2ms/step - loss: 5.3342 - acc: 0.3622\n",
      "\n",
      "Epoch 00047: acc did not improve from 0.36287\n",
      "Epoch 48/100\n",
      "199949/199949 [==============================] - 313s 2ms/step - loss: 5.3221 - acc: 0.3619\n",
      "\n",
      "Epoch 00048: acc did not improve from 0.36287\n",
      "Epoch 49/100\n",
      "199949/199949 [==============================] - 313s 2ms/step - loss: 5.3020 - acc: 0.3640\n",
      "\n",
      "Epoch 00049: acc improved from 0.36287 to 0.36397, saving model to ../dat/lm_model.h5\n",
      "Epoch 50/100\n",
      "199949/199949 [==============================] - 312s 2ms/step - loss: 5.2734 - acc: 0.3665\n",
      "\n",
      "Epoch 00050: acc improved from 0.36397 to 0.36654, saving model to ../dat/lm_model.h5\n",
      "Epoch 51/100\n",
      "199949/199949 [==============================] - 312s 2ms/step - loss: 5.2270 - acc: 0.3717\n",
      "\n",
      "Epoch 00051: acc improved from 0.36654 to 0.37172, saving model to ../dat/lm_model.h5\n",
      "Epoch 52/100\n",
      "199949/199949 [==============================] - 312s 2ms/step - loss: 5.2185 - acc: 0.3745\n",
      "\n",
      "Epoch 00052: acc improved from 0.37172 to 0.37452, saving model to ../dat/lm_model.h5\n",
      "Epoch 53/100\n",
      "199949/199949 [==============================] - 312s 2ms/step - loss: 5.2305 - acc: 0.3740\n",
      "\n",
      "Epoch 00053: acc did not improve from 0.37452\n",
      "Epoch 54/100\n",
      "199949/199949 [==============================] - 312s 2ms/step - loss: 5.2371 - acc: 0.3759\n",
      "\n",
      "Epoch 00054: acc improved from 0.37452 to 0.37587, saving model to ../dat/lm_model.h5\n",
      "Epoch 55/100\n",
      "199949/199949 [==============================] - 312s 2ms/step - loss: 5.2640 - acc: 0.3754\n",
      "\n",
      "Epoch 00055: acc did not improve from 0.37587\n",
      "Epoch 56/100\n",
      "199949/199949 [==============================] - 312s 2ms/step - loss: 5.2590 - acc: 0.3756\n",
      "\n",
      "Epoch 00056: acc did not improve from 0.37587\n",
      "Epoch 57/100\n",
      "199949/199949 [==============================] - 312s 2ms/step - loss: 5.2653 - acc: 0.3753\n",
      "\n",
      "Epoch 00057: acc did not improve from 0.37587\n",
      "Epoch 58/100\n",
      "199949/199949 [==============================] - 312s 2ms/step - loss: 5.2760 - acc: 0.3742\n",
      "\n",
      "Epoch 00058: acc did not improve from 0.37587\n",
      "Epoch 59/100\n",
      "199949/199949 [==============================] - 312s 2ms/step - loss: 5.2688 - acc: 0.3738\n",
      "\n",
      "Epoch 00059: acc did not improve from 0.37587\n",
      "Epoch 60/100\n",
      "199949/199949 [==============================] - 312s 2ms/step - loss: 5.2679 - acc: 0.3739\n",
      "\n",
      "Epoch 00060: acc did not improve from 0.37587\n",
      "Epoch 61/100\n",
      "199949/199949 [==============================] - 312s 2ms/step - loss: 5.2605 - acc: 0.3742\n",
      "\n",
      "Epoch 00061: acc did not improve from 0.37587\n",
      "Epoch 62/100\n",
      "199949/199949 [==============================] - 312s 2ms/step - loss: 5.2480 - acc: 0.3749\n",
      "\n",
      "Epoch 00062: acc did not improve from 0.37587\n",
      "Epoch 63/100\n",
      "199949/199949 [==============================] - 312s 2ms/step - loss: 5.2138 - acc: 0.3756\n",
      "\n",
      "Epoch 00063: acc did not improve from 0.37587\n",
      "Epoch 64/100\n",
      "199949/199949 [==============================] - 312s 2ms/step - loss: 5.1920 - acc: 0.3787\n",
      "\n",
      "Epoch 00064: acc improved from 0.37587 to 0.37868, saving model to ../dat/lm_model.h5\n",
      "Epoch 65/100\n",
      "199949/199949 [==============================] - 312s 2ms/step - loss: 5.2454 - acc: 0.3762\n",
      "\n",
      "Epoch 00065: acc did not improve from 0.37868\n",
      "Epoch 66/100\n",
      "199949/199949 [==============================] - 312s 2ms/step - loss: 5.2044 - acc: 0.3811\n",
      "\n",
      "Epoch 00066: acc improved from 0.37868 to 0.38114, saving model to ../dat/lm_model.h5\n",
      "Epoch 67/100\n",
      "199949/199949 [==============================] - 312s 2ms/step - loss: 5.2090 - acc: 0.3817\n",
      "\n",
      "Epoch 00067: acc improved from 0.38114 to 0.38174, saving model to ../dat/lm_model.h5\n",
      "Epoch 68/100\n",
      "199949/199949 [==============================] - 312s 2ms/step - loss: 5.2169 - acc: 0.3817\n",
      "\n",
      "Epoch 00068: acc did not improve from 0.38174\n",
      "Epoch 69/100\n",
      "199949/199949 [==============================] - 312s 2ms/step - loss: 5.2199 - acc: 0.3815\n",
      "\n",
      "Epoch 00069: acc did not improve from 0.38174\n",
      "Epoch 70/100\n",
      "199949/199949 [==============================] - 312s 2ms/step - loss: 5.1933 - acc: 0.3843\n",
      "\n",
      "Epoch 00070: acc improved from 0.38174 to 0.38433, saving model to ../dat/lm_model.h5\n",
      "Epoch 71/100\n",
      "199949/199949 [==============================] - 312s 2ms/step - loss: 5.1930 - acc: 0.3844\n",
      "\n",
      "Epoch 00071: acc improved from 0.38433 to 0.38445, saving model to ../dat/lm_model.h5\n",
      "Epoch 72/100\n",
      "199949/199949 [==============================] - 312s 2ms/step - loss: 5.2072 - acc: 0.3837\n",
      "\n",
      "Epoch 00072: acc did not improve from 0.38445\n",
      "Epoch 73/100\n",
      "199949/199949 [==============================] - 312s 2ms/step - loss: 5.2112 - acc: 0.3850\n",
      "\n",
      "Epoch 00073: acc improved from 0.38445 to 0.38503, saving model to ../dat/lm_model.h5\n",
      "Epoch 74/100\n",
      "199949/199949 [==============================] - 312s 2ms/step - loss: 5.1936 - acc: 0.3862\n",
      "\n",
      "Epoch 00074: acc improved from 0.38503 to 0.38624, saving model to ../dat/lm_model.h5\n",
      "Epoch 75/100\n",
      "199949/199949 [==============================] - 312s 2ms/step - loss: 5.1811 - acc: 0.3861\n",
      "\n",
      "Epoch 00075: acc did not improve from 0.38624\n",
      "Epoch 76/100\n",
      "199949/199949 [==============================] - 312s 2ms/step - loss: 5.1705 - acc: 0.3849\n",
      "\n",
      "Epoch 00076: acc did not improve from 0.38624\n",
      "Epoch 77/100\n",
      "199949/199949 [==============================] - 312s 2ms/step - loss: 5.2034 - acc: 0.3843\n",
      "\n",
      "Epoch 00077: acc did not improve from 0.38624\n",
      "Epoch 78/100\n",
      "199949/199949 [==============================] - 312s 2ms/step - loss: 5.1716 - acc: 0.3892\n",
      "\n",
      "Epoch 00078: acc improved from 0.38624 to 0.38916, saving model to ../dat/lm_model.h5\n",
      "Epoch 79/100\n",
      "199949/199949 [==============================] - 312s 2ms/step - loss: 5.1659 - acc: 0.3924\n",
      "\n",
      "Epoch 00079: acc improved from 0.38916 to 0.39238, saving model to ../dat/lm_model.h5\n",
      "Epoch 80/100\n",
      "199949/199949 [==============================] - 312s 2ms/step - loss: 5.1687 - acc: 0.3908\n",
      "\n",
      "Epoch 00080: acc did not improve from 0.39238\n",
      "Epoch 81/100\n",
      "199949/199949 [==============================] - 312s 2ms/step - loss: 5.1686 - acc: 0.3914\n",
      "\n",
      "Epoch 00081: acc did not improve from 0.39238\n",
      "Epoch 82/100\n",
      "199949/199949 [==============================] - 312s 2ms/step - loss: 5.1702 - acc: 0.3940\n",
      "\n",
      "Epoch 00082: acc improved from 0.39238 to 0.39399, saving model to ../dat/lm_model.h5\n",
      "Epoch 83/100\n",
      "199949/199949 [==============================] - 312s 2ms/step - loss: 5.1549 - acc: 0.3955\n",
      "\n",
      "Epoch 00083: acc improved from 0.39399 to 0.39552, saving model to ../dat/lm_model.h5\n",
      "Epoch 84/100\n",
      "199949/199949 [==============================] - 312s 2ms/step - loss: 5.1613 - acc: 0.3946\n",
      "\n",
      "Epoch 00084: acc did not improve from 0.39552\n",
      "Epoch 85/100\n",
      "199949/199949 [==============================] - 312s 2ms/step - loss: 5.1512 - acc: 0.3960\n",
      "\n",
      "Epoch 00085: acc improved from 0.39552 to 0.39596, saving model to ../dat/lm_model.h5\n",
      "Epoch 86/100\n",
      "199949/199949 [==============================] - 312s 2ms/step - loss: 5.1621 - acc: 0.3945\n",
      "\n",
      "Epoch 00086: acc did not improve from 0.39596\n",
      "Epoch 87/100\n",
      "199949/199949 [==============================] - 313s 2ms/step - loss: 5.1460 - acc: 0.3958\n",
      "\n",
      "Epoch 00087: acc did not improve from 0.39596\n",
      "Epoch 88/100\n",
      "199949/199949 [==============================] - 312s 2ms/step - loss: 5.1879 - acc: 0.3910\n",
      "\n",
      "Epoch 00088: acc did not improve from 0.39596\n",
      "Epoch 89/100\n",
      "199949/199949 [==============================] - 312s 2ms/step - loss: 5.1557 - acc: 0.3948\n",
      "\n",
      "Epoch 00089: acc did not improve from 0.39596\n",
      "Epoch 90/100\n",
      "199949/199949 [==============================] - 312s 2ms/step - loss: 5.1667 - acc: 0.3952\n",
      "\n",
      "Epoch 00090: acc did not improve from 0.39596\n",
      "Epoch 91/100\n",
      "199949/199949 [==============================] - 312s 2ms/step - loss: 5.1925 - acc: 0.3923\n",
      "\n",
      "Epoch 00091: acc did not improve from 0.39596\n",
      "Epoch 92/100\n",
      "199949/199949 [==============================] - 312s 2ms/step - loss: 5.1774 - acc: 0.3943\n",
      "\n",
      "Epoch 00092: acc did not improve from 0.39596\n",
      "Epoch 93/100\n",
      "199949/199949 [==============================] - 312s 2ms/step - loss: 5.2045 - acc: 0.3925\n",
      "\n",
      "Epoch 00093: acc did not improve from 0.39596\n",
      "Epoch 94/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199949/199949 [==============================] - 312s 2ms/step - loss: 5.2075 - acc: 0.3901\n",
      "\n",
      "Epoch 00094: acc did not improve from 0.39596\n",
      "Epoch 95/100\n",
      "199949/199949 [==============================] - 312s 2ms/step - loss: 5.1651 - acc: 0.3941\n",
      "\n",
      "Epoch 00095: acc did not improve from 0.39596\n",
      "Epoch 96/100\n",
      "199949/199949 [==============================] - 312s 2ms/step - loss: 5.1468 - acc: 0.3962\n",
      "\n",
      "Epoch 00096: acc improved from 0.39596 to 0.39625, saving model to ../dat/lm_model.h5\n",
      "Epoch 97/100\n",
      "199949/199949 [==============================] - 312s 2ms/step - loss: 5.2204 - acc: 0.3888\n",
      "\n",
      "Epoch 00097: acc did not improve from 0.39625\n",
      "Epoch 98/100\n",
      "199949/199949 [==============================] - 312s 2ms/step - loss: 5.1695 - acc: 0.3935\n",
      "\n",
      "Epoch 00098: acc did not improve from 0.39625\n",
      "Epoch 99/100\n",
      "199949/199949 [==============================] - 312s 2ms/step - loss: 5.1579 - acc: 0.3950\n",
      "\n",
      "Epoch 00099: acc did not improve from 0.39625\n",
      "Epoch 100/100\n",
      "199949/199949 [==============================] - 312s 2ms/step - loss: 5.1519 - acc: 0.3955\n",
      "\n",
      "Epoch 00100: acc did not improve from 0.39625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7faa084a7a90>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# fit model\n",
    "model.fit(X, y, batch_size=128, epochs=100, callbacks=callbacks_lst)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4MCo2QuBKB9R"
   },
   "outputs": [],
   "source": [
    "word_enc.save(os.path.join(gdrive_path, word_enc_model_file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YYFCfBaPSZgP"
   },
   "source": [
    "# HATT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sXEuwCHEUe7D"
   },
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6dqlE0HwSbyS"
   },
   "outputs": [],
   "source": [
    "from nltk import tokenize\n",
    "def prepare_hier_data(in_texts, in_labels):\n",
    "    \n",
    "    reviews = []\n",
    "    labels = []\n",
    "    texts = []\n",
    "    \n",
    "    for idx in range(len(in_texts)):\n",
    "        text = in_texts[idx]\n",
    "        label = in_labels[idx]\n",
    "        if label != 2:\n",
    "          #print('Parsing review ' + str(idx))\n",
    "          texts.append(text)\n",
    "          sentences = tokenize.sent_tokenize(text)\n",
    "          reviews.append(sentences)       \n",
    "          labels.append(label)\n",
    "    return reviews, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J4thy4I0UNGH"
   },
   "outputs": [],
   "source": [
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "def binarize_hier_data(reviews, labels, tokenizer):\n",
    "    data_lst = []\n",
    "    labels_lst = []\n",
    "    for i, sentences in enumerate(reviews):\n",
    "        data = UNK_ID * np.ones((MAX_SENTS, MAX_SENT_LENGTH), dtype='int32') # Init all as UNK\n",
    "        for j, sent in enumerate(sentences):\n",
    "            if j< MAX_SENTS:\n",
    "                wordTokens = text_to_word_sequence(sent)\n",
    "                k=0\n",
    "                for _, word in enumerate(wordTokens):\n",
    "                    if word in tokenizer.word_index:\n",
    "                      if k<MAX_SENT_LENGTH and tokenizer.word_index[word]<MAX_NB_WORDS:\n",
    "                          data[j,k] = tokenizer.word_index[word]\n",
    "                          k=k+1\n",
    "        data_lst.append(data)\n",
    "        labels_lst.append(labels[i])\n",
    "    data = np.array(data_lst)\n",
    "    targets = np.array(labels_lst) \n",
    "    targets = to_categorical(np.asarray(targets))\n",
    "    return data, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LOrdvgToVZML"
   },
   "outputs": [],
   "source": [
    "train_texts_, train_labels_ = prepare_hier_data(trn_texts, trn_labels)\n",
    "train_data, train_targets = binarize_hier_data(train_texts_, train_labels_, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8eZdCDnkZZpP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (25000, 15, 50)\n",
      "Shape of label tensor: (25000, 2)\n"
     ]
    }
   ],
   "source": [
    "# 25k only are training out of 75k, becasue 50k are unsup --> label = 2\n",
    "print('Shape of data tensor:', train_data.shape)\n",
    "print('Shape of label tensor:', train_targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QASX84GaagR-"
   },
   "source": [
    "## Split train/val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OjzhWOSjZwu_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive and negative reviews in traing and validation set\n",
      "[10012.  9988.]\n",
      "[2488. 2512.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "indices = np.arange(train_data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "train_data = train_data[indices]\n",
    "train_targets = train_targets[indices]\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * train_data.shape[0])\n",
    "\n",
    "x_train = train_data[:-nb_validation_samples]\n",
    "y_train = train_targets[:-nb_validation_samples]\n",
    "x_val = train_data[-nb_validation_samples:]\n",
    "y_val = train_targets[-nb_validation_samples:]\n",
    "\n",
    "print('Number of positive and negative reviews in traing and validation set')\n",
    "print(y_train.sum(axis=0))\n",
    "print(y_val.sum(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "82YW-4Tkat1w"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jKu5d4fQB4Fs"
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "def load_word_enc_model(word_enc_model_file_name):\n",
    "    word_enc_model = load_model(word_enc_model_file_name)\n",
    "\n",
    "    '''\n",
    "    embeddings_file = os.path.join(GLOVE_DIR, 'glove.6B.100d.txt')\n",
    "    embedding_matrix = load_embeddings(embeddings_file)\n",
    "\n",
    "\n",
    "    embedding_layer = Embedding(VOCAB_SIZE+1,\n",
    "                              EMBEDDING_DIM,\n",
    "                              weights=[embedding_matrix],\n",
    "                              input_length=MAX_SENT_LENGTH,\n",
    "                              trainable=True)\n",
    "    sentence_input = Input(shape=(LM_SEQ_LEN,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sentence_input)\n",
    "    l_lstm = Bidirectional(GRU(100, return_sequences=True))(embedded_sequences)\n",
    "    l_word_enc = TimeDistributed(Dense(200))(l_lstm)\n",
    "\n",
    "\n",
    "    word_enc_model = Model(sentence_input, l_word_enc)  \n",
    "    '''\n",
    "    print(word_enc_model.summary())\n",
    "    return word_enc_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vaHckN9PBacV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 50, 100)           6000100   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 50, 200)           120600    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 50, 200)           40200     \n",
      "=================================================================\n",
      "Total params: 6,160,900\n",
      "Trainable params: 6,160,900\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmad/anaconda3/lib/python3.6/site-packages/keras/engine/saving.py:269: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "\n",
    "word_enc_model = load_word_enc_model(os.path.join(gdrive_path, word_enc_model_file_name))#model.load(word_enc_model_file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zzxO4B8IasVR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 7396 word vectors.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "embeddings_file_name = os.path.join(GLOVE_DIR, 'glove.6B.100d.txt')\n",
    "\n",
    "# building Hierachical Attention network\n",
    "embedding_matrix = load_embeddings(embeddings_file_name)\n",
    "\n",
    "        \n",
    "embedding_layer = Embedding(VOCAB_SIZE,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SENT_LENGTH,\n",
    "                            trainable=True)\n",
    "\n",
    "class AttLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.init = initializers.he_normal()\n",
    "        super(AttLayer, self).__init__(**kwargs)\n",
    "    '''\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape)==3\n",
    "        self.W = self.init((input_shape[-1],1))\n",
    "        self.trainable_weights = [self.W]\n",
    "        super(AttLayer, self).build(input_shape)  # be sure you call this somewhere!\n",
    "    '''\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape)==3\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.W = self.add_weight(name='kernel', \n",
    "                                      shape=(input_shape[-1], 1),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        super(AttLayer, self).build(input_shape)  # Be sure to call this at the end\n",
    "    \n",
    "    def call(self, x, mask=None):\n",
    "        eij = K.tanh(K.dot(x, self.W))\n",
    "        \n",
    "        ai = K.exp(eij)\n",
    "        weights = ai/tf.expand_dims(K.sum(ai, axis=1), 1)\n",
    "        \n",
    "        weighted_input = x*weights\n",
    "        return tf.reduce_sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "\n",
    "sentence_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\n",
    "'''\n",
    "embedded_sequences = embedding_layer(sentence_input)\n",
    "l_lstm = Bidirectional(GRU(100, return_sequences=True))(embedded_sequences)\n",
    "l_dense = TimeDistributed(Dense(200))(l_lstm)\n",
    "'''\n",
    "l_dense = word_enc_model(sentence_input)\n",
    "l_att = AttLayer()(l_dense)\n",
    "sentEncoder = Model(sentence_input, l_att)\n",
    "\n",
    "review_input = Input(shape=(MAX_SENTS,MAX_SENT_LENGTH), dtype='int32')\n",
    "review_encoder = TimeDistributed(sentEncoder)(review_input)\n",
    "l_lstm_sent = Bidirectional(GRU(100, return_sequences=True))(review_encoder)\n",
    "l_dense_sent = TimeDistributed(Dense(200))(l_lstm_sent)\n",
    "l_att_sent = AttLayer()(l_dense_sent)\n",
    "preds = Dense(2, activation='softmax')(l_att_sent)\n",
    "model = Model(review_input, preds)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1YwHgN2BZZn_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_17 (InputLayer)        (None, 15, 50)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_16 (TimeDis (None, 15, 200)           6161100   \n",
      "_________________________________________________________________\n",
      "bidirectional_10 (Bidirectio (None, 15, 200)           180600    \n",
      "_________________________________________________________________\n",
      "time_distributed_17 (TimeDis (None, 15, 200)           40200     \n",
      "_________________________________________________________________\n",
      "att_layer_14 (AttLayer)      (None, 200)               200       \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 2)                 402       \n",
      "=================================================================\n",
      "Total params: 6,382,502\n",
      "Trainable params: 221,602\n",
      "Non-trainable params: 6,160,900\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GEOcjyCjAYmc"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown layer: AttLayer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-e83e2cc1f9a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgdrive_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhatt_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mload_prev_model\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'HATT model loaded'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No model found in config file.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0mmodel_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_from_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;31m# set weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mmodel_from_config\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m    332\u001b[0m                         '`Sequential.from_config(config)`?')\n\u001b[1;32m    333\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/layers/__init__.py\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m     53\u001b[0m                                     \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                                     \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m                                     printable_module_name='layer')\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    143\u001b[0m                     \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'config'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                     custom_objects=dict(list(_GLOBAL_CUSTOM_OBJECTS.items()) +\n\u001b[0;32m--> 145\u001b[0;31m                                         list(custom_objects.items())))\n\u001b[0m\u001b[1;32m    146\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mCustomObjectScope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'config'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m   1015\u001b[0m         \u001b[0;31m# First, we create all layers and enqueue nodes to be processed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'layers'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1017\u001b[0;31m             \u001b[0mprocess_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1018\u001b[0m         \u001b[0;31m# Then we process nodes in order of layer depth.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m         \u001b[0;31m# Nodes that cannot yet be processed (if the inbound node\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36mprocess_layer\u001b[0;34m(layer_data)\u001b[0m\n\u001b[1;32m   1001\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m             layer = deserialize_layer(layer_data,\n\u001b[0;32m-> 1003\u001b[0;31m                                       custom_objects=custom_objects)\n\u001b[0m\u001b[1;32m   1004\u001b[0m             \u001b[0mcreated_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/layers/__init__.py\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m     53\u001b[0m                                     \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                                     \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m                                     printable_module_name='layer')\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    143\u001b[0m                     \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'config'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                     custom_objects=dict(list(_GLOBAL_CUSTOM_OBJECTS.items()) +\n\u001b[0;32m--> 145\u001b[0;31m                                         list(custom_objects.items())))\n\u001b[0m\u001b[1;32m    146\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mCustomObjectScope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'config'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/layers/wrappers.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeserialize\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdeserialize_layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         layer = deserialize_layer(config.pop('layer'),\n\u001b[0;32m--> 110\u001b[0;31m                                   custom_objects=custom_objects)\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/layers/__init__.py\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m     53\u001b[0m                                     \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                                     \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m                                     printable_module_name='layer')\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    143\u001b[0m                     \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'config'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                     custom_objects=dict(list(_GLOBAL_CUSTOM_OBJECTS.items()) +\n\u001b[0;32m--> 145\u001b[0;31m                                         list(custom_objects.items())))\n\u001b[0m\u001b[1;32m    146\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mCustomObjectScope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'config'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m   1015\u001b[0m         \u001b[0;31m# First, we create all layers and enqueue nodes to be processed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'layers'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1017\u001b[0;31m             \u001b[0mprocess_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1018\u001b[0m         \u001b[0;31m# Then we process nodes in order of layer depth.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m         \u001b[0;31m# Nodes that cannot yet be processed (if the inbound node\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36mprocess_layer\u001b[0;34m(layer_data)\u001b[0m\n\u001b[1;32m   1001\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m             layer = deserialize_layer(layer_data,\n\u001b[0;32m-> 1003\u001b[0;31m                                       custom_objects=custom_objects)\n\u001b[0m\u001b[1;32m   1004\u001b[0m             \u001b[0mcreated_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/layers/__init__.py\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m     53\u001b[0m                                     \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                                     \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m                                     printable_module_name='layer')\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                 raise ValueError('Unknown ' + printable_module_name +\n\u001b[0;32m--> 138\u001b[0;31m                                  ': ' + class_name)\n\u001b[0m\u001b[1;32m    139\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'from_config'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0mcustom_objects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcustom_objects\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown layer: AttLayer"
     ]
    }
   ],
   "source": [
    "hatt_model = 'hatt_model.h5'\n",
    "load_prev_model = False\n",
    "filepath = os.path.join(gdrive_path, hatt_model)\n",
    "if load_prev_model and os.path.exists(filepath):\n",
    "    model = load_model(filepath) \n",
    "    print('HATT model loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RPJMtx9yAd88"
   },
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g5G_WMCuAm05"
   },
   "outputs": [],
   "source": [
    "callbacks_lst = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_enc_model.trainable = False\n",
    "model.summary()\n",
    "# Must call compile for trainable to take effect\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JQUZsTvmgxH9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model fitting - Hierachical attention network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmad/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/100\n",
      "20000/20000 [==============================] - 37s 2ms/step - loss: 0.7511 - acc: 0.5090 - val_loss: 0.6887 - val_acc: 0.5396\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.53960, saving model to ../dat/lm_model.h5\n",
      "Epoch 2/100\n",
      "20000/20000 [==============================] - 33s 2ms/step - loss: 0.6930 - acc: 0.5378 - val_loss: 0.6834 - val_acc: 0.5542\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.53960 to 0.55420, saving model to ../dat/lm_model.h5\n",
      "Epoch 3/100\n",
      "20000/20000 [==============================] - 33s 2ms/step - loss: 0.6852 - acc: 0.5570 - val_loss: 0.6838 - val_acc: 0.5502\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.55420\n",
      "Epoch 4/100\n",
      " 2450/20000 [==>...........................] - ETA: 25s - loss: 0.6823 - acc: 0.5567"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-7f8d64423e82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model fitting - Hierachical attention network\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_lst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2664\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m                                 session)\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1400\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 50\n",
    "print(\"model fitting - Hierachical attention network\")\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val), nb_epoch=NUM_EPOCHS, batch_size=BATCH_SIZE, callbacks=callbacks_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_17 (InputLayer)        (None, 15, 50)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_16 (TimeDis (None, 15, 200)           6161100   \n",
      "_________________________________________________________________\n",
      "bidirectional_10 (Bidirectio (None, 15, 200)           180600    \n",
      "_________________________________________________________________\n",
      "time_distributed_17 (TimeDis (None, 15, 200)           40200     \n",
      "_________________________________________________________________\n",
      "att_layer_14 (AttLayer)      (None, 200)               200       \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 2)                 402       \n",
      "=================================================================\n",
      "Total params: 221,602\n",
      "Trainable params: 221,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmad/anaconda3/lib/python3.6/site-packages/keras/engine/training.py:479: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    }
   ],
   "source": [
    "word_enc_model.trainable = True\n",
    "model.summary()\n",
    "# Must call compile for trainable to take effect\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model fitting - Hierachical attention network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmad/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/100\n",
      "20000/20000 [==============================] - 73s 4ms/step - loss: 0.6258 - acc: 0.6221 - val_loss: 0.4547 - val_acc: 0.7994\n",
      "\n",
      "Epoch 00001: val_acc improved from 0.59380 to 0.79940, saving model to ../dat/lm_model.h5\n",
      "Epoch 2/100\n",
      "20000/20000 [==============================] - 67s 3ms/step - loss: 0.3876 - acc: 0.8304 - val_loss: 0.4232 - val_acc: 0.8124\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.79940 to 0.81240, saving model to ../dat/lm_model.h5\n",
      "Epoch 3/100\n",
      "20000/20000 [==============================] - 67s 3ms/step - loss: 0.2696 - acc: 0.8913 - val_loss: 0.4126 - val_acc: 0.8214\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.81240 to 0.82140, saving model to ../dat/lm_model.h5\n",
      "Epoch 4/100\n",
      "20000/20000 [==============================] - 67s 3ms/step - loss: 0.1841 - acc: 0.9288 - val_loss: 0.3664 - val_acc: 0.8512\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.82140 to 0.85120, saving model to ../dat/lm_model.h5\n",
      "Epoch 5/100\n",
      "20000/20000 [==============================] - 67s 3ms/step - loss: 0.1137 - acc: 0.9592 - val_loss: 0.4240 - val_acc: 0.8538\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.85120 to 0.85380, saving model to ../dat/lm_model.h5\n",
      "Epoch 6/100\n",
      "20000/20000 [==============================] - 67s 3ms/step - loss: 0.0621 - acc: 0.9791 - val_loss: 0.5717 - val_acc: 0.8622\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.85380 to 0.86220, saving model to ../dat/lm_model.h5\n",
      "Epoch 7/100\n",
      "20000/20000 [==============================] - 67s 3ms/step - loss: 0.0365 - acc: 0.9878 - val_loss: 0.5098 - val_acc: 0.8556\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.86220\n",
      "Epoch 8/100\n",
      "20000/20000 [==============================] - 67s 3ms/step - loss: 0.0207 - acc: 0.9946 - val_loss: 0.6929 - val_acc: 0.8580\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.86220\n",
      "Epoch 9/100\n",
      " 6000/20000 [========>.....................] - ETA: 44s - loss: 0.0105 - acc: 0.9978"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-7f8d64423e82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model fitting - Hierachical attention network\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_lst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2664\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m                                 session)\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1400\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 50\n",
    "print(\"model fitting - Hierachical attention network\")\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val), nb_epoch=NUM_EPOCHS, batch_size=BATCH_SIZE, callbacks=callbacks_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CIUs79avhOIx"
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FUzWac-3ZZmi"
   },
   "outputs": [],
   "source": [
    "test_texts_, test_labels_ = prepare_hier_data(val_texts, val_labels)\n",
    "test_data, test_targets = binarize_hier_data(test_texts_, test_labels_, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wcabJk6gGea8"
   },
   "outputs": [],
   "source": [
    "\n",
    "print('Shape of data tensor:', test_data.shape)\n",
    "print('Shape of label tensor:', test_targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nY8VkCYvhUc6"
   },
   "outputs": [],
   "source": [
    "for i, rev in enumerate(test_texts_):\n",
    "    print(rev)\n",
    "    test_input = test_data[i].copy()\n",
    "    test_input = np.reshape(test_input, (1,test_input.shape[0], test_input.shape[1]))\n",
    "    prediction = model.predict(test_input)\n",
    "    print('Prediction: ', prediction)\n",
    "    sentiment = np.argmax(prediction)\n",
    "    print('Sentiment: ' + str(sentiment))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "READ.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
