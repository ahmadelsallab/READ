{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import array\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import string\n",
    "os.environ['KERAS_BACKEND']='tensorflow'\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout, LSTM, GRU, Bidirectional, TimeDistributed\n",
    "from keras.models import Model, Sequential\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from pickle import dump\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpu_alloc(device_id):\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]=device_id\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_alloc(\"1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_path = '../dat/imdb/'\n",
    "dat_file_name = 'imdb_master.csv'\n",
    "dat_file = os.path.join(dat_path, dat_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(text):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for dataset\n",
    "    Every dataset is lower cased except\n",
    "    \"\"\"\n",
    "    text = re.sub(r\"\\\\\", \"\", text)    \n",
    "    text = re.sub(r\"\\'\", \"\", text)    \n",
    "    text = re.sub(r\"\\\"\", \"\", text)    \n",
    "    text = text.lower()\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    text = [w.translate(table) for w in text]    \n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(df):\n",
    "    df_train = df[df.train == 1]\n",
    "    df_test= df[df.test == 1]\n",
    "    return df_train.reviews, df_train.sentiment, df_test.reviews, df_test.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dat_file):\n",
    "    df = pd.read_csv(dat_file)\n",
    "    df_train = df[df.train == 1]\n",
    "    df_test= df[df.test == 1]\n",
    "    return df_train.reviews, df_train.sentiment.apply(clean_str), df_test.reviews.apply(clean_str), df_test.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, train_labels, test_texts, test_labels = load_data(dat_file)\n",
    "texts = train_texts + test_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 20000\n",
    "tokenizer = Tokenizer(num_words=VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "word_index = tokenizer.word_index\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the tokenizer\n",
    "tok_file_name = 'imdb_keras_tokenizer.pkl'\n",
    "dump(tokenizer, open(tok_file_name, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLM\n",
    "https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_lm_data(texts, seq_len):\n",
    "    \n",
    "    # organize into sequences of tokens\n",
    "    length = seq_len + 1\n",
    "    sequences = list()\n",
    "    for i in range(length, len(texts)):\n",
    "        # select sequence of tokens\n",
    "        seq = texts[i-length:i]\n",
    "        # convert into a line\n",
    "        line = ' '.join(seq)\n",
    "        # store\n",
    "        sequences.append(line) \n",
    "        \n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize_lm_data(texts, tokenizer):\n",
    "    return tokenizer.texts_to_sequences(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9741\n"
     ]
    }
   ],
   "source": [
    "LM_DATA_SIZE = 200000\n",
    "LM_SEQ_LEN = 50\n",
    "\n",
    "text_sequences = prepare_lm_data(texts, seq_len)\n",
    "sequences = binarize_lm_data(text_sequences, tokenizer)\n",
    "\n",
    "sz_limit = LM_DATA_SIZE# len(sequences)\n",
    "\n",
    "# separate into input and output\n",
    "sequences = array(sequences[:sz_limit])\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "\n",
    "#y = to_categorical(y, num_classes=vocab_size)\n",
    "seq_length = X.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_embeddings(embeddings_file):\n",
    "    embeddings_index = {}\n",
    "    f = open()\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "    f.close()\n",
    "\n",
    "    print('Total %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "    embedding_matrix = np.random.random((vocab_size, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector    \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 50, 50)            487050    \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 50, 100)           60400     \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 9741)              983841    \n",
      "=================================================================\n",
      "Total params: 1,621,791\n",
      "Trainable params: 1,621,791\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "#model = Sequential()\n",
    "#model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
    "#model.add(LSTM(100, return_sequences=True))\n",
    "#model.add(LSTM(100))\n",
    "\n",
    "GLOVE_DIR = \"./dat/glove\"\n",
    "\n",
    "embeddings_file = os.path.join(GLOVE_DIR, 'glove.6B.100d.txt')\n",
    "embedding_matrix = load_embeddings(embeddings_file)\n",
    "        \n",
    "  \n",
    "embedding_layer = Embedding(vocab_size,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SENT_LENGTH,\n",
    "                            trainable=True)\n",
    "sentence_input = Input(shape=(seq_length,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sentence_input)\n",
    "l_lstm = Bidirectional(GRU(100, return_sequences=True))(embedded_sequences)\n",
    "l_word_enc = TimeDistributed(Dense(200))(l_lstm)\n",
    "\n",
    "#model.add(Dense(100, activation='relu'))\n",
    "l_dense = Dense(100, activation='relu')(l_word_enc)\n",
    "#model.add(Dense(vocab_size, activation='softmax'))\n",
    "output = Dense(vocab_size, activation='softmax')(l_dense)\n",
    "model = Model(sentence_input, output)\n",
    "print(model.summary())\n",
    "word_enc = Model(sentence_input, l_word_enc)\n",
    "\n",
    "# compile model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "  383872/10976162 [>.............................] - ETA: 5:14:00 - loss: 6.3967 - acc: 0.0818"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1322624/10976162 [==>...........................] - ETA: 4:45:13 - loss: 6.0646 - acc: 0.1098"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10976162/10976162 [==============================] - 16708s 2ms/step - loss: 5.5343 - acc: 0.1476\n",
      "Epoch 2/100\n",
      " 7204736/10976162 [==================>...........] - ETA: 1:27:22 - loss: 5.2872 - acc: 0.1667"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 8404608/10976162 [=====================>........] - ETA: 57:07 - loss: 5.2842 - acc: 0.1670"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9843200/10976162 [=========================>....] - ETA: 24:02 - loss: 5.2801 - acc: 0.1676"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  400512/10976162 [>.............................] - ETA: 2:42:22 - loss: 5.2093 - acc: 0.1721"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1604736/10976162 [===>..........................] - ETA: 2:24:11 - loss: 5.2058 - acc: 0.1730"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2764032/10976162 [======>.......................] - ETA: 2:06:23 - loss: 5.2075 - acc: 0.1730"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3084416/10976162 [=======>......................] - ETA: 2:01:29 - loss: 5.2076 - acc: 0.1732"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4493568/10976162 [===========>..................] - ETA: 1:39:35 - loss: 5.2084 - acc: 0.1733"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 5482752/10976162 [=============>................] - ETA: 1:23:39 - loss: 5.2082 - acc: 0.1734"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 5748864/10976162 [==============>...............] - ETA: 1:19:36 - loss: 5.2083 - acc: 0.1734"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 7100800/10976162 [==================>...........] - ETA: 59:07 - loss: 5.2085 - acc: 0.1735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 8385024/10976162 [=====================>........] - ETA: 39:35 - loss: 5.2075 - acc: 0.1737"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9411072/10976162 [========================>.....] - ETA: 23:56 - loss: 5.2069 - acc: 0.1738"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9977984/10976162 [==========================>...] - ETA: 15:16 - loss: 5.2062 - acc: 0.1739"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10976162/10976162 [==============================] - 10082s 919us/step - loss: 5.2055 - acc: 0.1740\n",
      "Epoch 4/100\n",
      "   85376/10976162 [..............................] - ETA: 2:49:05 - loss: 5.1698 - acc: 0.1754"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  649216/10976162 [>.............................] - ETA: 2:38:58 - loss: 5.1623 - acc: 0.1770"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1346432/10976162 [==>...........................] - ETA: 2:28:13 - loss: 5.1619 - acc: 0.1769"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10976162/10976162 [==============================] - 10079s 918us/step - loss: 5.1224 - acc: 0.1809\n",
      "Epoch 7/100\n",
      "10976162/10976162 [==============================] - 10116s 922us/step - loss: 5.1073 - acc: 0.1821\n",
      "Epoch 8/100\n",
      " 2490496/10976162 [=====>........................] - ETA: 2:11:04 - loss: 5.0931 - acc: 0.1831"
     ]
    }
   ],
   "source": [
    "# fit model\n",
    "model.fit(X, y, batch_size=128, epochs=100)\n",
    "\n",
    "# save the model to file\n",
    "lm_model_file_name = 'imdb_keras_lm_model.h5'\n",
    "model.save(lm_model_file_name)\n",
    "# save the tokenizer\n",
    "tok_file_name = 'imdb_keras_tokenizer.pkl'\n",
    "dump(tokenizer, open(tok_file_name, 'wb'))\n",
    "word_enc_model_file_name = 'word_enc_model.h5'\n",
    "word_enc.save(word_enc_model_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use LM\n",
    "from random import randint\n",
    "from pickle import load\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "# generate a sequence from a language model\n",
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "    result = list()\n",
    "    in_text = seed_text\n",
    "    # generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "        # encode the text as integer\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # truncate sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "        # predict probabilities for each word\n",
    "        yhat = model.predict_classes(encoded, verbose=0)\n",
    "        # map predicted word index to word\n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                out_word = word\n",
    "                break\n",
    "        # append to input\n",
    "        in_text += ' ' + out_word\n",
    "        result.append(out_word)\n",
    "    return ' '.join(result)\n",
    "\n",
    "# load cleaned text sequences\n",
    "\n",
    "in_filename = out_filename\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')\n",
    "seq_length = len(lines[0].split()) - 1\n",
    "\n",
    "# load the model\n",
    "model = load_model(lm_model_file_name)\n",
    "\n",
    "# load the tokenizer\n",
    "tokenizer = load(open(tok_file_name, 'rb'))\n",
    "\n",
    "# select a seed text\n",
    "seed_text = lines[randint(0,len(lines))]\n",
    "print(seed_text + '\\n')\n",
    "\n",
    "# generate new text\n",
    "generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HATT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENT_LENGTH = 100\n",
    "MAX_SENTS = 15\n",
    "MAX_NB_WORDS = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import tokenize\n",
    "def prepare_hier_data(in_texts, labels):\n",
    "    \n",
    "    reviews = []\n",
    "    labels = []\n",
    "    texts = []\n",
    "\n",
    "    for idx in range(len(in_texts):\n",
    "        print('Parsing review ' + str(idx))\n",
    "        text = in_texts[idx]\n",
    "        texts.append(text)\n",
    "        sentences = tokenize.sent_tokenize(text)\n",
    "        reviews.append(sentences)       \n",
    "        labels.append(labels[idx])\n",
    "    return reviews, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize_hier_data(reviews, labels, tokenizer):\n",
    "    data_lst = []\n",
    "    labels_lst = []\n",
    "    for i, sentences in enumerate(reviews):\n",
    "        data = np.zeros((MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')\n",
    "        for j, sent in enumerate(sentences):\n",
    "            if j< MAX_SENTS:\n",
    "                wordTokens = text_to_word_sequence(sent)\n",
    "                k=0\n",
    "                for _, word in enumerate(wordTokens):\n",
    "                    if k<MAX_SENT_LENGTH and tokenizer.word_index[word]<MAX_NB_WORDS:\n",
    "                        data[j,k] = tokenizer.word_index[word]\n",
    "                        k=k+1\n",
    "        data_lst.append(data)\n",
    "        labels_lst.append(labels[i])\n",
    "    data = np.array(data_lst)\n",
    "    targets = np.array(labels_lst) \n",
    "    targets = to_categorical(np.asarray(targets))\n",
    "    return data, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts_, train_labels_ = prepare_hier_data(train_texts, train_labels)\n",
    "train_data, train_targets = binarize_hier_data(train_texts_, train_labels_, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape of data tensor:', train_data.shape)\n",
    "print('Shape of label tensor:', train_targets.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train/val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_SPLIT = 0.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "indices = np.arange(train_data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "train_data = train_data[indices]\n",
    "train_targets = train_targets[indices]\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * train_data.shape[0])\n",
    "\n",
    "x_train = train_data[:-nb_validation_samples]\n",
    "y_train = train_targets[:-nb_validation_samples]\n",
    "x_val = train_data[-nb_validation_samples:]\n",
    "y_val = train_targets[-nb_validation_samples:]\n",
    "\n",
    "print('Number of positive and negative reviews in traing and validation set')\n",
    "print(y_train.sum(axis=0))\n",
    "print(y_val.sum(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_file_name = os.path.join(GLOVE_DIR, 'glove.6B.100d.txt')\n",
    "\n",
    "# building Hierachical Attention network\n",
    "embedding_matrix = load_embeddings(embeddings_file_name)\n",
    "\n",
    "        \n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SENT_LENGTH,\n",
    "                            trainable=True)\n",
    "\n",
    "class AttLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.init = initializers.he_normal()\n",
    "        super(AttLayer, self).__init__(**kwargs)\n",
    "    '''\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape)==3\n",
    "        self.W = self.init((input_shape[-1],1))\n",
    "        self.trainable_weights = [self.W]\n",
    "        super(AttLayer, self).build(input_shape)  # be sure you call this somewhere!\n",
    "    '''\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape)==3\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.W = self.add_weight(name='kernel', \n",
    "                                      shape=(input_shape[-1], 1),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        super(AttLayer, self).build(input_shape)  # Be sure to call this at the end\n",
    "    \n",
    "    def call(self, x, mask=None):\n",
    "        eij = K.tanh(K.dot(x, self.W))\n",
    "        \n",
    "        ai = K.exp(eij)\n",
    "        weights = ai/tf.expand_dims(K.sum(ai, axis=1), 1)\n",
    "        \n",
    "        weighted_input = x*weights\n",
    "        return tf.reduce_sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "\n",
    "sentence_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sentence_input)\n",
    "l_lstm = Bidirectional(GRU(100, return_sequences=True))(embedded_sequences)\n",
    "l_dense = TimeDistributed(Dense(200))(l_lstm)\n",
    "l_att = AttLayer()(l_dense)\n",
    "sentEncoder = Model(sentence_input, l_att)\n",
    "\n",
    "review_input = Input(shape=(MAX_SENTS,MAX_SENT_LENGTH), dtype='int32')\n",
    "review_encoder = TimeDistributed(sentEncoder)(review_input)\n",
    "l_lstm_sent = Bidirectional(GRU(100, return_sequences=True))(review_encoder)\n",
    "l_dense_sent = TimeDistributed(Dense(200))(l_lstm_sent)\n",
    "l_att_sent = AttLayer()(l_dense_sent)\n",
    "preds = Dense(2, activation='softmax')(l_att_sent)\n",
    "model = Model(review_input, preds)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"model fitting - Hierachical attention network\")\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val), nb_epoch=NUM_EPOCHS, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts_, test_labels_ = prepare_hier_data(test_texts, test_labels)\n",
    "test_data, test_targets = binarize_hier_data(test_texts_, test_labels_, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, rev in enumerate(test_texts_):\n",
    "    print(rev)\n",
    "    test_input = test_data[i].copy()\n",
    "    test_input = np.reshape(test_input, (1,test_input.shape[0], test_input.shape[1]))\n",
    "    prediction = model.predict(test_input)\n",
    "    print('Prediction: ', prediction)\n",
    "    sentiment = np.argmax(prediction)\n",
    "    print('Sentiment: ' + str(sentiment))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
