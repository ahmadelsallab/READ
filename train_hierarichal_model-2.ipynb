{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Bidirectional, Concatenate, GRU, Dot, TimeDistributed, Activation, Embedding, Lambda, Concatenate, Reshape\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, LearningRateScheduler\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from keras.models import load_model\n",
    "from nltk.tokenize import word_tokenize\n",
    "from autocorrect import spell\n",
    "import re\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit gpu allocation. allow_growth, or gpu_fraction\n",
    "def gpu_alloc():\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_alloc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_WER_sent(gt, pred):\n",
    "    '''\n",
    "    calculate_WER('calculating wer between two sentences', 'calculate wer between two sentences')\n",
    "    '''\n",
    "    gt_words = gt.lower().split(' ')\n",
    "    pred_words = pred.lower().split(' ')\n",
    "    d = np.zeros(((len(gt_words) + 1), (len(pred_words) + 1)), dtype=np.uint8)\n",
    "    # d = d.reshape((len(gt_words)+1, len(pred_words)+1))\n",
    "\n",
    "    # Initializing error matrix\n",
    "    for i in range(len(gt_words) + 1):\n",
    "        for j in range(len(pred_words) + 1):\n",
    "            if i == 0:\n",
    "                d[0][j] = j\n",
    "            elif j == 0:\n",
    "                d[i][0] = i\n",
    "\n",
    "    # computation\n",
    "    for i in range(1, len(gt_words) + 1):\n",
    "        for j in range(1, len(pred_words) + 1):\n",
    "            if gt_words[i - 1] == pred_words[j - 1]:\n",
    "                d[i][j] = d[i - 1][j - 1]\n",
    "            else:\n",
    "                substitution = d[i - 1][j - 1] + 1\n",
    "                insertion = d[i][j - 1] + 1\n",
    "                deletion = d[i - 1][j] + 1\n",
    "                d[i][j] = min(substitution, insertion, deletion)\n",
    "    return d[len(gt_words)][len(pred_words)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_WER(gt, pred):\n",
    "    '''\n",
    "\n",
    "    :param gt: list of sentences of the ground truth\n",
    "    :param pred: list of sentences of the predictions\n",
    "    both lists must have the same length\n",
    "    :return: accumulated WER\n",
    "    '''\n",
    "#    assert len(gt) == len(pred)\n",
    "    WER = 0\n",
    "    nb_w = 0\n",
    "    for i in range(len(gt)):\n",
    "        #print(gt[i])\n",
    "        #print(pred[i])\n",
    "        WER += calculate_WER_sent(gt[i], pred[i])\n",
    "        nb_w += len(gt[i])\n",
    "\n",
    "    return WER / nb_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Artificial noisy spelling mistakes\n",
    "def noise_maker(sentence, threshold):\n",
    "    '''Relocate, remove, or add characters to create spelling mistakes'''\n",
    "    letters = ['a','b','c','d','e','f','g','h','i','j','k','l','m',\n",
    "           'n','o','p','q','r','s','t','u','v','w','x','y','z',]\n",
    "    noisy_sentence = []\n",
    "    i = 0\n",
    "    while i < len(sentence):\n",
    "        random = np.random.uniform(0, 1, 1)\n",
    "        # Most characters will be correct since the threshold value is high\n",
    "        if random < threshold:\n",
    "            noisy_sentence.append(sentence[i])\n",
    "        else:\n",
    "            new_random = np.random.uniform(0, 1, 1)\n",
    "            # ~33% chance characters will swap locations\n",
    "            if new_random > 0.67:\n",
    "                if i == (len(sentence) - 1):\n",
    "                    # If last character in sentence, it will not be typed\n",
    "                    continue\n",
    "                else:\n",
    "                    # if any other character, swap order with following character\n",
    "                    noisy_sentence.append(sentence[i + 1])\n",
    "                    noisy_sentence.append(sentence[i])\n",
    "                    i += 1\n",
    "            # ~33% chance an extra lower case letter will be added to the sentence\n",
    "            elif new_random < 0.33:\n",
    "                random_letter = np.random.choice(letters, 1)[0]\n",
    "                noisy_sentence.append(random_letter)\n",
    "                noisy_sentence.append(sentence[i])\n",
    "            # ~33% chance a character will not be typed\n",
    "            else:\n",
    "                pass\n",
    "        i += 1\n",
    "\n",
    "    return ''.join(noisy_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_with_gt(file_name, num_samples, max_sent_len, min_sent_len, delimiter='\\t', gt_index=1, prediction_index=0):\n",
    "    '''Load data from txt file, with each line has: <TXT><TAB><GT>. The  target to the decoder muxt have \\t as the start trigger and \\n as the stop trigger.'''\n",
    "    cnt = 0  \n",
    "    input_texts = []\n",
    "    gt_texts = []\n",
    "    target_texts = []\n",
    "    for row in open(file_name, encoding='utf8'):\n",
    "        if cnt < num_samples :\n",
    "            #print(row)\n",
    "            sents = row.split(delimiter)\n",
    "            if (len(sents) < 2):\n",
    "                continue             \n",
    "            input_text = sents[prediction_index]\n",
    "            \n",
    "            target_text = '\\t' + sents[gt_index].strip() + '\\n'\n",
    "            if len(input_text) > min_sent_len and len(input_text) < max_sent_len and len(target_text) > min_sent_len and len(target_text) < max_sent_len:\n",
    "                cnt += 1\n",
    "                \n",
    "                input_texts.append(input_text)\n",
    "                target_texts.append(target_text)\n",
    "                gt_texts.append(sents[gt_index])\n",
    "    return input_texts, target_texts, gt_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_with_noise(file_name, num_samples, noise_threshold, max_sent_len, min_sent_len):\n",
    "    '''Load data from txt file, with each line has: <TXT>. The GT is just a noisy version of TXT. The  target to the decoder muxt have \\t as the start trigger and \\n as the stop trigger.'''\n",
    "    cnt = 0  \n",
    "    input_texts = []\n",
    "    gt_texts = []\n",
    "    target_texts = []\n",
    "    while cnt < num_samples :\n",
    "        for row in open(file_name, encoding='utf8'):\n",
    "        #for row in open(file_name):\n",
    "            if cnt < num_samples :\n",
    "                sents = row.split(\"\\t\")\n",
    "                if (len(sents) < 2):\n",
    "                    continue                 \n",
    "                input_text = noise_maker(sents[1], noise_threshold)\n",
    "                input_text = input_text[:-1]\n",
    "\n",
    "                target_text = '\\t' + sents[1].strip() + '\\n'            \n",
    "                if len(input_text) > min_sent_len and len(input_text) < max_sent_len and len(target_text) > min_sent_len and len(target_text) < max_sent_len:\n",
    "                    cnt += 1\n",
    "                    input_texts.append(input_text)\n",
    "                    target_texts.append(target_text)\n",
    "                    gt_texts.append(target_text[1:-1])\n",
    "                    \n",
    "    return input_texts, target_texts, gt_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_words_vocab(all_texts):\n",
    "    '''Build vocab dictionary to victorize chars into ints'''\n",
    "    vocab_to_int = {}\n",
    "    count = 0 # Start index for any char will be 1, as 0 is masked by the Embedding/Masking layer\n",
    "    codes = ['UNK', ' ', '\\t','\\n']# Start 'UNK' at the first entry, to keep its index=0 to be masked\n",
    "    for code in codes:\n",
    "        if code not in vocab_to_int:\n",
    "            vocab_to_int[code] = count\n",
    "            count += 1    \n",
    "    \n",
    "    for sentence in all_texts:       \n",
    "        for word in word_tokenize(sentence):\n",
    "            word = process_word(word)\n",
    "            if word not in vocab_to_int:\n",
    "                vocab_to_int[word] = count\n",
    "                count += 1\n",
    "\n",
    "\n",
    "    '''''Build inverse translation from int to word'''\n",
    "    int_to_vocab = {}\n",
    "    for word, value in vocab_to_int.items():\n",
    "        int_to_vocab[value] = word\n",
    "        \n",
    "    return vocab_to_int, int_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_chars_vocab(all_texts):\n",
    "    '''Build vocab dictionary to victorize chars into ints'''\n",
    "    vocab_to_int = {}\n",
    "    count = 0 # Start index for any char will be 1, as 0 is masked by the Embedding/Masking layer\n",
    "    codes = ['UNK', ' ', '\\t','\\n']# Start 'UNK' at the first entry, to keep its index=0 to be masked\n",
    "    for code in codes:\n",
    "        if code not in vocab_to_int:\n",
    "            vocab_to_int[code] = count\n",
    "            count += 1    \n",
    "    \n",
    "    for sentence in all_texts:\n",
    "        for char in sentence:\n",
    "            if char not in vocab_to_int:\n",
    "                vocab_to_int[char] = count\n",
    "                count += 1\n",
    "\n",
    "\n",
    "    '''''Build inverse translation from int to char'''\n",
    "    int_to_vocab = {}\n",
    "    for character, value in vocab_to_int.items():\n",
    "        int_to_vocab[value] = character\n",
    "        \n",
    "    return vocab_to_int, int_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_char_data(input_texts, target_texts, max_encoder_seq_length, num_encoder_tokens, vocab_to_int):\n",
    "    '''Prepares the input text and targets into the proper seq2seq numpy arrays'''\n",
    "    encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length),\n",
    "    dtype='float32')\n",
    "    decoder_input_data = np.zeros(\n",
    "        (len(input_texts), max_decoder_seq_length),\n",
    "        dtype='float32')\n",
    "    decoder_target_data = np.zeros(\n",
    "        (len(input_texts), max_decoder_seq_length, num_encoder_tokens),\n",
    "        dtype='float32')\n",
    "\n",
    "    for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "        for t, char in enumerate(input_text):\n",
    "            # c0..cn\n",
    "            encoder_input_data[i, t] = vocab_to_int[char]\n",
    "        for t, char in enumerate(target_text):\n",
    "            # c0'..cm'\n",
    "            # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "            decoder_input_data[i, t] = vocab_to_int[char]\n",
    "            if t > 0:\n",
    "                # decoder_target_data will be ahead by one timestep\n",
    "                # and will not include the start character.\n",
    "                decoder_target_data[i, t - 1, vocab_to_int[char]] = 1.\n",
    "                \n",
    "    return encoder_input_data, decoder_input_data, decoder_target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_hier_data(input_texts, target_texts, max_words_seq_length, max_chars_seq_length, num_char_tokens, num_word_tokens, word2int, char2int):\n",
    "\n",
    "    '''Prepares the input text and targets into the proper seq2seq numpy arrays'''\n",
    "    # \n",
    "    encoder_char_input_data = np.zeros(\n",
    "    (len(input_texts), max_words_seq_length, max_chars_seq_length),\n",
    "    dtype='float32')\n",
    "    \n",
    "    decoder_word_input_data = np.zeros(\n",
    "        (len(input_texts), max_words_seq_length),\n",
    "        dtype='float32')\n",
    "    \n",
    "    decoder_word_target_data = np.zeros(\n",
    "        (len(input_texts), max_words_seq_length, num_word_tokens),\n",
    "        dtype='float32')\n",
    "\n",
    "    for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "        words_lst = word_tokenize(input_text)\n",
    "        if(len(words_lst) > max_words_seq_length):\n",
    "            continue\n",
    "        for j, word in enumerate(words_lst):\n",
    "            if(len(word) > max_chars_seq_length):\n",
    "                continue\n",
    "            for k, char in enumerate(word):\n",
    "                # c0..cn\n",
    "                if(char in char2int):\n",
    "                    encoder_char_input_data[i, j, k] = char2int[char]\n",
    "                    \n",
    "        words_lst = word_tokenize(target_text)# word_tokenize removes the \\t and \\n, we need them to start and end a sequence\n",
    "        words_lst.insert(0, '\\t')\n",
    "        words_lst.append('\\n')        \n",
    "        if(len(words_lst) > max_words_seq_length):\n",
    "            continue                \n",
    "        for j, word in enumerate(words_lst):\n",
    "            processed_word = process_word(word)\n",
    "            if not processed_word in word2int:\n",
    "                continue\n",
    "            # c0'..cm'\n",
    "            # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "            decoder_word_input_data[i, j] = word2int[processed_word]\n",
    "            if j > 0:\n",
    "                # decoder_target_data will be ahead by one timestep\n",
    "                # and will not include the start character.\n",
    "                decoder_word_target_data[i, j - 1, word2int[processed_word]] = 1.\n",
    "                \n",
    "    return encoder_char_input_data, decoder_word_input_data, decoder_word_target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_gt_sequence(input_seq, int_to_vocab):\n",
    "\n",
    "    decoded_sentence = ''\n",
    "    for i in range(input_seq.shape[1]):\n",
    "        \n",
    "        # Sample a token\n",
    "        sampled_token_index = input_seq[0][i]\n",
    "        sampled_word = int_to_vocab[sampled_token_index]\n",
    "        decoded_sentence += sampled_word + ' '\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq, encoder_model, decoder_model, max_words_seq_len, num_decoder_tokens, int_to_vocab):\n",
    "    # Encode the input as state vectors.\n",
    "    encoder_outputs, h, c  = encoder_model.predict(input_seq)\n",
    "    states_value = [h,c]\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, max_words_seq_len))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0] = vocab_to_int['\\t']\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    #print(input_seq)\n",
    "    attention_density = []\n",
    "\n",
    "    special_chars = ['\\\\', '/', '-', '—' , ':', '[', ']', ',', '.', '\"', ';', '%', '~', '(', ')', '{', '}', '$']\n",
    "    i = 0\n",
    "    while not stop_condition:\n",
    "        #print(target_seq)\n",
    "        output_tokens, attention, h, c  = decoder_model.predict(\n",
    "            [target_seq, encoder_outputs] + states_value)\n",
    "        #print(attention.shape)\n",
    "        attention_density.append(attention[0][0])# attention is max_sent_len x 1 since we have num_time_steps = 1 for the output\n",
    "        # Sample a token\n",
    "        #print(output_tokens.shape)\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        \n",
    "        #print(sampled_token_index)\n",
    "        sampled_char = int_to_vocab[sampled_token_index]\n",
    "       \n",
    "        \n",
    "        #orig_char = int_to_vocab[int(input_seq[:,i][0])]\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(word_tokenize(decoded_sentence)) > max_words_seq_len):\n",
    "            stop_condition = True\n",
    "            sampled_char = ''\n",
    "\n",
    "        # Copy digits as it, since the spelling corrector is not good at digit corrections\n",
    "        '''\n",
    "        if(orig_char.isdigit() or orig_char in special_chars):\n",
    "            decoded_sentence += orig_char            \n",
    "        else:\n",
    "            if(sampled_char.isdigit() or sampled_char in special_chars):\n",
    "                decoded_sentence += ''\n",
    "            else:\n",
    "                decoded_sentence += sampled_char\n",
    "        '''\n",
    "        decoded_sentence += sampled_char + ' '\n",
    "\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        #target_seq = np.zeros((1, max_words_seq_len))\n",
    "        if i < max_words_seq_len:\n",
    "            target_seq[0, i] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "        i += 1\n",
    "        #if i > 48:\n",
    "        #    i = 0\n",
    "        \n",
    "\n",
    "    attention_density = np.array(attention_density)\n",
    "    return decoded_sentence, attention_density\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_char_sequence(input_seq, encoder_model, decoder_model, num_decoder_tokens, int_to_vocab):\n",
    "    # Encode the input as state vectors.\n",
    "    encoder_outputs, h, c  = encoder_model.predict(input_seq)\n",
    "    states_value = [h,c]\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0] = vocab_to_int['\\t']\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    #print(input_seq)\n",
    "    attention_density = []\n",
    "    i = 0\n",
    "    special_chars = ['\\\\', '/', '-', '—' , ':', '[', ']', ',', '.', '\"', ';', '%', '~', '(', ')', '{', '}', '$']\n",
    "    while not stop_condition:\n",
    "        #print(target_seq)\n",
    "        output_tokens, attention, h, c  = decoder_model.predict(\n",
    "            [target_seq, encoder_outputs] + states_value)\n",
    "        #print(attention.shape)\n",
    "        attention_density.append(attention[0][0])# attention is max_sent_len x 1 since we have num_time_steps = 1 for the output\n",
    "        # Sample a token\n",
    "        #print(output_tokens.shape)\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        \n",
    "        #print(sampled_token_index)\n",
    "        sampled_char = int_to_vocab[sampled_token_index]\n",
    "        orig_char = int_to_vocab[int(input_seq[:,i][0])]\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "            sampled_char = ''\n",
    "\n",
    "        # Copy digits as it, since the spelling corrector is not good at digit corrections\n",
    "        if(orig_char.isdigit() or orig_char in special_chars):\n",
    "            decoded_sentence += orig_char            \n",
    "        else:\n",
    "            if(sampled_char.isdigit() or sampled_char in special_chars):\n",
    "                decoded_sentence += ''\n",
    "            else:\n",
    "                decoded_sentence += sampled_char\n",
    "        \n",
    "\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "        \n",
    "        i += 1\n",
    "        if(i > 48):\n",
    "            i = 0\n",
    "    attention_density = np.array(attention_density)\n",
    "    return decoded_sentence, attention_density\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_char_model(num_encoder_tokens, latent_dim):\n",
    "    # Define an input sequence and process it.\n",
    "    encoder_inputs = Input(shape=(None,), dtype='float32')\n",
    "    encoder_inputs_ = Embedding(num_encoder_tokens, num_encoder_tokens,                           \n",
    "                            weights=[np.eye(num_encoder_tokens)],\n",
    "                            mask_zero=True, trainable=False)(encoder_inputs)    \n",
    "    #encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "    encoder = Bidirectional(LSTM(latent_dim, return_state=True, return_sequences=True)) # Bi LSTM\n",
    "    encoder_outputs, state_f_h, state_f_c, state_b_h, state_b_c = encoder(encoder_inputs_)# Bi LSTM\n",
    "    state_h = Concatenate()([state_f_h, state_b_h])# Bi LSTM\n",
    "    state_c = Concatenate()([state_f_c, state_b_c])# Bi LSTM\n",
    "\n",
    "    # We discard `encoder_outputs` and only keep the states.\n",
    "    encoder_states = [state_h, state_c]# Bi GRU, LSTM, BHi LSTM\n",
    "    #print(encoder_states)\n",
    "    \n",
    "    decoder_inputs = Input(shape=(None,))\n",
    "    decoder_inputs_ = Embedding(num_encoder_tokens, num_encoder_tokens,                           \n",
    "                            weights=[np.eye(num_encoder_tokens)],\n",
    "                            mask_zero=True, trainable=False)(decoder_inputs)    \n",
    "    # We set up our decoder to return full output sequences,\n",
    "    # and to return internal states as well. We don't use the\n",
    "    # return states in the training model, but we will use them in inference.\n",
    "    decoder_lstm = LSTM(latent_dim*2, return_sequences=True, return_state=True)# Bi LSTM\n",
    "    \n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs_, initial_state=encoder_states)\n",
    "\n",
    "    # Equation (7) with 'dot' score from Section 3.1 in the paper.\n",
    "    # Note that we reuse Softmax-activation layer instead of writing tensor calculation\n",
    "    #print(decoder_outputs)\n",
    "    #print(encoder_outputs)\n",
    "    att_dot = Dot(axes=[2, 2])\n",
    "    attention = att_dot([decoder_outputs, encoder_outputs])\n",
    "    att_activation = Activation('softmax', name='attention')\n",
    "    attention = att_activation(attention)\n",
    "    #print('attention', attention)\n",
    "    context_dot = Dot(axes=[2,1])\n",
    "    context = context_dot([attention, encoder_outputs])\n",
    "    #print('context', context)\n",
    "    att_context_concat = Concatenate()\n",
    "    decoder_combined_context = att_context_concat([context, decoder_outputs])\n",
    "    #print('decoder_combined_context', decoder_combined_context)\n",
    "\n",
    "    # Has another weight + tanh layer as described in equation (5) of the paper\n",
    "    #decoder_outputs = TimeDistributed(Dense(64, activation=\"tanh\"))(decoder_combined_context)\n",
    "    #decoder_outputs = TimeDistributed(Dense(num_encoder_tokens, activation=\"softmax\"))(decoder_outputs)\n",
    "\n",
    "    decoder_dense = Dense(num_encoder_tokens, activation='softmax')\n",
    "    #decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_outputs = decoder_dense(decoder_combined_context)\n",
    "\n",
    "    # Define the model that will turn\n",
    "    # `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "    #model = Model(decoder_inputs, decoder_outputs)\n",
    "    #print('encoder-decoder  model:')\n",
    "    print(model.summary()) \n",
    "    \n",
    "    #print(encoder_inputs)\n",
    "    #print(encoder_outputs)\n",
    "    #print(encoder_states)\n",
    "    #encoder_model = Model(encoder_inputs, [encoder_outputs, encoder_states])\n",
    "    encoder_model = Model(input=encoder_inputs, output=[encoder_outputs] + encoder_states)\n",
    "\n",
    "    #decoder_state_input_h = Input(shape=(latent_dim,))# LSTM\n",
    "    #decoder_state_input_c = Input(shape=(latent_dim,))# LSTM\n",
    "    decoder_encoder_inputs = Input(shape=(None, latent_dim*2,))\n",
    "    decoder_state_input_h = Input(shape=(latent_dim*2,))# Bi LSTM\n",
    "    decoder_state_input_c = Input(shape=(latent_dim*2,)) # Bi LSTM\n",
    "    #decoder_state_input = Input(shape=(latent_dim*2,)) # Bi GRU\n",
    "\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    #decoder_states_inputs = [decoder_state_input] # Bi GRU\n",
    "\n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs_, initial_state=decoder_states_inputs)\n",
    "\n",
    "    #decoder_outputs, state = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "\n",
    "    decoder_states = [state_h, state_c]\n",
    "    \n",
    "    # Equation (7) with 'dot' score from Section 3.1 in the paper.\n",
    "    # Note that we reuse Softmax-activation layer instead of writing tensor calculation\n",
    "    \n",
    "    attention = att_dot([decoder_outputs, decoder_encoder_inputs])\n",
    "    \n",
    "    attention = att_activation(attention)\n",
    "    #print('attention', attention)\n",
    "    \n",
    "    context = context_dot([attention, decoder_encoder_inputs])\n",
    "    #print('context', context)\n",
    "    \n",
    "    \n",
    "    \n",
    "    decoder_combined_context = att_context_concat([context, decoder_outputs])\n",
    "    #print('decoder_combined_context', decoder_combined_context)\n",
    "\n",
    "    # Has another weight + tanh layer as described in equation (5) of the paper\n",
    "    #decoder_outputs = TimeDistributed(Dense(64, activation=\"tanh\"))(decoder_combined_context)\n",
    "    #decoder_outputs = TimeDistributed(Dense(num_encoder_tokens, activation=\"softmax\"))(decoder_outputs)\n",
    "    \n",
    "    #decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "    #decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_outputs = decoder_dense(decoder_combined_context)\n",
    "    \n",
    "    decoder_model = Model(\n",
    "        [decoder_inputs, decoder_encoder_inputs] + decoder_states_inputs,\n",
    "        [decoder_outputs, attention] + decoder_states)\n",
    "    \n",
    "    return model, encoder_model, decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_hier_model(encoder_char_model, max_words_seq_len, max_char_seq_len, num_word_tokens, num_char_tokens, latent_dim):\n",
    "    # Define an input sequence and process it.\n",
    "\n",
    "    inputs = Input(shape=(max_words_seq_len, max_char_seq_len,), dtype='float32')\n",
    "    decoder_inputs_words = Input(shape=(max_words_seq_len,), dtype='float32')\n",
    "    words_states = []\n",
    "    \n",
    "    for w in range(max_words_seq_len):\n",
    "        \n",
    "        encoder_char_inputs = Lambda(lambda x: x[:,w,:])(inputs)\n",
    "        _, h, c = encoder_char_model(encoder_char_inputs)\n",
    "        encoder_chars_states = Concatenate()([h,c])\n",
    "        #print(encoder_chars_states)\n",
    "        encoder_chars_states = Reshape((1,latent_dim*4))(encoder_chars_states)\n",
    "        words_states.append(encoder_chars_states)\n",
    "    \n",
    "    input_words = Concatenate(axis=-2)(words_states)\n",
    "\n",
    "\n",
    "    \n",
    "    encoder_inputs_ = input_words   \n",
    "    #encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "    encoder = Bidirectional(LSTM(latent_dim, return_state=True, return_sequences=True)) # Bi LSTM\n",
    "    encoder_outputs, state_f_h, state_f_c, state_b_h, state_b_c = encoder(encoder_inputs_)# Bi LSTM\n",
    "    state_h = Concatenate()([state_f_h, state_b_h])# Bi LSTM\n",
    "    state_c = Concatenate()([state_f_c, state_b_c])# Bi LSTM\n",
    "\n",
    "    # We discard `encoder_outputs` and only keep the states.\n",
    "    encoder_states = [state_h, state_c]# Bi GRU, LSTM, BHi LSTM\n",
    "    \n",
    "    decoder_inputs = decoder_inputs_words\n",
    "    decoder_inputs_ = Embedding(num_word_tokens, latent_dim*4,                           \n",
    "                            #weights=[np.eye(num_word_tokens)],\n",
    "                            mask_zero=True, trainable=True)(decoder_inputs)    \n",
    "    # We set up our decoder to return full output sequences,\n",
    "    # and to return internal states as well. We don't use the\n",
    "    # return states in the training model, but we will use them in inference.\n",
    "    decoder_lstm = LSTM(latent_dim*2, return_sequences=True, return_state=True)# Bi LSTM\n",
    "    \n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs_, initial_state=encoder_states)\n",
    "\n",
    "    att_dot = Dot(axes=[2, 2])\n",
    "    attention = att_dot([decoder_outputs, encoder_outputs])\n",
    "    att_activation = Activation('softmax')\n",
    "    attention = att_activation(attention)\n",
    "    #print('attention', attention)\n",
    "    context_dot = Dot(axes=[2,1])\n",
    "    context = context_dot([attention, encoder_outputs])\n",
    "    #print('context', context)\n",
    "    att_context_concat = Concatenate()\n",
    "    decoder_combined_context = att_context_concat([context, decoder_outputs])\n",
    "    #print('decoder_combined_context', decoder_combined_context)\n",
    "\n",
    "    # Has another weight + tanh layer as described in equation (5) of the paper\n",
    "    #decoder_outputs = TimeDistributed(Dense(64, activation=\"tanh\"))(decoder_combined_context)\n",
    "    #decoder_outputs = TimeDistributed(Dense(num_encoder_tokens, activation=\"softmax\"))(decoder_outputs)\n",
    "\n",
    "    decoder_dense = Dense(num_word_tokens, activation='softmax')\n",
    "    #decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_outputs = decoder_dense(decoder_combined_context)\n",
    "\n",
    "    # Define the model that will turn\n",
    "    # `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "    model = Model([inputs, decoder_inputs_words], decoder_outputs)\n",
    "    #model = Model(decoder_inputs, decoder_outputs)\n",
    "    print('encoder-decoder  model:')\n",
    "    print(model.summary()) \n",
    "    \n",
    "    #encoder_model = Model(encoder_inputs, [encoder_outputs, encoder_states])\n",
    "    encoder_model = Model(input=inputs, output=[encoder_outputs] + encoder_states)\n",
    "\n",
    "    #decoder_state_input_h = Input(shape=(latent_dim,))# LSTM\n",
    "    #decoder_state_input_c = Input(shape=(latent_dim,))# LSTM\n",
    "    decoder_encoder_inputs = Input(shape=(max_words_seq_len, latent_dim*2,))\n",
    "    decoder_state_input_h = Input(shape=(latent_dim*2,))# Bi LSTM\n",
    "    decoder_state_input_c = Input(shape=(latent_dim*2,)) # Bi LSTM\n",
    "    #decoder_state_input = Input(shape=(latent_dim*2,)) # Bi GRU\n",
    "\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    #decoder_states_inputs = [decoder_state_input] # Bi GRU\n",
    "\n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs_, initial_state=decoder_states_inputs)\n",
    "\n",
    "    #decoder_outputs, state = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "\n",
    "    decoder_states = [state_h, state_c]\n",
    "    \n",
    "    # Equation (7) with 'dot' score from Section 3.1 in the paper.\n",
    "    # Note that we reuse Softmax-activation layer instead of writing tensor calculation\n",
    "    \n",
    "    attention = att_dot([decoder_outputs, decoder_encoder_inputs])\n",
    "    \n",
    "    attention = att_activation(attention)\n",
    "    #print('attention', attention)\n",
    "    \n",
    "    context = context_dot([attention, decoder_encoder_inputs])\n",
    "    #print('context', context)\n",
    "    \n",
    "    \n",
    "    \n",
    "    decoder_combined_context = att_context_concat([context, decoder_outputs])\n",
    "    #print('decoder_combined_context', decoder_combined_context)\n",
    "\n",
    "    # Has another weight + tanh layer as described in equation (5) of the paper\n",
    "    #decoder_outputs = TimeDistributed(Dense(64, activation=\"tanh\"))(decoder_combined_context)\n",
    "    #decoder_outputs = TimeDistributed(Dense(num_encoder_tokens, activation=\"softmax\"))(decoder_outputs)\n",
    "    \n",
    "    #decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "    #decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_outputs = decoder_dense(decoder_combined_context)\n",
    "    \n",
    "    decoder_model = Model(\n",
    "        [decoder_inputs_words, decoder_encoder_inputs] + decoder_states_inputs,\n",
    "        [decoder_outputs, attention] + decoder_states)\n",
    "    \n",
    "    return model, encoder_model, decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n_, encoder_char_model, _ = build_char_model(num_encoder_tokens=28, latent_dim=256)\\nhier_model = build_hier_model(encoder_char_model=encoder_char_model, max_words_seq_len=40, max_char_seq_len=20, num_word_tokens=1000, num_char_tokens=28, latent_dim=256)\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "_, encoder_char_model, _ = build_char_model(num_encoder_tokens=28, latent_dim=256)\n",
    "hier_model = build_hier_model(encoder_char_model=encoder_char_model, max_words_seq_len=40, max_char_seq_len=20, num_word_tokens=1000, num_char_tokens=28, latent_dim=256)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nx = Input(shape=(50,))\\nx = Reshape((1,50))(x)\\nprint(x)\\nl = []\\nl.append(x)\\nl.append(x)\\nprint(l)\\ny = Concatenate(axis=-2)(l)\\nprint(y)\\nz = Reshape((-1,2,50))(y)\\nprint(z)\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "x = Input(shape=(50,))\n",
    "x = Reshape((1,50))(x)\n",
    "print(x)\n",
    "l = []\n",
    "l.append(x)\n",
    "l.append(x)\n",
    "print(l)\n",
    "y = Concatenate(axis=-2)(l)\n",
    "print(y)\n",
    "z = Reshape((-1,2,50))(y)\n",
    "print(z)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def visualize_attention(text, encoder_model, decoder_model, max_encoder_seq_length, num_decoder_tokens, vocab_to_int, int_to_vocab):\n",
    "\n",
    "    encoder_input_data = np.zeros((1, max_encoder_seq_length), dtype='float32')\n",
    "    \n",
    "    for t, char in enumerate(text):\n",
    "        # c0..cn\n",
    "        encoder_input_data[0, t] = vocab_to_int[char]\n",
    "\n",
    "    input_seq = encoder_input_data[0:1]\n",
    "\n",
    "    decoded_sentence, attention_density = decode_sequence(input_seq, encoder_model, decoder_model, num_decoder_tokens, int_to_vocab)\n",
    "\n",
    "    plt.clf()\n",
    "    plt.figure(figsize=(28,12))\n",
    "    \n",
    "    ax = sns.heatmap(attention_density[:, : len(text) + 2],\n",
    "        xticklabels=[w for w in text],\n",
    "        yticklabels=[w for w in decoded_sentence])\n",
    "\n",
    "    ax.invert_yaxis()\n",
    "    plt.show()\n",
    "    \n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_word(word):\n",
    "    # Try to correct the word from known dict\n",
    "    #word = spell(word)\n",
    "    # Option 1: Replace special chars and digits\n",
    "    #processed_word = re.sub(r'[\\\\\\/\\-\\—\\:\\[\\]\\,\\.\\\"\\;\\%\\~\\(\\)\\{\\}\\$\\#\\?\\●\\@\\+\\-\\*\\d]', r'', w.lower())\n",
    "    \n",
    "    # Option 2: skip all words with special chars or digits\n",
    "    if(len(re.findall(r'[\\\\\\/\\-\\—\\:\\[\\]\\,\\.\\\"\\;\\%\\~\\(\\)\\{\\}\\$\\#\\?\\●\\@\\+\\-\\*\\d]', word.lower())) == 0):\n",
    "        #processed_word = word.lower()\n",
    "        processed_word = word\n",
    "    else:\n",
    "        processed_word = 'UNK'\n",
    "\n",
    "    # Skip stop words\n",
    "    #stop_words = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]        \n",
    "    stop_words = []\n",
    "    if processed_word in stop_words:\n",
    "        processed_word = 'UNK'\n",
    "        \n",
    "    return processed_word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-train char model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../dat/'\n",
    "max_sent_len = 50\n",
    "min_sent_len = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1000\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "files_list = ['all_ocr_data_2.txt', 'field_class_21.txt', 'field_class_32.txt', 'field_class_30.txt']\n",
    "desired_file_sizes = [num_samples, num_samples, num_samples, num_samples]\n",
    "noise_threshold = 0.9\n",
    "\n",
    "for file_name, num_file_samples in zip(files_list, desired_file_sizes):\n",
    "    tess_correction_data = os.path.join(data_path, file_name)\n",
    "    input_texts_OCR, target_texts_OCR, gt_OCR = load_data_with_noise(tess_correction_data, num_file_samples, noise_threshold, max_sent_len, min_sent_len)\n",
    "\n",
    "    input_texts += input_texts_OCR\n",
    "    target_texts += target_texts_OCR\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram of lenghts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chars_per_words_lengths = []\n",
    "words_per_sents_lengths = []\n",
    "\n",
    "# Chars per word should be on all text\n",
    "for text in (input_texts_OCR+target_texts_OCR):\n",
    "    words = word_tokenize(text)\n",
    "    #words_per_sents_lengths.append(len(words))\n",
    "    for word in words:\n",
    "        chars_per_words_lengths.append(len(word))\n",
    "\n",
    "# Words in sent should be on target only        \n",
    "for text in target_texts_OCR:\n",
    "    words = word_tokenize(text)\n",
    "    words_per_sents_lengths.append(len(words))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAD0tJREFUeJzt3H+s3XV9x/HnaxTx5waMC6lt3WWuc6CZxdyQbiSLEx0/ZiwmYynZsHEs9Q90uJhsxf2hS8biMpXNbGOpgtSNgQQxNMKcXWUxJhO8IKuUyuiU0Ws7eh2KbGa64nt/3G/jXbm999zzg9P7yfOR3Jzv+ZzvOef9De3zHr4956SqkCS168fGPYAkabQMvSQ1ztBLUuMMvSQ1ztBLUuMMvSQ1ztBLUuMMvSQ1ztBLUuNWjXsAgDPOOKMmJyfHPYYkrSgPPPDAt6pqYqn9TojQT05OMj09Pe4xJGlFSfLvveznqRtJapyhl6TGGXpJapyhl6TGGXpJapyhl6TGGXpJapyhl6TGLRn6JC9Mcn+Sf0myN8kfdutnJ7kvyWNJPpnkBd36Kd31/d3tk6M9BEnSYnp5Rf994A1V9VpgA3Bxko3AnwDXV9V64NvAVd3+VwHfrqqfAa7v9lMPJrfdPe4RJDVoydDXnP/qrp7c/RTwBuCObn0HcFm3vam7Tnf7hUkytIklScvS0zn6JCcleQg4DOwC/g34TlUd6XaZAdZ022uAAwDd7U8DP7nAY25NMp1kenZ2drCjkCQdV0+hr6pnq2oDsBY4Hzhnod26y4VevddzFqq2V9VUVU1NTCz55WuSpD4t6103VfUd4J+AjcCpSY5+++Va4GC3PQOsA+hu/wngqWEMK0lavl7edTOR5NRu+0XAG4F9wL3Ar3W7bQHu6rZ3dtfpbv98VT3nFb0k6fnRy/fRrwZ2JDmJuV8Mt1fVZ5I8AtyW5I+ArwA3dvvfCPxNkv3MvZLfPIK5JUk9WjL0VbUHOG+B9a8zd77+2PX/AS4fynSSpIH5yVhJapyhl6TGGXpJapyhl6TGGXpJapyhl6TGGXpJapyhl6TGGXpJapyhl6TGGXpJapyhl6TGGXpJapyhl6TGGXpJapyhl6TGGXpJapyhl6TGGXpJapyhl6TGGXpJapyhl6TGGXpJapyhl6TGGXpJatySoU+yLsm9SfYl2Zvkmm79/Um+meSh7ufSefe5Nsn+JI8muWiUByBJWtyqHvY5Arynqh5M8jLggSS7utuur6oPzt85ybnAZuDVwMuBf0zys1X17DAHlyT1ZslX9FV1qKoe7LafAfYBaxa5yybgtqr6flV9A9gPnD+MYSVJy7esc/RJJoHzgPu6pXcm2ZPkpiSndWtrgAPz7jbDAr8YkmxNMp1kenZ2dtmDS5J603Pok7wU+BTw7qr6LnAD8EpgA3AI+NDRXRe4ez1noWp7VU1V1dTExMSyB5ck9aan0Cc5mbnI31JVdwJU1ZNV9WxV/RD4KD86PTMDrJt397XAweGNLElajl7edRPgRmBfVX143vrqebu9FXi4294JbE5ySpKzgfXA/cMbWZK0HL286+YC4Ergq0ke6tbeC1yRZANzp2UeB94BUFV7k9wOPMLcO3au9h03kjQ+S4a+qr7Iwufd71nkPtcB1w0wlyRpSPxkrCQ1ztBLUuMMvSQ1ztBLUuMMvSQ1ztBLUuMMvSQ1ztBLUuMMvSQ1ztBLUuMMvSQ1ztBLUuMMvSQ1ztBLUuMMvSQ1ztBLUuMMvSQ1ztBLUuMMvSQ1ztBLUuMMvSQ1ztBLUuMMfQ8mt9097hEkqW+GXpIaZ+glqXFLhj7JuiT3JtmXZG+Sa7r105PsSvJYd3lat54kH0myP8meJK8b9UFIko6vl1f0R4D3VNU5wEbg6iTnAtuA3VW1HtjdXQe4BFjf/WwFbhj61JKkni0Z+qo6VFUPdtvPAPuANcAmYEe32w7gsm57E/CJmvMl4NQkq4c+uSSpJ8s6R59kEjgPuA84q6oOwdwvA+DMbrc1wIF5d5vp1iRJY9Bz6JO8FPgU8O6q+u5iuy6wVgs83tYk00mmZ2dnex1DkrRMPYU+ycnMRf6WqrqzW37y6CmZ7vJwtz4DrJt397XAwWMfs6q2V9VUVU1NTEz0O78kaQm9vOsmwI3Avqr68LybdgJbuu0twF3z1t/WvftmI/D00VM8kqTn36oe9rkAuBL4apKHurX3Ah8Abk9yFfAEcHl32z3ApcB+4HvA24c6sSRpWZYMfVV9kYXPuwNcuMD+BVw94FySpCHxk7GS1DhDL0mNM/SS1DhDL0mNM/SS1DhDL0mNM/SS1DhDL0mNM/SS1DhDL0mNM/SS1DhDL0mNM/SS1DhDL0mNM/SS1DhDL0mNM/SS1DhDL0mNM/SS1DhDL0mNM/SS1DhDL0mNM/SS1DhDL0mNM/SS1LglQ5/kpiSHkzw8b+39Sb6Z5KHu59J5t12bZH+SR5NcNKrBJUm96eUV/c3AxQusX19VG7qfewCSnAtsBl7d3eevkpw0rGElScu3ZOir6gvAUz0+3ibgtqr6flV9A9gPnD/AfJKkAQ1yjv6dSfZ0p3ZO69bWAAfm7TPTrUmSxqTf0N8AvBLYABwCPtStZ4F9a6EHSLI1yXSS6dnZ2T7HkCQtpa/QV9WTVfVsVf0Q+Cg/Oj0zA6ybt+ta4OBxHmN7VU1V1dTExEQ/Y0iSetBX6JOsnnf1rcDRd+TsBDYnOSXJ2cB64P7BRpQkDWLVUjskuRV4PXBGkhngfcDrk2xg7rTM48A7AKpqb5LbgUeAI8DVVfXsaEaXJPViydBX1RULLN+4yP7XAdcNMpQkaXj8ZKwkNc7QS1LjDL0kNc7QS1LjDL0kNc7QS1LjDL0kNc7QS1LjDL0kNc7QS1LjDL0kNc7QS1LjDL0kNc7QS1LjDL0kNW7Fh35y293jHkGSTmgrPvSSpMUZeklqnKGXpMYZeklqnKGXpMYZeklqnKGXpMYZeklqnKGXpMYtGfokNyU5nOTheWunJ9mV5LHu8rRuPUk+kmR/kj1JXjfK4SVJS+vlFf3NwMXHrG0DdlfVemB3dx3gEmB997MVuGE4Y0qS+rVk6KvqC8BTxyxvAnZ02zuAy+atf6LmfAk4NcnqYQ0rSVq+fs/Rn1VVhwC6yzO79TXAgXn7zXRr0tD5hXZSb4b9j7FZYK0W3DHZmmQ6yfTs7OyQx5AkHdVv6J88ekqmuzzcrc8A6+bttxY4uNADVNX2qpqqqqmJiYk+x5AkLaXf0O8EtnTbW4C75q2/rXv3zUbg6aOneCRJ47FqqR2S3Aq8HjgjyQzwPuADwO1JrgKeAC7vdr8HuBTYD3wPePsIZpYkLcOSoa+qK45z04UL7FvA1YMOJUkaHj8ZK0mNM/SS1DhDL0mNM/SS1DhDL0mNM/QaOr+aQDqxGHpJapyhl6TGGXpJapyhl6TGGXpJapyhl6TGGXpJapyhl6TGGXpJapyhl6TGGXpJapyhl6TGGXpJapyhl6TGGXpJapyhl6TGGXpJapyhl6TGGXpJatyqQe6c5HHgGeBZ4EhVTSU5HfgkMAk8Dvx6VX17sDElSf0axiv6X66qDVU11V3fBuyuqvXA7u66JGlMRnHqZhOwo9veAVw2gueQJPVo0NAX8LkkDyTZ2q2dVVWHALrLMwd8DknSAAY6Rw9cUFUHk5wJ7ErytV7v2P1i2Arwile8YsAxJEnHM9Ar+qo62F0eBj4NnA88mWQ1QHd5+Dj33V5VU1U1NTExMcgYkqRF9B36JC9J8rKj28CvAA8DO4Et3W5bgLsGHVKS1L9BTt2cBXw6ydHH+buq+mySLwO3J7kKeAK4fPAxJUn96jv0VfV14LULrP8ncOEgQ0mShsdPxkpS4wy9JDXO0EtS4wy9JDXO0EtS4wy9JDXO0EtS4wy9JDXO0EtS4wy9JDXO0EtS4wy9JDXO0EtS4wy9JDXO0EtS4wy9JDXO0EtS4wy9JDXO0EtS4wy9JDXO0EtS4wy9JDXO0EtS4wy9JDXO0EtS40YW+iQXJ3k0yf4k20b1PJK0Uk1uu/t5eZ6RhD7JScBfApcA5wJXJDl3FM8lSVrcqF7Rnw/sr6qvV9UPgNuATSN6LknSIkYV+jXAgXnXZ7o1SdLzLFU1/AdNLgcuqqrf7q5fCZxfVe+at89WYGt39VXAo0MfZLjOAL417iGGpJVjaeU4wGM5Ea2E4/ipqppYaqdVI3ryGWDdvOtrgYPzd6iq7cD2ET3/0CWZrqqpcc8xDK0cSyvHAR7LiaiV44DRnbr5MrA+ydlJXgBsBnaO6LkkSYsYySv6qjqS5J3APwAnATdV1d5RPJckaXGjOnVDVd0D3DOqxx+DFXOaqQetHEsrxwEey4moleMYzT/GSpJOHH4FgiQ1ztAvIsm6JPcm2Zdkb5Jrxj3ToJKclOQrST4z7lkGkeTUJHck+Vr33+cXxj1TP5L8bvdn6+EktyZ54bhnWo4kNyU5nOTheWunJ9mV5LHu8rRxztiL4xzHn3Z/vvYk+XSSU8c54yAM/eKOAO+pqnOAjcDVDXyVwzXAvnEPMQR/Dny2qn4OeC0r8JiSrAF+B5iqqtcw98aFzeOdatluBi4+Zm0bsLuq1gO7u+snupt57nHsAl5TVT8P/Ctw7fM91LAY+kVU1aGqerDbfoa5mKzYT/gmWQv8KvCxcc8yiCQ/DvwScCNAVf2gqr4z3qn6tgp4UZJVwIs55vMmJ7qq+gLw1DHLm4Ad3fYO4LLndag+LHQcVfW5qjrSXf0Sc58HWpEMfY+STALnAfeNd5KB/Bnwe8APxz3IgH4amAU+3p2G+liSl4x7qOWqqm8CHwSeAA4BT1fV58Y71VCcVVWHYO7FEnDmmOcZht8C/n7cQ/TL0PcgyUuBTwHvrqrvjnuefiR5M3C4qh4Y9yxDsAp4HXBDVZ0H/Dcr4/TA/9Odu94EnA28HHhJkt8c71Q6VpI/YO407i3jnqVfhn4JSU5mLvK3VNWd455nABcAb0nyOHPfJvqGJH873pH6NgPMVNXR/7u6g7nwrzRvBL5RVbNV9b/AncAvjnmmYXgyyWqA7vLwmOfpW5ItwJuB36gV/F50Q7+IJGHuPPC+qvrwuOcZRFVdW1Vrq2qSuX/w+3xVrchXj1X1H8CBJK/qli4EHhnjSP16AtiY5MXdn7ULWYH/qLyAncCWbnsLcNcYZ+lbkouB3wfeUlXfG/c8gzD0i7sAuJK5V78PdT+XjnsoAfAu4JYke4ANwB+PeZ5l6/6P5A7gQeCrzP19XFGfxkxyK/DPwKuSzCS5CvgA8KYkjwFv6q6f0I5zHH8BvAzY1f3d/+uxDjkAPxkrSY3zFb0kNc7QS1LjDL0kNc7QS1LjDL0kNc7QS1LjDL0kNc7QS1Lj/g8kadRRW4HkCgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2546dd6400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "h_w = plt.hist(words_per_sents_lengths, bins=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2546d1a9b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "h_c = plt.hist(chars_per_words_lengths, bins=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build char vocab (all text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts = target_texts + input_texts\n",
    "vocab_to_int, int_to_vocab = build_chars_vocab(all_texts)\n",
    "np.savez('vocab_char-{}'.format(max_sent_len), vocab_to_int=vocab_to_int, int_to_vocab=int_to_vocab, max_sent_len=max_sent_len, min_sent_len=min_sent_len )\n",
    "char2int = vocab_to_int\n",
    "int2char = int_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_characters = sorted(list(vocab_to_int))\n",
    "target_characters = sorted(list(vocab_to_int))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 4000\n",
      "Number of unique input tokens: 91\n",
      "Number of unique output tokens: 91\n",
      "Max sequence length for inputs: 49\n",
      "Max sequence length for outputs: 49\n"
     ]
    }
   ],
   "source": [
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\t': 2,\n",
       " '\\n': 3,\n",
       " ' ': 1,\n",
       " '#': 67,\n",
       " '$': 80,\n",
       " '%': 85,\n",
       " '&': 73,\n",
       " \"'\": 83,\n",
       " '(': 64,\n",
       " ')': 65,\n",
       " '*': 77,\n",
       " '+': 76,\n",
       " ',': 69,\n",
       " '-': 21,\n",
       " '.': 48,\n",
       " '/': 29,\n",
       " '0': 54,\n",
       " '1': 43,\n",
       " '2': 53,\n",
       " '3': 57,\n",
       " '4': 56,\n",
       " '5': 74,\n",
       " '6': 55,\n",
       " '7': 70,\n",
       " '8': 61,\n",
       " '9': 72,\n",
       " ':': 13,\n",
       " ';': 75,\n",
       " '=': 89,\n",
       " '?': 60,\n",
       " '@': 81,\n",
       " 'A': 16,\n",
       " 'B': 15,\n",
       " 'C': 4,\n",
       " 'D': 40,\n",
       " 'E': 46,\n",
       " 'F': 33,\n",
       " 'G': 41,\n",
       " 'H': 52,\n",
       " 'I': 22,\n",
       " 'J': 68,\n",
       " 'K': 50,\n",
       " 'L': 37,\n",
       " 'M': 36,\n",
       " 'N': 35,\n",
       " 'O': 30,\n",
       " 'P': 26,\n",
       " 'Q': 78,\n",
       " 'R': 45,\n",
       " 'S': 38,\n",
       " 'T': 9,\n",
       " 'U': 49,\n",
       " 'UNK': 0,\n",
       " 'V': 14,\n",
       " 'W': 51,\n",
       " 'X': 79,\n",
       " 'Y': 47,\n",
       " 'Z': 71,\n",
       " '^': 86,\n",
       " '_': 90,\n",
       " 'a': 6,\n",
       " 'b': 39,\n",
       " 'c': 17,\n",
       " 'd': 18,\n",
       " 'e': 12,\n",
       " 'f': 32,\n",
       " 'g': 42,\n",
       " 'h': 28,\n",
       " 'i': 7,\n",
       " 'j': 23,\n",
       " 'k': 59,\n",
       " 'l': 5,\n",
       " 'm': 8,\n",
       " 'n': 19,\n",
       " 'o': 27,\n",
       " 'p': 11,\n",
       " 'q': 58,\n",
       " 'r': 25,\n",
       " 's': 34,\n",
       " 't': 20,\n",
       " 'u': 24,\n",
       " 'v': 44,\n",
       " 'w': 31,\n",
       " 'x': 62,\n",
       " 'y': 10,\n",
       " 'z': 63,\n",
       " '|': 82,\n",
       " '’': 66,\n",
       " '•': 84,\n",
       " '●': 87,\n",
       " 'ﬁ': 88}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char2int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'UNK',\n",
       " 1: ' ',\n",
       " 2: '\\t',\n",
       " 3: '\\n',\n",
       " 4: 'C',\n",
       " 5: 'l',\n",
       " 6: 'a',\n",
       " 7: 'i',\n",
       " 8: 'm',\n",
       " 9: 'T',\n",
       " 10: 'y',\n",
       " 11: 'p',\n",
       " 12: 'e',\n",
       " 13: ':',\n",
       " 14: 'V',\n",
       " 15: 'B',\n",
       " 16: 'A',\n",
       " 17: 'c',\n",
       " 18: 'd',\n",
       " 19: 'n',\n",
       " 20: 't',\n",
       " 21: '-',\n",
       " 22: 'I',\n",
       " 23: 'j',\n",
       " 24: 'u',\n",
       " 25: 'r',\n",
       " 26: 'P',\n",
       " 27: 'o',\n",
       " 28: 'h',\n",
       " 29: '/',\n",
       " 30: 'O',\n",
       " 31: 'w',\n",
       " 32: 'f',\n",
       " 33: 'F',\n",
       " 34: 's',\n",
       " 35: 'N',\n",
       " 36: 'M',\n",
       " 37: 'L',\n",
       " 38: 'S',\n",
       " 39: 'b',\n",
       " 40: 'D',\n",
       " 41: 'G',\n",
       " 42: 'g',\n",
       " 43: '1',\n",
       " 44: 'v',\n",
       " 45: 'R',\n",
       " 46: 'E',\n",
       " 47: 'Y',\n",
       " 48: '.',\n",
       " 49: 'U',\n",
       " 50: 'K',\n",
       " 51: 'W',\n",
       " 52: 'H',\n",
       " 53: '2',\n",
       " 54: '0',\n",
       " 55: '6',\n",
       " 56: '4',\n",
       " 57: '3',\n",
       " 58: 'q',\n",
       " 59: 'k',\n",
       " 60: '?',\n",
       " 61: '8',\n",
       " 62: 'x',\n",
       " 63: 'z',\n",
       " 64: '(',\n",
       " 65: ')',\n",
       " 66: '’',\n",
       " 67: '#',\n",
       " 68: 'J',\n",
       " 69: ',',\n",
       " 70: '7',\n",
       " 71: 'Z',\n",
       " 72: '9',\n",
       " 73: '&',\n",
       " 74: '5',\n",
       " 75: ';',\n",
       " 76: '+',\n",
       " 77: '*',\n",
       " 78: 'Q',\n",
       " 79: 'X',\n",
       " 80: '$',\n",
       " 81: '@',\n",
       " 82: '|',\n",
       " 83: \"'\",\n",
       " 84: '•',\n",
       " 85: '%',\n",
       " 86: '^',\n",
       " 87: '●',\n",
       " 88: 'ﬁ',\n",
       " 89: '=',\n",
       " 90: '_'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int2char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(int_to_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize char data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data, decoder_input_data, decoder_target_data = vectorize_char_data(input_texts=input_texts,\n",
    "                                                                             target_texts=target_texts, \n",
    "                                                                             max_encoder_seq_length=max_encoder_seq_length, \n",
    "                                                                             num_encoder_tokens=num_encoder_tokens, \n",
    "                                                                             vocab_to_int=vocab_to_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build char model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 256 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 91)     8281        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) [(None, None, 512),  712704      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, None, 91)     8281        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 512)          0           bidirectional_1[0][1]            \n",
      "                                                                 bidirectional_1[0][3]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 512)          0           bidirectional_1[0][2]            \n",
      "                                                                 bidirectional_1[0][4]            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, None, 512),  1236992     embedding_2[0][0]                \n",
      "                                                                 concatenate_1[0][0]              \n",
      "                                                                 concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, None, None)   0           lstm_2[0][0]                     \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention (Activation)          (None, None, None)   0           dot_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dot_2 (Dot)                     (None, None, 512)    0           attention[0][0]                  \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, None, 1024)   0           dot_2[0][0]                      \n",
      "                                                                 lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 91)     93275       concatenate_3[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 2,059,533\n",
      "Trainable params: 2,042,971\n",
      "Non-trainable params: 16,562\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:63: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=[<tf.Tenso...)`\n"
     ]
    }
   ],
   "source": [
    "model, encoder_model, decoder_model = build_char_model(latent_dim=latent_dim, num_encoder_tokens=num_encoder_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit char model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3200 samples, validate on 800 samples\n",
      "Epoch 1/10\n",
      "3200/3200 [==============================] - 28s 9ms/step - loss: 3.4300 - categorical_accuracy: 0.1324 - val_loss: 2.8417 - val_categorical_accuracy: 0.1955\n",
      "\n",
      "Epoch 00001: val_categorical_accuracy improved from -inf to 0.19554, saving model to best_model_char-50.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/keras/engine/network.py:872: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_1/concat:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'concatenate_2/concat:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n",
      "3200/3200 [==============================] - 24s 8ms/step - loss: 2.1259 - categorical_accuracy: 0.3769 - val_loss: 1.9052 - val_categorical_accuracy: 0.4395\n",
      "\n",
      "Epoch 00002: val_categorical_accuracy improved from 0.19554 to 0.43955, saving model to best_model_char-50.hdf5\n",
      "Epoch 3/10\n",
      "3200/3200 [==============================] - 25s 8ms/step - loss: 0.9891 - categorical_accuracy: 0.6963 - val_loss: 0.9788 - val_categorical_accuracy: 0.6921\n",
      "\n",
      "Epoch 00003: val_categorical_accuracy improved from 0.43955 to 0.69205, saving model to best_model_char-50.hdf5\n",
      "Epoch 4/10\n",
      "3200/3200 [==============================] - 21s 7ms/step - loss: 0.3984 - categorical_accuracy: 0.8532 - val_loss: 0.5804 - val_categorical_accuracy: 0.7922\n",
      "\n",
      "Epoch 00004: val_categorical_accuracy improved from 0.69205 to 0.79217, saving model to best_model_char-50.hdf5\n",
      "Epoch 5/10\n",
      "3200/3200 [==============================] - 21s 7ms/step - loss: 0.1973 - categorical_accuracy: 0.9054 - val_loss: 0.3952 - val_categorical_accuracy: 0.8423\n",
      "\n",
      "Epoch 00005: val_categorical_accuracy improved from 0.79217 to 0.84228, saving model to best_model_char-50.hdf5\n",
      "Epoch 6/10\n",
      "3200/3200 [==============================] - 26s 8ms/step - loss: 0.1238 - categorical_accuracy: 0.9255 - val_loss: 0.3637 - val_categorical_accuracy: 0.8497\n",
      "\n",
      "Epoch 00006: val_categorical_accuracy improved from 0.84228 to 0.84970, saving model to best_model_char-50.hdf5\n",
      "Epoch 7/10\n",
      "3200/3200 [==============================] - 25s 8ms/step - loss: 0.0795 - categorical_accuracy: 0.9377 - val_loss: 0.2838 - val_categorical_accuracy: 0.8732\n",
      "\n",
      "Epoch 00007: val_categorical_accuracy improved from 0.84970 to 0.87323, saving model to best_model_char-50.hdf5\n",
      "Epoch 8/10\n",
      "3200/3200 [==============================] - 26s 8ms/step - loss: 0.0423 - categorical_accuracy: 0.9481 - val_loss: 0.2730 - val_categorical_accuracy: 0.8789\n",
      "\n",
      "Epoch 00008: val_categorical_accuracy improved from 0.87323 to 0.87895, saving model to best_model_char-50.hdf5\n",
      "Epoch 9/10\n",
      "3200/3200 [==============================] - 26s 8ms/step - loss: 0.0300 - categorical_accuracy: 0.9507 - val_loss: 0.2360 - val_categorical_accuracy: 0.8891\n",
      "\n",
      "Epoch 00009: val_categorical_accuracy improved from 0.87895 to 0.88905, saving model to best_model_char-50.hdf5\n",
      "Epoch 10/10\n",
      "3200/3200 [==============================] - 27s 8ms/step - loss: 0.0202 - categorical_accuracy: 0.9537 - val_loss: 0.2205 - val_categorical_accuracy: 0.8964\n",
      "\n",
      "Epoch 00010: val_categorical_accuracy improved from 0.88905 to 0.89639, saving model to best_model_char-50.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2546790ef0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 10 \n",
    "lr = 0.01\n",
    "\n",
    "model.compile(optimizer=optimizers.Adam(lr=lr), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "#filepath=\"weights-improvement-{epoch:02d}-{val_categorical_accuracy:.2f}.hdf5\"\n",
    "filepath=\"best_model_char-{}.hdf5\".format(max_sent_len) # Save only the best model for inference step, as saving the epoch and metric might confuse the inference function which model to use\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_categorical_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "tbCallBack = TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "callbacks_list = [checkpoint, tbCallBack]\n",
    "#callbacks_list = [checkpoint, tbCallBack, lrate]\n",
    "\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          #validation_data = ([test_encoder_input_data, test_decoder_input_data], test_decoder_target_data),\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=callbacks_list,\n",
    "          validation_split=0.2,\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/keras/engine/network.py:872: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_4:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'input_5:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "encoder_char_model_file = 'encoder_char_model-{}.hdf5'\n",
    "decoder_char_model_file = 'decoder_char_model-{}.hdf5'\n",
    "encoder_model.save('encoder_char_model-{}.hdf5'.format(max_sent_len))\n",
    "decoder_model.save('decoder_char_model-{}.hdf5'.format(max_sent_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Hierarichal model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build word vocab (target_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words_seq_len=15\n",
    "max_chars_seq_len=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts = target_texts\n",
    "vocab_to_int, int_to_vocab = build_words_vocab(all_texts)\n",
    "word2int = vocab_to_int\n",
    "int2word = int_to_vocab\n",
    "np.savez('vocab_hier-{}-{}'.format(max_words_seq_len, max_chars_seq_len), char2int=char2int, int2char=int2char, word2int=word2int, int2word=int2word, max_words_seq_len=max_words_seq_len, max_char_seq_len=max_chars_seq_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'UNK': 0,\n",
       " ' ': 1,\n",
       " '\\t': 2,\n",
       " '\\n': 3,\n",
       " 'Claim': 4,\n",
       " 'Type': 5,\n",
       " 'VB': 6,\n",
       " 'Accident': 7,\n",
       " 'Accidental': 8,\n",
       " 'Injury': 9,\n",
       " 'Information': 10,\n",
       " 'First': 11,\n",
       " 'Name': 12,\n",
       " 'Middle': 13,\n",
       " 'Last': 14,\n",
       " 'Social': 15,\n",
       " 'Security': 16,\n",
       " 'Number': 17,\n",
       " 'Birth': 18,\n",
       " 'Date': 19,\n",
       " 'Gender': 20,\n",
       " 'Language': 21,\n",
       " 'Preference': 22,\n",
       " 'Address': 23,\n",
       " 'Line': 24,\n",
       " 'City': 25,\n",
       " 'Postal': 26,\n",
       " 'Code': 27,\n",
       " 'Country': 28,\n",
       " 'Best': 29,\n",
       " 'Phone': 30,\n",
       " 'to': 31,\n",
       " 'be': 32,\n",
       " 'Reached': 33,\n",
       " 'During': 34,\n",
       " 'the': 35,\n",
       " 'Day': 36,\n",
       " 'Email': 37,\n",
       " 'Page': 38,\n",
       " 'of': 39,\n",
       " 'RADIOLOGY': 40,\n",
       " 'REPORT': 41,\n",
       " 'Patient': 42,\n",
       " 'MRN': 43,\n",
       " 'Accession': 44,\n",
       " 'No': 45,\n",
       " 'Ref': 46,\n",
       " 'Physician': 47,\n",
       " 'UNKNOWN': 48,\n",
       " 'Study': 49,\n",
       " 'Hospital': 50,\n",
       " 'DOB': 51,\n",
       " 'Technique': 52,\n",
       " 'views': 53,\n",
       " 'left': 54,\n",
       " 'wrist': 55,\n",
       " 'Cormarison': 56,\n",
       " 'None': 57,\n",
       " 'availabie': 58,\n",
       " 'Comparison': 59,\n",
       " 'available': 60,\n",
       " 'FINDINGS': 61,\n",
       " 'IMPRESSION': 62,\n",
       " 'acute': 63,\n",
       " 'osseous': 64,\n",
       " 'abnormality': 65,\n",
       " 'identified': 66,\n",
       " 'Daytime': 67,\n",
       " 'Event': 68,\n",
       " 'Stopped': 69,\n",
       " 'Working': 70,\n",
       " 'Yes': 71,\n",
       " 'Physically': 72,\n",
       " 'at': 73,\n",
       " 'Work': 74,\n",
       " 'Hours': 75,\n",
       " 'Worked': 76,\n",
       " 'on': 77,\n",
       " 'Scheduled': 78,\n",
       " 'Missed': 79,\n",
       " 'Returned': 80,\n",
       " 'Related': 81,\n",
       " 'Time': 82,\n",
       " 'Diagnosis': 83,\n",
       " 'Arthiscopic': 84,\n",
       " 'surgery': 85,\n",
       " 'Surgery': 86,\n",
       " 'Is': 87,\n",
       " 'Required': 88,\n",
       " 'Indicator': 89,\n",
       " 'Outpatient': 90,\n",
       " 'Medical': 91,\n",
       " 'Provider': 92,\n",
       " 'Roles': 93,\n",
       " 'Treating': 94,\n",
       " 'Patrick': 95,\n",
       " 'Emerson': 96,\n",
       " 'Business': 97,\n",
       " 'Telephone': 98,\n",
       " 'Fax': 99,\n",
       " 'Visit': 100,\n",
       " 'Next': 101,\n",
       " 'Hospitalization': 102,\n",
       " 'Discharge': 103,\n",
       " 'Procedure': 104,\n",
       " 'Left': 105,\n",
       " 'arthiscopic': 106,\n",
       " 'Employment': 107,\n",
       " 'Employer': 108,\n",
       " 'Policy': 109,\n",
       " 'Electronic': 110,\n",
       " 'Submission': 111,\n",
       " 'Identifier': 112,\n",
       " 'Electronically': 113,\n",
       " 'Signed': 114,\n",
       " 'Fraud': 115,\n",
       " 'Statements': 116,\n",
       " 'Reviewed': 117,\n",
       " 'and': 118,\n",
       " 'unum': 119,\n",
       " 'The': 120,\n",
       " 'Benefits': 121,\n",
       " 'Center': 122,\n",
       " 'Not': 123,\n",
       " 'for': 124,\n",
       " 'FMLA': 125,\n",
       " 'Requests': 126,\n",
       " 'Insured': 127,\n",
       " '’': 128,\n",
       " 's': 129,\n",
       " 'Signature': 130,\n",
       " 'Printed': 131,\n",
       " 'Unum': 132,\n",
       " 'Confirmation': 133,\n",
       " 'Coverage': 134,\n",
       " 'Group': 135,\n",
       " 'Customer': 136,\n",
       " 'EE': 137,\n",
       " 'Effective': 138,\n",
       " 'Employee': 139,\n",
       " 'Acc': 140,\n",
       " 'January': 141,\n",
       " 'Wellness': 142,\n",
       " 'Benefit': 143,\n",
       " 'Total': 144,\n",
       " 'Monthly': 145,\n",
       " 'Premium': 146,\n",
       " 'Montly': 147,\n",
       " 'Payroll': 148,\n",
       " 'Deduction': 149,\n",
       " 'Account': 150,\n",
       " 'F': 151,\n",
       " 'Exam': 152,\n",
       " 'Referring': 153,\n",
       " 'Phys': 154,\n",
       " 'STEPHEN': 155,\n",
       " 'GELOVICH': 156,\n",
       " 'Tax': 157,\n",
       " 'MRI': 158,\n",
       " 'OF': 159,\n",
       " 'THE': 160,\n",
       " 'LEFT': 161,\n",
       " 'WRIST': 162,\n",
       " 'WITHOUT': 163,\n",
       " 'CONTRAST': 164,\n",
       " 'COMPARISON': 165,\n",
       " 'These': 166,\n",
       " 'results': 167,\n",
       " 'were': 168,\n",
       " 'faxed': 169,\n",
       " 'Gelovich': 170,\n",
       " 'signed': 171,\n",
       " 'by': 172,\n",
       " 'Stephen': 173,\n",
       " 'Bravo': 174,\n",
       " 'Dependent': 175,\n",
       " 'Detail': 176,\n",
       " 'Zachary': 177,\n",
       " 'Jager': 178,\n",
       " 'Billed': 179,\n",
       " 'Amounts': 180,\n",
       " 'Contract': 181,\n",
       " 'Adjustment': 182,\n",
       " 'Allowed': 183,\n",
       " 'Amount': 184,\n",
       " 'Covered': 185,\n",
       " 'Reason': 186,\n",
       " 'Deductible': 187,\n",
       " 'Other': 188,\n",
       " 'Carrier': 189,\n",
       " 'Paid': 190,\n",
       " 'Responsibility': 191,\n",
       " 'Totals': 192,\n",
       " 'For': 193,\n",
       " 'Appeals': 194,\n",
       " 'Rights': 195,\n",
       " 'Important': 196,\n",
       " 'about': 197,\n",
       " 'Your': 198,\n",
       " 'Appeal': 199,\n",
       " 'All': 200,\n",
       " 'Languages': 201,\n",
       " 'Contact': 202,\n",
       " 'Did': 203,\n",
       " 'You': 204,\n",
       " 'Know': 205,\n",
       " 'Specialty': 206,\n",
       " 'Orthopedic': 207,\n",
       " 'Surgeon': 208,\n",
       " 'Unknown': 209,\n",
       " 'Kari': 210,\n",
       " 'Lund': 211,\n",
       " 'Orthopedist': 212,\n",
       " 'Dan': 213,\n",
       " 'Palmer': 214,\n",
       " 'On': 215,\n",
       " '&': 216,\n",
       " 'May': 217,\n",
       " 'Spouse': 218,\n",
       " 'Child': 219,\n",
       " 'BLACK': 220,\n",
       " 'HILLS': 221,\n",
       " 'ORTHOPEDIC': 222,\n",
       " 'CENTER': 223,\n",
       " 'PC': 224,\n",
       " 'LAST': 225,\n",
       " 'PMT': 226,\n",
       " 'AMOUNT': 227,\n",
       " 'DUE': 228,\n",
       " 'DATE': 229,\n",
       " 'PAGE': 230,\n",
       " 'STATEMENT': 231,\n",
       " 'Ins': 232,\n",
       " 'Description': 233,\n",
       " 'E': 234,\n",
       " 'm': 235,\n",
       " 'New': 236,\n",
       " 'Moderat': 237,\n",
       " 'S': 238,\n",
       " 'Clo': 239,\n",
       " 'Tx': 240,\n",
       " 'Phalangealfx': 241,\n",
       " 'Finger': 242,\n",
       " 'Splint': 243,\n",
       " 'Offic': 244,\n",
       " 'Cons': 245,\n",
       " 'Moderate': 246,\n",
       " 'Sever': 247,\n",
       " 'Rad': 248,\n",
       " 'Mini': 249,\n",
       " 'Views': 250,\n",
       " 'Applic': 251,\n",
       " 'Hand': 252,\n",
       " 'Lower': 253,\n",
       " 'Forearm': 254,\n",
       " 'Fiberglass': 255,\n",
       " 'gauntlet': 256,\n",
       " 'Cast': 257,\n",
       " 'Yrs': 258,\n",
       " 'Charge': 259,\n",
       " 'Pmt': 260,\n",
       " 'Pat': 261,\n",
       " 'Adjust': 262,\n",
       " 'Current': 263,\n",
       " 'Days': 264,\n",
       " 'Balance': 265,\n",
       " 'Pending': 266,\n",
       " 'Now': 267,\n",
       " 'Due': 268,\n",
       " 'Message': 269,\n",
       " 'Make': 270,\n",
       " 'Checks': 271,\n",
       " 'Payable': 272,\n",
       " 'To': 273,\n",
       " 'Statement': 274,\n",
       " 'Billing': 275,\n",
       " 'Questions': 276,\n",
       " 'Choice': 277,\n",
       " 'Health': 278,\n",
       " 'Administrators': 279,\n",
       " 'Forwarding': 280,\n",
       " 'Service': 281,\n",
       " 'Requested': 282,\n",
       " 'REGIONAL': 283,\n",
       " 'HEALTH': 284,\n",
       " 'INC': 285,\n",
       " 'Participant': 286,\n",
       " 'ID': 287,\n",
       " 'Original': 288,\n",
       " 'Print': 289,\n",
       " 'Website': 290,\n",
       " 'DEA': 291,\n",
       " 'Individual': 292,\n",
       " 'Summary': 293,\n",
       " 'By': 294,\n",
       " 'Plan': 295,\n",
       " 'Status': 296,\n",
       " 'Period': 297,\n",
       " 'DEDUCTIBLE': 298,\n",
       " 'OUT': 299,\n",
       " 'POCKET': 300,\n",
       " 'Regional': 301,\n",
       " 'Inc': 302,\n",
       " 'EXPLANATION': 303,\n",
       " 'BENEFITS': 304,\n",
       " 'Retain': 305,\n",
       " 'Purposes': 306,\n",
       " 'Family': 307,\n",
       " 'Out': 308,\n",
       " 'Network': 309,\n",
       " 'Karl': 310,\n",
       " 'Services': 311,\n",
       " 'exam': 312,\n",
       " 'hand': 313,\n",
       " 'Modifiers': 314,\n",
       " 'TC': 315,\n",
       " 'RT': 316,\n",
       " 'This': 317,\n",
       " 'Qualified': 318,\n",
       " 'sign': 319,\n",
       " 'language': 320,\n",
       " 'interpreters': 321,\n",
       " 'written': 322,\n",
       " 'in': 323,\n",
       " 'other': 324,\n",
       " 'languages': 325,\n",
       " 'Jacquelin': 326,\n",
       " 'Brainard': 327,\n",
       " 'Compliance': 328,\n",
       " 'Officer': 329,\n",
       " 'or': 330,\n",
       " 'mail': 331,\n",
       " 'phone': 332,\n",
       " 'Department': 333,\n",
       " 'Human': 334,\n",
       " 'Complaint': 335,\n",
       " 'forms': 336,\n",
       " 'are': 337,\n",
       " 'Sign': 338,\n",
       " 'up': 339,\n",
       " 'paperless': 340,\n",
       " 'DAKOTA': 341,\n",
       " 'EZ': 342,\n",
       " 'Ways': 343,\n",
       " 'Pay': 344,\n",
       " 'Automated': 345,\n",
       " 'Attendant': 346,\n",
       " 'hours': 347,\n",
       " 'a': 348,\n",
       " 'day': 349,\n",
       " 'Payments': 350,\n",
       " 'Please': 351,\n",
       " 'Call': 352,\n",
       " 'Upon': 353,\n",
       " 'Receipt': 354,\n",
       " 'Improved': 355,\n",
       " 'Online': 356,\n",
       " 'Experience': 357,\n",
       " '|': 358,\n",
       " 'Update': 359,\n",
       " 'Info': 360,\n",
       " 'About': 361,\n",
       " 'See': 362,\n",
       " 'Details': 363,\n",
       " 'Back': 364,\n",
       " 'ACCOUNT': 365,\n",
       " 'NO': 366,\n",
       " 'SHOW': 367,\n",
       " 'PAID': 368,\n",
       " 'HERE': 369,\n",
       " 'MAKE': 370,\n",
       " 'CHECKS': 371,\n",
       " 'TO': 372,\n",
       " 'PROC': 373,\n",
       " 'CODE': 374,\n",
       " 'UNITS': 375,\n",
       " 'DETAILS': 376,\n",
       " 'SERVICES': 377,\n",
       " 'CHARGES': 378,\n",
       " 'INSUR': 379,\n",
       " 'PENDING': 380,\n",
       " 'PATIENT': 381,\n",
       " 'BALANCE': 382,\n",
       " 'EXAM': 383,\n",
       " 'HAND': 384,\n",
       " 'THORAC': 385,\n",
       " 'SPINE': 386,\n",
       " 'COMMERCIAL': 387,\n",
       " 'NON': 388,\n",
       " 'ALLOWED': 389,\n",
       " 'CT': 390,\n",
       " 'ABD': 391,\n",
       " 'PELV': 392,\n",
       " 'PAYMENT': 393,\n",
       " 'CHEST': 394,\n",
       " 'VIEWS': 395,\n",
       " 'HARGES': 396,\n",
       " 'Over': 397,\n",
       " 'DIGIT': 398,\n",
       " 'Of': 399,\n",
       " 'Today': 400,\n",
       " \"'s\": 401,\n",
       " 'Ethnicity': 402,\n",
       " 'Hispanic': 403,\n",
       " 'Latino': 404,\n",
       " 'Preferred': 405,\n",
       " 'English': 406,\n",
       " 'visit': 407,\n",
       " 'with': 408,\n",
       " 'Suzanne': 409,\n",
       " 'Newsom': 410,\n",
       " 'CNP': 411,\n",
       " '•': 412,\n",
       " 'Lethargy': 413,\n",
       " 'cough': 414,\n",
       " 'Vitals': 415,\n",
       " 'lbs': 416,\n",
       " 'kg': 417,\n",
       " 'Wt': 418,\n",
       " 'Temp': 419,\n",
       " 'HR': 420,\n",
       " 'Oxygen': 421,\n",
       " 'sat': 422,\n",
       " 'Allergies': 423,\n",
       " 'Amoxicillin': 424,\n",
       " 'rash': 425,\n",
       " 'possible': 426,\n",
       " 'hives': 427,\n",
       " 'Active': 428,\n",
       " 'Diagnoses': 429,\n",
       " 'Include': 430,\n",
       " 'Acute': 431,\n",
       " 'frontal': 432,\n",
       " 'sinusitis': 433,\n",
       " 'unspecified': 434,\n",
       " 'Dizziness': 435,\n",
       " 'giddiness': 436,\n",
       " 'Medication': 437,\n",
       " 'List': 438,\n",
       " 'medications': 439,\n",
       " 'you': 440,\n",
       " 'Taking': 441,\n",
       " 'Zyrtec': 442,\n",
       " 'Childrens': 443,\n",
       " 'Allergy': 444,\n",
       " 'Notes': 445,\n",
       " 'Tests': 446,\n",
       " 'Labs': 447,\n",
       " 'Illumigene': 448,\n",
       " 'MYCO': 449,\n",
       " 'http': 450,\n",
       " 'BASIC': 451,\n",
       " 'METABOLIC': 452,\n",
       " 'SODIUM': 453,\n",
       " 'Range': 454,\n",
       " 'POTASSIUM': 455,\n",
       " 'CHLORIDE': 456,\n",
       " 'GLUCOSE': 457,\n",
       " 'BUN': 458,\n",
       " 'CREATININE': 459,\n",
       " 'CALCIUM': 460,\n",
       " 'CREA': 461,\n",
       " 'RATIO': 462,\n",
       " 'Ratio': 463,\n",
       " 'ANION': 464,\n",
       " 'GAP': 465,\n",
       " 'Calc': 466,\n",
       " 'CBC': 467,\n",
       " 'DIFF': 468,\n",
       " 'WBC': 469,\n",
       " 'RBC': 470,\n",
       " 'HGB': 471,\n",
       " 'HCT': 472,\n",
       " 'MCV': 473,\n",
       " 'fL': 474,\n",
       " 'MCH': 475,\n",
       " 'pg': 476,\n",
       " 'MCHC': 477,\n",
       " 'MPV': 478,\n",
       " 'PLATELETS': 479,\n",
       " 'NEUTROPHILS': 480,\n",
       " 'LYMPHOCYTES': 481,\n",
       " 'MONOCYTES': 482,\n",
       " 'Conditions': 483,\n",
       " 'Problem': 484,\n",
       " 'Idiopathic': 485,\n",
       " 'urticaria': 486,\n",
       " 'document': 487,\n",
       " 'wish': 488,\n",
       " 'keep': 489,\n",
       " 'Policyholder': 490,\n",
       " 'Owner': 491,\n",
       " 'Eastside': 492,\n",
       " 'Acct': 493,\n",
       " 'Jasminder': 494,\n",
       " 'Singh': 495,\n",
       " 'Dev': 496,\n",
       " 'PA': 497,\n",
       " 'EXCUSE': 498,\n",
       " 'east': 499,\n",
       " 'side': 500,\n",
       " 'medical': 501,\n",
       " 'center': 502,\n",
       " 'April': 503,\n",
       " 'Weekly': 504,\n",
       " 'MEDICAL': 505,\n",
       " 'Ph': 506,\n",
       " 'MR': 507,\n",
       " 'Primary': 508,\n",
       " 'Thoracic': 509,\n",
       " 'Strain': 510,\n",
       " 'have': 511,\n",
       " 'strained': 512,\n",
       " 'your': 513,\n",
       " 'thoracic': 514,\n",
       " 'spine': 515,\n",
       " 'IF': 516,\n",
       " 'ANY': 517,\n",
       " 'FOLLOWING': 518,\n",
       " 'OCCURS': 519,\n",
       " 'feel': 520,\n",
       " 'weakness': 521,\n",
       " 'arms': 522,\n",
       " 'legs': 523,\n",
       " 'severe': 524,\n",
       " 'increase': 525,\n",
       " 'pain': 526,\n",
       " 'Lumbosacral': 527,\n",
       " 'weak': 528,\n",
       " 'becomes': 529,\n",
       " 'more': 530,\n",
       " 'Follow': 531,\n",
       " 'Up': 532,\n",
       " 'What': 533,\n",
       " 'Do': 534,\n",
       " 'Take': 535,\n",
       " 'all': 536,\n",
       " 'as': 537,\n",
       " 'directed': 538,\n",
       " 'Additional': 539,\n",
       " 'Prescriptions': 540,\n",
       " 'Written': 541,\n",
       " 'Prescriber': 542,\n",
       " 'Paper': 543,\n",
       " 'Prescription': 544,\n",
       " 'given': 545,\n",
       " 'patient': 546,\n",
       " 'Preventative': 547,\n",
       " 'Instructions': 548,\n",
       " 'knee': 549,\n",
       " 'injury': 550,\n",
       " 'David': 551,\n",
       " 'Bruce': 552,\n",
       " 'Identiﬁer': 553,\n",
       " 'June': 554,\n",
       " 'Explanation': 555,\n",
       " 'Gap': 556,\n",
       " 'no': 557,\n",
       " 'concussion': 558,\n",
       " 'Assistant': 559,\n",
       " 'devin': 560,\n",
       " 'conrad': 561,\n",
       " 'September': 562,\n",
       " 'ACCIDENT': 563,\n",
       " 'CLAIM': 564,\n",
       " 'FORM': 565,\n",
       " 'ATTENDING': 566,\n",
       " 'PHYSICIAN': 567,\n",
       " 'PLEASE': 568,\n",
       " 'PRINT': 569,\n",
       " 'PART': 570,\n",
       " 'I': 571,\n",
       " 'BE': 572,\n",
       " 'COMPLETED': 573,\n",
       " 'BY': 574,\n",
       " 'ICD': 575,\n",
       " 'first': 576,\n",
       " 'unable': 577,\n",
       " 'work': 578,\n",
       " 'Expected': 579,\n",
       " 'Delivery': 580,\n",
       " 'Actual': 581,\n",
       " 'Unable': 582,\n",
       " 'Vaginal': 583,\n",
       " 'per': 584,\n",
       " 'Continued': 585,\n",
       " 'Facility': 586,\n",
       " 'State': 587,\n",
       " 'Zip': 588,\n",
       " 'Performed': 589,\n",
       " 'Surgical': 590,\n",
       " 'CPT': 591,\n",
       " 'Attending': 592,\n",
       " 'Degree': 593,\n",
       " 'A': 594,\n",
       " 'B': 595,\n",
       " 'Suffix': 596,\n",
       " 'MI': 597,\n",
       " 'Spanish': 598,\n",
       " 'Short': 599,\n",
       " 'Term': 600,\n",
       " 'Disability': 601,\n",
       " 'Long': 602,\n",
       " 'Life': 603,\n",
       " 'Insurance': 604,\n",
       " 'Voluntary': 605,\n",
       " 'Was': 606,\n",
       " 'this': 607,\n",
       " 'motor': 608,\n",
       " 'vehicle': 609,\n",
       " 'accident': 610,\n",
       " 'Physicians': 611,\n",
       " 'Hospitals': 612,\n",
       " 'Considerations': 613,\n",
       " 'Folder': 614,\n",
       " 'Contents': 615,\n",
       " 'Claimant': 616,\n",
       " 'Unauthorized': 617,\n",
       " 'access': 618,\n",
       " 'is': 619,\n",
       " 'strictly': 620,\n",
       " 'probihited': 621,\n",
       " 'Male': 622,\n",
       " 'Reach': 623,\n",
       " 'Return': 624,\n",
       " 'Sprained': 625,\n",
       " 'Ankle': 626,\n",
       " 'Practitioner': 627,\n",
       " 'Monica': 628,\n",
       " 'Shaffer': 629,\n",
       " 'Johnson': 630,\n",
       " 'Ave': 631,\n",
       " 'Bridgeport': 632,\n",
       " 'WV': 633,\n",
       " 'US': 634,\n",
       " 'Accountability': 635,\n",
       " 'Act': 636,\n",
       " 'HIPAA': 637,\n",
       " 'Privacy': 638,\n",
       " 'Rule': 639,\n",
       " 'banks': 640,\n",
       " 'governmental': 641,\n",
       " 'entities': 642,\n",
       " 'communicable': 643,\n",
       " 'disease': 644,\n",
       " 'CL': 645,\n",
       " 'GREGORY': 646,\n",
       " 'we': 647,\n",
       " 'can': 648,\n",
       " 'assist': 649,\n",
       " 'prohibited': 650,\n",
       " 'otherwise': 651,\n",
       " 'permitted': 652,\n",
       " 'law': 653,\n",
       " 'EMS': 654,\n",
       " 'Christopher': 655,\n",
       " 'Bartruff': 656,\n",
       " 'health': 657,\n",
       " 'park': 658,\n",
       " 'blvd': 659,\n",
       " 'Naples': 660,\n",
       " 'FL': 661,\n",
       " 'NCH': 662,\n",
       " 'Emergency': 663,\n",
       " 'Healthcare': 664,\n",
       " 'System': 665,\n",
       " 'Napies': 666,\n",
       " 'North': 667,\n",
       " 'Collier': 668,\n",
       " 'Northeast': 669,\n",
       " 'ED': 670,\n",
       " 'Chief': 671,\n",
       " 'ICD=CC': 672,\n",
       " 'Numbers': 673,\n",
       " 'Feeling': 674,\n",
       " 'Suicidal': 675,\n",
       " 'Help': 676,\n",
       " 'With': 677,\n",
       " 'Where': 678,\n",
       " 'When': 679,\n",
       " 'Comments': 680,\n",
       " 'Dear': 681,\n",
       " 'However': 682,\n",
       " 'Estimado': 683,\n",
       " 'Paciente': 684,\n",
       " 'has': 685,\n",
       " 'been': 686,\n",
       " 'these': 687,\n",
       " 'instructions': 688,\n",
       " 'HOWEVER': 689,\n",
       " 'Education': 690,\n",
       " 'Materials': 691,\n",
       " 'Peds': 692,\n",
       " 'Upper': 693,\n",
       " 'Extremity': 694,\n",
       " 'Contusion': 695,\n",
       " 'Home': 696,\n",
       " 'care': 697,\n",
       " 'any': 698,\n",
       " 'Special': 699,\n",
       " 'note': 700,\n",
       " 'parents': 701,\n",
       " 'seek': 702,\n",
       " 'advice': 703,\n",
       " 'Bruising': 704,\n",
       " 'that': 705,\n",
       " 'gets': 706,\n",
       " 'worse': 707,\n",
       " 'Numbness': 708,\n",
       " 'tingling': 709,\n",
       " 'injured': 710,\n",
       " 'arm': 711,\n",
       " 'Trauma': 712,\n",
       " 'Watch': 713,\n",
       " 'following': 714,\n",
       " 'symptoms': 715,\n",
       " 'Headache': 716,\n",
       " 'Nausea': 717,\n",
       " 'vomiting': 718,\n",
       " 'Sensitivity': 719,\n",
       " 'light': 720,\n",
       " 'noise': 721,\n",
       " 'Unusual': 722,\n",
       " 'sleepiness': 723,\n",
       " 'grogginess': 724,\n",
       " 'Trouble': 725,\n",
       " 'falling': 726,\n",
       " 'asleep': 727,\n",
       " 'Personality': 728,\n",
       " 'changes': 729,\n",
       " 'Vision': 730,\n",
       " 'Memory': 731,\n",
       " 'loss': 732,\n",
       " 'Confusion': 733,\n",
       " 'walking': 734,\n",
       " 'clumsiness': 735,\n",
       " 'Loss': 736,\n",
       " 'consciousness': 737,\n",
       " 'even': 738,\n",
       " 'short': 739,\n",
       " 'time': 740,\n",
       " 'Inability': 741,\n",
       " 'awakened': 742,\n",
       " 'Stiff': 743,\n",
       " 'neck': 744,\n",
       " 'Weakness': 745,\n",
       " 'numbness': 746,\n",
       " 'part': 747,\n",
       " 'body': 748,\n",
       " 'Seizures': 749,\n",
       " 'General': 750,\n",
       " 'Avoid': 751,\n",
       " 'lifting': 752,\n",
       " 'strenuous': 753,\n",
       " 'activities': 754,\n",
       " 'Pain': 755,\n",
       " 'doesn': 756,\n",
       " 't': 757,\n",
       " 'get': 758,\n",
       " 'better': 759,\n",
       " 'worsens': 760,\n",
       " 'increased': 761,\n",
       " 'swelling': 762,\n",
       " 'bruising': 763,\n",
       " 'Sick': 764,\n",
       " 'appearance': 765,\n",
       " 'behaviors': 766,\n",
       " 'worry': 767,\n",
       " 'Excuse': 768,\n",
       " 'From': 769,\n",
       " 'School': 770,\n",
       " 'excuse': 771,\n",
       " 'from': 772,\n",
       " 'until': 773,\n",
       " 'Caregiver': 774,\n",
       " 'Document': 775,\n",
       " 'Released': 776,\n",
       " 'reminders': 777,\n",
       " 'regarding': 778,\n",
       " 'prescriptions': 779,\n",
       " 'provided': 780,\n",
       " 'leaflets': 781,\n",
       " 'COLLIER': 782,\n",
       " 'COUNTRY': 783,\n",
       " 'DIGITECH': 784,\n",
       " 'COMPUTER': 785,\n",
       " 'BILLING': 786,\n",
       " 'ON': 787,\n",
       " 'BEHALF': 788,\n",
       " 'BEDFORD': 789,\n",
       " 'RD': 790,\n",
       " 'BLDG': 791,\n",
       " 'FLOOR': 792,\n",
       " 'CHAPPAQUA': 793,\n",
       " 'NY': 794,\n",
       " 'VISIT': 795,\n",
       " 'HTTPS': 796,\n",
       " 'PAY': 797,\n",
       " 'THIS': 798,\n",
       " 'INVOICE': 799,\n",
       " 'COUNTY': 800,\n",
       " 'NAPLES': 801,\n",
       " 'N': 802,\n",
       " 'NORTH': 803,\n",
       " 'HOSPITAL': 804,\n",
       " 'PARK': 805,\n",
       " 'BLVD': 806,\n",
       " 'BAKER': 807,\n",
       " 'MARCO': 808,\n",
       " 'HEALTHCARE': 809,\n",
       " 'NORTHEAST': 810,\n",
       " 'EMERGENCY': 811,\n",
       " 'DEPARTMENT': 812,\n",
       " 'Lockbox': 813,\n",
       " 'Processing': 814,\n",
       " 'Atlanta': 815,\n",
       " 'GA': 816,\n",
       " 'SYSTEM': 817,\n",
       " 'FOR': 818,\n",
       " 'AADC': 819,\n",
       " 'RE': 820,\n",
       " 'thirty': 821,\n",
       " 'days': 822,\n",
       " 'receipt': 823,\n",
       " 'letter': 824,\n",
       " 'Accounting': 825,\n",
       " 'SOUTHWEST': 826,\n",
       " 'FLORIDA': 827,\n",
       " 'MANAGEMENT': 828,\n",
       " 'CINCINNATI': 829,\n",
       " 'OH': 830,\n",
       " 'PHONE': 831,\n",
       " 'TH_AR_LTR': 832,\n",
       " 'SERVICE': 833,\n",
       " 'NUMBER': 834,\n",
       " 'JASON': 835,\n",
       " 'SANTANA': 836,\n",
       " 'Sincerely': 837,\n",
       " 'PLANTATION': 838,\n",
       " 'Allstate': 839,\n",
       " 'Youre': 840,\n",
       " 'good': 841,\n",
       " 'hands': 842,\n",
       " 'Florida': 843,\n",
       " 'PIP': 844,\n",
       " 'Central': 845,\n",
       " 'CLINTON': 846,\n",
       " 'IA': 847,\n",
       " 'February': 848,\n",
       " 'INSURED': 849,\n",
       " 'LOSS': 850,\n",
       " 'FAX': 851,\n",
       " 'OFFICE': 852,\n",
       " 'HOURS': 853,\n",
       " 'VYPHAPHONE': 854,\n",
       " 'INTHALANGSY': 855,\n",
       " 'EXT': 856,\n",
       " \"'re\": 857,\n",
       " 'Jacksonville': 858,\n",
       " 'DALLAS': 859,\n",
       " 'TX': 860,\n",
       " 'Some': 861,\n",
       " 'specifics': 862,\n",
       " 'request': 863,\n",
       " 'sincerely': 864,\n",
       " 'CAITLEN': 865,\n",
       " 'CARROLL': 866,\n",
       " 'Ext': 867,\n",
       " 'date': 868,\n",
       " 'check': 869,\n",
       " 'type': 870,\n",
       " 'claim': 871,\n",
       " 'filing': 872,\n",
       " 'name': 873,\n",
       " 'Cellular': 874,\n",
       " 'MedSupport': 875,\n",
       " 'policy': 876,\n",
       " 'policies': 877,\n",
       " 'Female': 878,\n",
       " 'Domestic': 879,\n",
       " 'Partner': 880,\n",
       " 'information': 881,\n",
       " 'Condition': 882,\n",
       " 'am': 883,\n",
       " 'pm': 884,\n",
       " 'Confinement': 885,\n",
       " 'Dates': 886,\n",
       " 'Teri': 887,\n",
       " 'Willochell': 888,\n",
       " 'phycian': 889,\n",
       " 'considerations': 890,\n",
       " 'each': 891,\n",
       " 'such': 892,\n",
       " 'violation': 893,\n",
       " 'number': 894,\n",
       " 'indicated': 895,\n",
       " 'above': 896,\n",
       " 'My': 897,\n",
       " 'Member': 898,\n",
       " 'Relationship': 899,\n",
       " 'person': 900,\n",
       " 'granting': 901,\n",
       " 'authority': 902,\n",
       " 'birth': 903,\n",
       " 'Including': 904,\n",
       " 'L': 905,\n",
       " 'lesser': 906,\n",
       " 'If': 907,\n",
       " 'yes': 908,\n",
       " 'please': 909,\n",
       " 'provide': 910,\n",
       " 'tha': 911,\n",
       " 'diagnosis': 912,\n",
       " 'Treatment': 913,\n",
       " 'advise': 914,\n",
       " 'stop': 915,\n",
       " 'working': 916,\n",
       " 'what': 917,\n",
       " 'Willochel': 918,\n",
       " 'Specially': 919,\n",
       " 'internal': 920,\n",
       " 'medicine': 921,\n",
       " 'MD': 922,\n",
       " 'MedExpress': 923,\n",
       " 'Urgent': 924,\n",
       " 'Care': 925,\n",
       " 'Route': 926,\n",
       " 'HIPPA': 927,\n",
       " 'Socia': 928,\n",
       " 'Location': 929,\n",
       " 'Norwin': 930,\n",
       " 'Huntingdon': 931,\n",
       " 'Holder': 932,\n",
       " 'Sex': 933,\n",
       " 'COMP': 934,\n",
       " 'Clinical': 935,\n",
       " 'Report': 936,\n",
       " 'BP': 937,\n",
       " 'mmHg': 938,\n",
       " 'PULSE': 939,\n",
       " 'bpm': 940,\n",
       " 'RESP': 941,\n",
       " 'TEMP': 942,\n",
       " 'WEIGTH': 943,\n",
       " 'ft': 944,\n",
       " 'BMI': 945,\n",
       " 'LMP': 946,\n",
       " 'PMP': 947,\n",
       " 'SAT': 948,\n",
       " 'CONTUSION': 949,\n",
       " 'Meds': 950,\n",
       " 'ACTIVE': 951,\n",
       " 'acetaminophen': 952,\n",
       " 'albuterol': 953,\n",
       " 'bulk': 954,\n",
       " 'Dilantin': 955,\n",
       " 'gabapentin': 956,\n",
       " 'Humalog': 957,\n",
       " 'Lamictal': 958,\n",
       " 'Lyrica': 959,\n",
       " 'Neurontin': 960,\n",
       " 'valacyclovir': 961,\n",
       " 'Procedures': 962,\n",
       " 'FOOT': 963,\n",
       " 'MIN': 964,\n",
       " 'THREE': 965,\n",
       " 'ESTAB': 966,\n",
       " 'URGENT': 967,\n",
       " 'CARE': 968,\n",
       " 'Pocket': 969,\n",
       " 'Met': 970,\n",
       " 'Estimated': 971,\n",
       " 'charges': 972,\n",
       " 'MSO': 973,\n",
       " 'LLC': 974,\n",
       " 'Med': 975,\n",
       " 'Express': 976,\n",
       " 'UC': 977,\n",
       " 'HUNINGTON': 978,\n",
       " 'Merchant': 979,\n",
       " 'Transaction': 980,\n",
       " 'PURCHASE': 981,\n",
       " 'Approval': 982,\n",
       " 'code': 983,\n",
       " 'Record': 984,\n",
       " 'Visa': 985,\n",
       " 'Trace': 986,\n",
       " 'reference': 987,\n",
       " 'Cardholder': 988,\n",
       " 'identifier': 989,\n",
       " 'Application': 990,\n",
       " 'label': 991,\n",
       " 'TVR': 992,\n",
       " 'AID': 993,\n",
       " 'Subtotal': 994,\n",
       " 'Sales': 995,\n",
       " 'customer': 996,\n",
       " 'copy': 997,\n",
       " 'send': 998,\n",
       " 'payments': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'UNK',\n",
       " 1: ' ',\n",
       " 2: '\\t',\n",
       " 3: '\\n',\n",
       " 4: 'Claim',\n",
       " 5: 'Type',\n",
       " 6: 'VB',\n",
       " 7: 'Accident',\n",
       " 8: 'Accidental',\n",
       " 9: 'Injury',\n",
       " 10: 'Information',\n",
       " 11: 'First',\n",
       " 12: 'Name',\n",
       " 13: 'Middle',\n",
       " 14: 'Last',\n",
       " 15: 'Social',\n",
       " 16: 'Security',\n",
       " 17: 'Number',\n",
       " 18: 'Birth',\n",
       " 19: 'Date',\n",
       " 20: 'Gender',\n",
       " 21: 'Language',\n",
       " 22: 'Preference',\n",
       " 23: 'Address',\n",
       " 24: 'Line',\n",
       " 25: 'City',\n",
       " 26: 'Postal',\n",
       " 27: 'Code',\n",
       " 28: 'Country',\n",
       " 29: 'Best',\n",
       " 30: 'Phone',\n",
       " 31: 'to',\n",
       " 32: 'be',\n",
       " 33: 'Reached',\n",
       " 34: 'During',\n",
       " 35: 'the',\n",
       " 36: 'Day',\n",
       " 37: 'Email',\n",
       " 38: 'Page',\n",
       " 39: 'of',\n",
       " 40: 'RADIOLOGY',\n",
       " 41: 'REPORT',\n",
       " 42: 'Patient',\n",
       " 43: 'MRN',\n",
       " 44: 'Accession',\n",
       " 45: 'No',\n",
       " 46: 'Ref',\n",
       " 47: 'Physician',\n",
       " 48: 'UNKNOWN',\n",
       " 49: 'Study',\n",
       " 50: 'Hospital',\n",
       " 51: 'DOB',\n",
       " 52: 'Technique',\n",
       " 53: 'views',\n",
       " 54: 'left',\n",
       " 55: 'wrist',\n",
       " 56: 'Cormarison',\n",
       " 57: 'None',\n",
       " 58: 'availabie',\n",
       " 59: 'Comparison',\n",
       " 60: 'available',\n",
       " 61: 'FINDINGS',\n",
       " 62: 'IMPRESSION',\n",
       " 63: 'acute',\n",
       " 64: 'osseous',\n",
       " 65: 'abnormality',\n",
       " 66: 'identified',\n",
       " 67: 'Daytime',\n",
       " 68: 'Event',\n",
       " 69: 'Stopped',\n",
       " 70: 'Working',\n",
       " 71: 'Yes',\n",
       " 72: 'Physically',\n",
       " 73: 'at',\n",
       " 74: 'Work',\n",
       " 75: 'Hours',\n",
       " 76: 'Worked',\n",
       " 77: 'on',\n",
       " 78: 'Scheduled',\n",
       " 79: 'Missed',\n",
       " 80: 'Returned',\n",
       " 81: 'Related',\n",
       " 82: 'Time',\n",
       " 83: 'Diagnosis',\n",
       " 84: 'Arthiscopic',\n",
       " 85: 'surgery',\n",
       " 86: 'Surgery',\n",
       " 87: 'Is',\n",
       " 88: 'Required',\n",
       " 89: 'Indicator',\n",
       " 90: 'Outpatient',\n",
       " 91: 'Medical',\n",
       " 92: 'Provider',\n",
       " 93: 'Roles',\n",
       " 94: 'Treating',\n",
       " 95: 'Patrick',\n",
       " 96: 'Emerson',\n",
       " 97: 'Business',\n",
       " 98: 'Telephone',\n",
       " 99: 'Fax',\n",
       " 100: 'Visit',\n",
       " 101: 'Next',\n",
       " 102: 'Hospitalization',\n",
       " 103: 'Discharge',\n",
       " 104: 'Procedure',\n",
       " 105: 'Left',\n",
       " 106: 'arthiscopic',\n",
       " 107: 'Employment',\n",
       " 108: 'Employer',\n",
       " 109: 'Policy',\n",
       " 110: 'Electronic',\n",
       " 111: 'Submission',\n",
       " 112: 'Identifier',\n",
       " 113: 'Electronically',\n",
       " 114: 'Signed',\n",
       " 115: 'Fraud',\n",
       " 116: 'Statements',\n",
       " 117: 'Reviewed',\n",
       " 118: 'and',\n",
       " 119: 'unum',\n",
       " 120: 'The',\n",
       " 121: 'Benefits',\n",
       " 122: 'Center',\n",
       " 123: 'Not',\n",
       " 124: 'for',\n",
       " 125: 'FMLA',\n",
       " 126: 'Requests',\n",
       " 127: 'Insured',\n",
       " 128: '’',\n",
       " 129: 's',\n",
       " 130: 'Signature',\n",
       " 131: 'Printed',\n",
       " 132: 'Unum',\n",
       " 133: 'Confirmation',\n",
       " 134: 'Coverage',\n",
       " 135: 'Group',\n",
       " 136: 'Customer',\n",
       " 137: 'EE',\n",
       " 138: 'Effective',\n",
       " 139: 'Employee',\n",
       " 140: 'Acc',\n",
       " 141: 'January',\n",
       " 142: 'Wellness',\n",
       " 143: 'Benefit',\n",
       " 144: 'Total',\n",
       " 145: 'Monthly',\n",
       " 146: 'Premium',\n",
       " 147: 'Montly',\n",
       " 148: 'Payroll',\n",
       " 149: 'Deduction',\n",
       " 150: 'Account',\n",
       " 151: 'F',\n",
       " 152: 'Exam',\n",
       " 153: 'Referring',\n",
       " 154: 'Phys',\n",
       " 155: 'STEPHEN',\n",
       " 156: 'GELOVICH',\n",
       " 157: 'Tax',\n",
       " 158: 'MRI',\n",
       " 159: 'OF',\n",
       " 160: 'THE',\n",
       " 161: 'LEFT',\n",
       " 162: 'WRIST',\n",
       " 163: 'WITHOUT',\n",
       " 164: 'CONTRAST',\n",
       " 165: 'COMPARISON',\n",
       " 166: 'These',\n",
       " 167: 'results',\n",
       " 168: 'were',\n",
       " 169: 'faxed',\n",
       " 170: 'Gelovich',\n",
       " 171: 'signed',\n",
       " 172: 'by',\n",
       " 173: 'Stephen',\n",
       " 174: 'Bravo',\n",
       " 175: 'Dependent',\n",
       " 176: 'Detail',\n",
       " 177: 'Zachary',\n",
       " 178: 'Jager',\n",
       " 179: 'Billed',\n",
       " 180: 'Amounts',\n",
       " 181: 'Contract',\n",
       " 182: 'Adjustment',\n",
       " 183: 'Allowed',\n",
       " 184: 'Amount',\n",
       " 185: 'Covered',\n",
       " 186: 'Reason',\n",
       " 187: 'Deductible',\n",
       " 188: 'Other',\n",
       " 189: 'Carrier',\n",
       " 190: 'Paid',\n",
       " 191: 'Responsibility',\n",
       " 192: 'Totals',\n",
       " 193: 'For',\n",
       " 194: 'Appeals',\n",
       " 195: 'Rights',\n",
       " 196: 'Important',\n",
       " 197: 'about',\n",
       " 198: 'Your',\n",
       " 199: 'Appeal',\n",
       " 200: 'All',\n",
       " 201: 'Languages',\n",
       " 202: 'Contact',\n",
       " 203: 'Did',\n",
       " 204: 'You',\n",
       " 205: 'Know',\n",
       " 206: 'Specialty',\n",
       " 207: 'Orthopedic',\n",
       " 208: 'Surgeon',\n",
       " 209: 'Unknown',\n",
       " 210: 'Kari',\n",
       " 211: 'Lund',\n",
       " 212: 'Orthopedist',\n",
       " 213: 'Dan',\n",
       " 214: 'Palmer',\n",
       " 215: 'On',\n",
       " 216: '&',\n",
       " 217: 'May',\n",
       " 218: 'Spouse',\n",
       " 219: 'Child',\n",
       " 220: 'BLACK',\n",
       " 221: 'HILLS',\n",
       " 222: 'ORTHOPEDIC',\n",
       " 223: 'CENTER',\n",
       " 224: 'PC',\n",
       " 225: 'LAST',\n",
       " 226: 'PMT',\n",
       " 227: 'AMOUNT',\n",
       " 228: 'DUE',\n",
       " 229: 'DATE',\n",
       " 230: 'PAGE',\n",
       " 231: 'STATEMENT',\n",
       " 232: 'Ins',\n",
       " 233: 'Description',\n",
       " 234: 'E',\n",
       " 235: 'm',\n",
       " 236: 'New',\n",
       " 237: 'Moderat',\n",
       " 238: 'S',\n",
       " 239: 'Clo',\n",
       " 240: 'Tx',\n",
       " 241: 'Phalangealfx',\n",
       " 242: 'Finger',\n",
       " 243: 'Splint',\n",
       " 244: 'Offic',\n",
       " 245: 'Cons',\n",
       " 246: 'Moderate',\n",
       " 247: 'Sever',\n",
       " 248: 'Rad',\n",
       " 249: 'Mini',\n",
       " 250: 'Views',\n",
       " 251: 'Applic',\n",
       " 252: 'Hand',\n",
       " 253: 'Lower',\n",
       " 254: 'Forearm',\n",
       " 255: 'Fiberglass',\n",
       " 256: 'gauntlet',\n",
       " 257: 'Cast',\n",
       " 258: 'Yrs',\n",
       " 259: 'Charge',\n",
       " 260: 'Pmt',\n",
       " 261: 'Pat',\n",
       " 262: 'Adjust',\n",
       " 263: 'Current',\n",
       " 264: 'Days',\n",
       " 265: 'Balance',\n",
       " 266: 'Pending',\n",
       " 267: 'Now',\n",
       " 268: 'Due',\n",
       " 269: 'Message',\n",
       " 270: 'Make',\n",
       " 271: 'Checks',\n",
       " 272: 'Payable',\n",
       " 273: 'To',\n",
       " 274: 'Statement',\n",
       " 275: 'Billing',\n",
       " 276: 'Questions',\n",
       " 277: 'Choice',\n",
       " 278: 'Health',\n",
       " 279: 'Administrators',\n",
       " 280: 'Forwarding',\n",
       " 281: 'Service',\n",
       " 282: 'Requested',\n",
       " 283: 'REGIONAL',\n",
       " 284: 'HEALTH',\n",
       " 285: 'INC',\n",
       " 286: 'Participant',\n",
       " 287: 'ID',\n",
       " 288: 'Original',\n",
       " 289: 'Print',\n",
       " 290: 'Website',\n",
       " 291: 'DEA',\n",
       " 292: 'Individual',\n",
       " 293: 'Summary',\n",
       " 294: 'By',\n",
       " 295: 'Plan',\n",
       " 296: 'Status',\n",
       " 297: 'Period',\n",
       " 298: 'DEDUCTIBLE',\n",
       " 299: 'OUT',\n",
       " 300: 'POCKET',\n",
       " 301: 'Regional',\n",
       " 302: 'Inc',\n",
       " 303: 'EXPLANATION',\n",
       " 304: 'BENEFITS',\n",
       " 305: 'Retain',\n",
       " 306: 'Purposes',\n",
       " 307: 'Family',\n",
       " 308: 'Out',\n",
       " 309: 'Network',\n",
       " 310: 'Karl',\n",
       " 311: 'Services',\n",
       " 312: 'exam',\n",
       " 313: 'hand',\n",
       " 314: 'Modifiers',\n",
       " 315: 'TC',\n",
       " 316: 'RT',\n",
       " 317: 'This',\n",
       " 318: 'Qualified',\n",
       " 319: 'sign',\n",
       " 320: 'language',\n",
       " 321: 'interpreters',\n",
       " 322: 'written',\n",
       " 323: 'in',\n",
       " 324: 'other',\n",
       " 325: 'languages',\n",
       " 326: 'Jacquelin',\n",
       " 327: 'Brainard',\n",
       " 328: 'Compliance',\n",
       " 329: 'Officer',\n",
       " 330: 'or',\n",
       " 331: 'mail',\n",
       " 332: 'phone',\n",
       " 333: 'Department',\n",
       " 334: 'Human',\n",
       " 335: 'Complaint',\n",
       " 336: 'forms',\n",
       " 337: 'are',\n",
       " 338: 'Sign',\n",
       " 339: 'up',\n",
       " 340: 'paperless',\n",
       " 341: 'DAKOTA',\n",
       " 342: 'EZ',\n",
       " 343: 'Ways',\n",
       " 344: 'Pay',\n",
       " 345: 'Automated',\n",
       " 346: 'Attendant',\n",
       " 347: 'hours',\n",
       " 348: 'a',\n",
       " 349: 'day',\n",
       " 350: 'Payments',\n",
       " 351: 'Please',\n",
       " 352: 'Call',\n",
       " 353: 'Upon',\n",
       " 354: 'Receipt',\n",
       " 355: 'Improved',\n",
       " 356: 'Online',\n",
       " 357: 'Experience',\n",
       " 358: '|',\n",
       " 359: 'Update',\n",
       " 360: 'Info',\n",
       " 361: 'About',\n",
       " 362: 'See',\n",
       " 363: 'Details',\n",
       " 364: 'Back',\n",
       " 365: 'ACCOUNT',\n",
       " 366: 'NO',\n",
       " 367: 'SHOW',\n",
       " 368: 'PAID',\n",
       " 369: 'HERE',\n",
       " 370: 'MAKE',\n",
       " 371: 'CHECKS',\n",
       " 372: 'TO',\n",
       " 373: 'PROC',\n",
       " 374: 'CODE',\n",
       " 375: 'UNITS',\n",
       " 376: 'DETAILS',\n",
       " 377: 'SERVICES',\n",
       " 378: 'CHARGES',\n",
       " 379: 'INSUR',\n",
       " 380: 'PENDING',\n",
       " 381: 'PATIENT',\n",
       " 382: 'BALANCE',\n",
       " 383: 'EXAM',\n",
       " 384: 'HAND',\n",
       " 385: 'THORAC',\n",
       " 386: 'SPINE',\n",
       " 387: 'COMMERCIAL',\n",
       " 388: 'NON',\n",
       " 389: 'ALLOWED',\n",
       " 390: 'CT',\n",
       " 391: 'ABD',\n",
       " 392: 'PELV',\n",
       " 393: 'PAYMENT',\n",
       " 394: 'CHEST',\n",
       " 395: 'VIEWS',\n",
       " 396: 'HARGES',\n",
       " 397: 'Over',\n",
       " 398: 'DIGIT',\n",
       " 399: 'Of',\n",
       " 400: 'Today',\n",
       " 401: \"'s\",\n",
       " 402: 'Ethnicity',\n",
       " 403: 'Hispanic',\n",
       " 404: 'Latino',\n",
       " 405: 'Preferred',\n",
       " 406: 'English',\n",
       " 407: 'visit',\n",
       " 408: 'with',\n",
       " 409: 'Suzanne',\n",
       " 410: 'Newsom',\n",
       " 411: 'CNP',\n",
       " 412: '•',\n",
       " 413: 'Lethargy',\n",
       " 414: 'cough',\n",
       " 415: 'Vitals',\n",
       " 416: 'lbs',\n",
       " 417: 'kg',\n",
       " 418: 'Wt',\n",
       " 419: 'Temp',\n",
       " 420: 'HR',\n",
       " 421: 'Oxygen',\n",
       " 422: 'sat',\n",
       " 423: 'Allergies',\n",
       " 424: 'Amoxicillin',\n",
       " 425: 'rash',\n",
       " 426: 'possible',\n",
       " 427: 'hives',\n",
       " 428: 'Active',\n",
       " 429: 'Diagnoses',\n",
       " 430: 'Include',\n",
       " 431: 'Acute',\n",
       " 432: 'frontal',\n",
       " 433: 'sinusitis',\n",
       " 434: 'unspecified',\n",
       " 435: 'Dizziness',\n",
       " 436: 'giddiness',\n",
       " 437: 'Medication',\n",
       " 438: 'List',\n",
       " 439: 'medications',\n",
       " 440: 'you',\n",
       " 441: 'Taking',\n",
       " 442: 'Zyrtec',\n",
       " 443: 'Childrens',\n",
       " 444: 'Allergy',\n",
       " 445: 'Notes',\n",
       " 446: 'Tests',\n",
       " 447: 'Labs',\n",
       " 448: 'Illumigene',\n",
       " 449: 'MYCO',\n",
       " 450: 'http',\n",
       " 451: 'BASIC',\n",
       " 452: 'METABOLIC',\n",
       " 453: 'SODIUM',\n",
       " 454: 'Range',\n",
       " 455: 'POTASSIUM',\n",
       " 456: 'CHLORIDE',\n",
       " 457: 'GLUCOSE',\n",
       " 458: 'BUN',\n",
       " 459: 'CREATININE',\n",
       " 460: 'CALCIUM',\n",
       " 461: 'CREA',\n",
       " 462: 'RATIO',\n",
       " 463: 'Ratio',\n",
       " 464: 'ANION',\n",
       " 465: 'GAP',\n",
       " 466: 'Calc',\n",
       " 467: 'CBC',\n",
       " 468: 'DIFF',\n",
       " 469: 'WBC',\n",
       " 470: 'RBC',\n",
       " 471: 'HGB',\n",
       " 472: 'HCT',\n",
       " 473: 'MCV',\n",
       " 474: 'fL',\n",
       " 475: 'MCH',\n",
       " 476: 'pg',\n",
       " 477: 'MCHC',\n",
       " 478: 'MPV',\n",
       " 479: 'PLATELETS',\n",
       " 480: 'NEUTROPHILS',\n",
       " 481: 'LYMPHOCYTES',\n",
       " 482: 'MONOCYTES',\n",
       " 483: 'Conditions',\n",
       " 484: 'Problem',\n",
       " 485: 'Idiopathic',\n",
       " 486: 'urticaria',\n",
       " 487: 'document',\n",
       " 488: 'wish',\n",
       " 489: 'keep',\n",
       " 490: 'Policyholder',\n",
       " 491: 'Owner',\n",
       " 492: 'Eastside',\n",
       " 493: 'Acct',\n",
       " 494: 'Jasminder',\n",
       " 495: 'Singh',\n",
       " 496: 'Dev',\n",
       " 497: 'PA',\n",
       " 498: 'EXCUSE',\n",
       " 499: 'east',\n",
       " 500: 'side',\n",
       " 501: 'medical',\n",
       " 502: 'center',\n",
       " 503: 'April',\n",
       " 504: 'Weekly',\n",
       " 505: 'MEDICAL',\n",
       " 506: 'Ph',\n",
       " 507: 'MR',\n",
       " 508: 'Primary',\n",
       " 509: 'Thoracic',\n",
       " 510: 'Strain',\n",
       " 511: 'have',\n",
       " 512: 'strained',\n",
       " 513: 'your',\n",
       " 514: 'thoracic',\n",
       " 515: 'spine',\n",
       " 516: 'IF',\n",
       " 517: 'ANY',\n",
       " 518: 'FOLLOWING',\n",
       " 519: 'OCCURS',\n",
       " 520: 'feel',\n",
       " 521: 'weakness',\n",
       " 522: 'arms',\n",
       " 523: 'legs',\n",
       " 524: 'severe',\n",
       " 525: 'increase',\n",
       " 526: 'pain',\n",
       " 527: 'Lumbosacral',\n",
       " 528: 'weak',\n",
       " 529: 'becomes',\n",
       " 530: 'more',\n",
       " 531: 'Follow',\n",
       " 532: 'Up',\n",
       " 533: 'What',\n",
       " 534: 'Do',\n",
       " 535: 'Take',\n",
       " 536: 'all',\n",
       " 537: 'as',\n",
       " 538: 'directed',\n",
       " 539: 'Additional',\n",
       " 540: 'Prescriptions',\n",
       " 541: 'Written',\n",
       " 542: 'Prescriber',\n",
       " 543: 'Paper',\n",
       " 544: 'Prescription',\n",
       " 545: 'given',\n",
       " 546: 'patient',\n",
       " 547: 'Preventative',\n",
       " 548: 'Instructions',\n",
       " 549: 'knee',\n",
       " 550: 'injury',\n",
       " 551: 'David',\n",
       " 552: 'Bruce',\n",
       " 553: 'Identiﬁer',\n",
       " 554: 'June',\n",
       " 555: 'Explanation',\n",
       " 556: 'Gap',\n",
       " 557: 'no',\n",
       " 558: 'concussion',\n",
       " 559: 'Assistant',\n",
       " 560: 'devin',\n",
       " 561: 'conrad',\n",
       " 562: 'September',\n",
       " 563: 'ACCIDENT',\n",
       " 564: 'CLAIM',\n",
       " 565: 'FORM',\n",
       " 566: 'ATTENDING',\n",
       " 567: 'PHYSICIAN',\n",
       " 568: 'PLEASE',\n",
       " 569: 'PRINT',\n",
       " 570: 'PART',\n",
       " 571: 'I',\n",
       " 572: 'BE',\n",
       " 573: 'COMPLETED',\n",
       " 574: 'BY',\n",
       " 575: 'ICD',\n",
       " 576: 'first',\n",
       " 577: 'unable',\n",
       " 578: 'work',\n",
       " 579: 'Expected',\n",
       " 580: 'Delivery',\n",
       " 581: 'Actual',\n",
       " 582: 'Unable',\n",
       " 583: 'Vaginal',\n",
       " 584: 'per',\n",
       " 585: 'Continued',\n",
       " 586: 'Facility',\n",
       " 587: 'State',\n",
       " 588: 'Zip',\n",
       " 589: 'Performed',\n",
       " 590: 'Surgical',\n",
       " 591: 'CPT',\n",
       " 592: 'Attending',\n",
       " 593: 'Degree',\n",
       " 594: 'A',\n",
       " 595: 'B',\n",
       " 596: 'Suffix',\n",
       " 597: 'MI',\n",
       " 598: 'Spanish',\n",
       " 599: 'Short',\n",
       " 600: 'Term',\n",
       " 601: 'Disability',\n",
       " 602: 'Long',\n",
       " 603: 'Life',\n",
       " 604: 'Insurance',\n",
       " 605: 'Voluntary',\n",
       " 606: 'Was',\n",
       " 607: 'this',\n",
       " 608: 'motor',\n",
       " 609: 'vehicle',\n",
       " 610: 'accident',\n",
       " 611: 'Physicians',\n",
       " 612: 'Hospitals',\n",
       " 613: 'Considerations',\n",
       " 614: 'Folder',\n",
       " 615: 'Contents',\n",
       " 616: 'Claimant',\n",
       " 617: 'Unauthorized',\n",
       " 618: 'access',\n",
       " 619: 'is',\n",
       " 620: 'strictly',\n",
       " 621: 'probihited',\n",
       " 622: 'Male',\n",
       " 623: 'Reach',\n",
       " 624: 'Return',\n",
       " 625: 'Sprained',\n",
       " 626: 'Ankle',\n",
       " 627: 'Practitioner',\n",
       " 628: 'Monica',\n",
       " 629: 'Shaffer',\n",
       " 630: 'Johnson',\n",
       " 631: 'Ave',\n",
       " 632: 'Bridgeport',\n",
       " 633: 'WV',\n",
       " 634: 'US',\n",
       " 635: 'Accountability',\n",
       " 636: 'Act',\n",
       " 637: 'HIPAA',\n",
       " 638: 'Privacy',\n",
       " 639: 'Rule',\n",
       " 640: 'banks',\n",
       " 641: 'governmental',\n",
       " 642: 'entities',\n",
       " 643: 'communicable',\n",
       " 644: 'disease',\n",
       " 645: 'CL',\n",
       " 646: 'GREGORY',\n",
       " 647: 'we',\n",
       " 648: 'can',\n",
       " 649: 'assist',\n",
       " 650: 'prohibited',\n",
       " 651: 'otherwise',\n",
       " 652: 'permitted',\n",
       " 653: 'law',\n",
       " 654: 'EMS',\n",
       " 655: 'Christopher',\n",
       " 656: 'Bartruff',\n",
       " 657: 'health',\n",
       " 658: 'park',\n",
       " 659: 'blvd',\n",
       " 660: 'Naples',\n",
       " 661: 'FL',\n",
       " 662: 'NCH',\n",
       " 663: 'Emergency',\n",
       " 664: 'Healthcare',\n",
       " 665: 'System',\n",
       " 666: 'Napies',\n",
       " 667: 'North',\n",
       " 668: 'Collier',\n",
       " 669: 'Northeast',\n",
       " 670: 'ED',\n",
       " 671: 'Chief',\n",
       " 672: 'ICD=CC',\n",
       " 673: 'Numbers',\n",
       " 674: 'Feeling',\n",
       " 675: 'Suicidal',\n",
       " 676: 'Help',\n",
       " 677: 'With',\n",
       " 678: 'Where',\n",
       " 679: 'When',\n",
       " 680: 'Comments',\n",
       " 681: 'Dear',\n",
       " 682: 'However',\n",
       " 683: 'Estimado',\n",
       " 684: 'Paciente',\n",
       " 685: 'has',\n",
       " 686: 'been',\n",
       " 687: 'these',\n",
       " 688: 'instructions',\n",
       " 689: 'HOWEVER',\n",
       " 690: 'Education',\n",
       " 691: 'Materials',\n",
       " 692: 'Peds',\n",
       " 693: 'Upper',\n",
       " 694: 'Extremity',\n",
       " 695: 'Contusion',\n",
       " 696: 'Home',\n",
       " 697: 'care',\n",
       " 698: 'any',\n",
       " 699: 'Special',\n",
       " 700: 'note',\n",
       " 701: 'parents',\n",
       " 702: 'seek',\n",
       " 703: 'advice',\n",
       " 704: 'Bruising',\n",
       " 705: 'that',\n",
       " 706: 'gets',\n",
       " 707: 'worse',\n",
       " 708: 'Numbness',\n",
       " 709: 'tingling',\n",
       " 710: 'injured',\n",
       " 711: 'arm',\n",
       " 712: 'Trauma',\n",
       " 713: 'Watch',\n",
       " 714: 'following',\n",
       " 715: 'symptoms',\n",
       " 716: 'Headache',\n",
       " 717: 'Nausea',\n",
       " 718: 'vomiting',\n",
       " 719: 'Sensitivity',\n",
       " 720: 'light',\n",
       " 721: 'noise',\n",
       " 722: 'Unusual',\n",
       " 723: 'sleepiness',\n",
       " 724: 'grogginess',\n",
       " 725: 'Trouble',\n",
       " 726: 'falling',\n",
       " 727: 'asleep',\n",
       " 728: 'Personality',\n",
       " 729: 'changes',\n",
       " 730: 'Vision',\n",
       " 731: 'Memory',\n",
       " 732: 'loss',\n",
       " 733: 'Confusion',\n",
       " 734: 'walking',\n",
       " 735: 'clumsiness',\n",
       " 736: 'Loss',\n",
       " 737: 'consciousness',\n",
       " 738: 'even',\n",
       " 739: 'short',\n",
       " 740: 'time',\n",
       " 741: 'Inability',\n",
       " 742: 'awakened',\n",
       " 743: 'Stiff',\n",
       " 744: 'neck',\n",
       " 745: 'Weakness',\n",
       " 746: 'numbness',\n",
       " 747: 'part',\n",
       " 748: 'body',\n",
       " 749: 'Seizures',\n",
       " 750: 'General',\n",
       " 751: 'Avoid',\n",
       " 752: 'lifting',\n",
       " 753: 'strenuous',\n",
       " 754: 'activities',\n",
       " 755: 'Pain',\n",
       " 756: 'doesn',\n",
       " 757: 't',\n",
       " 758: 'get',\n",
       " 759: 'better',\n",
       " 760: 'worsens',\n",
       " 761: 'increased',\n",
       " 762: 'swelling',\n",
       " 763: 'bruising',\n",
       " 764: 'Sick',\n",
       " 765: 'appearance',\n",
       " 766: 'behaviors',\n",
       " 767: 'worry',\n",
       " 768: 'Excuse',\n",
       " 769: 'From',\n",
       " 770: 'School',\n",
       " 771: 'excuse',\n",
       " 772: 'from',\n",
       " 773: 'until',\n",
       " 774: 'Caregiver',\n",
       " 775: 'Document',\n",
       " 776: 'Released',\n",
       " 777: 'reminders',\n",
       " 778: 'regarding',\n",
       " 779: 'prescriptions',\n",
       " 780: 'provided',\n",
       " 781: 'leaflets',\n",
       " 782: 'COLLIER',\n",
       " 783: 'COUNTRY',\n",
       " 784: 'DIGITECH',\n",
       " 785: 'COMPUTER',\n",
       " 786: 'BILLING',\n",
       " 787: 'ON',\n",
       " 788: 'BEHALF',\n",
       " 789: 'BEDFORD',\n",
       " 790: 'RD',\n",
       " 791: 'BLDG',\n",
       " 792: 'FLOOR',\n",
       " 793: 'CHAPPAQUA',\n",
       " 794: 'NY',\n",
       " 795: 'VISIT',\n",
       " 796: 'HTTPS',\n",
       " 797: 'PAY',\n",
       " 798: 'THIS',\n",
       " 799: 'INVOICE',\n",
       " 800: 'COUNTY',\n",
       " 801: 'NAPLES',\n",
       " 802: 'N',\n",
       " 803: 'NORTH',\n",
       " 804: 'HOSPITAL',\n",
       " 805: 'PARK',\n",
       " 806: 'BLVD',\n",
       " 807: 'BAKER',\n",
       " 808: 'MARCO',\n",
       " 809: 'HEALTHCARE',\n",
       " 810: 'NORTHEAST',\n",
       " 811: 'EMERGENCY',\n",
       " 812: 'DEPARTMENT',\n",
       " 813: 'Lockbox',\n",
       " 814: 'Processing',\n",
       " 815: 'Atlanta',\n",
       " 816: 'GA',\n",
       " 817: 'SYSTEM',\n",
       " 818: 'FOR',\n",
       " 819: 'AADC',\n",
       " 820: 'RE',\n",
       " 821: 'thirty',\n",
       " 822: 'days',\n",
       " 823: 'receipt',\n",
       " 824: 'letter',\n",
       " 825: 'Accounting',\n",
       " 826: 'SOUTHWEST',\n",
       " 827: 'FLORIDA',\n",
       " 828: 'MANAGEMENT',\n",
       " 829: 'CINCINNATI',\n",
       " 830: 'OH',\n",
       " 831: 'PHONE',\n",
       " 832: 'TH_AR_LTR',\n",
       " 833: 'SERVICE',\n",
       " 834: 'NUMBER',\n",
       " 835: 'JASON',\n",
       " 836: 'SANTANA',\n",
       " 837: 'Sincerely',\n",
       " 838: 'PLANTATION',\n",
       " 839: 'Allstate',\n",
       " 840: 'Youre',\n",
       " 841: 'good',\n",
       " 842: 'hands',\n",
       " 843: 'Florida',\n",
       " 844: 'PIP',\n",
       " 845: 'Central',\n",
       " 846: 'CLINTON',\n",
       " 847: 'IA',\n",
       " 848: 'February',\n",
       " 849: 'INSURED',\n",
       " 850: 'LOSS',\n",
       " 851: 'FAX',\n",
       " 852: 'OFFICE',\n",
       " 853: 'HOURS',\n",
       " 854: 'VYPHAPHONE',\n",
       " 855: 'INTHALANGSY',\n",
       " 856: 'EXT',\n",
       " 857: \"'re\",\n",
       " 858: 'Jacksonville',\n",
       " 859: 'DALLAS',\n",
       " 860: 'TX',\n",
       " 861: 'Some',\n",
       " 862: 'specifics',\n",
       " 863: 'request',\n",
       " 864: 'sincerely',\n",
       " 865: 'CAITLEN',\n",
       " 866: 'CARROLL',\n",
       " 867: 'Ext',\n",
       " 868: 'date',\n",
       " 869: 'check',\n",
       " 870: 'type',\n",
       " 871: 'claim',\n",
       " 872: 'filing',\n",
       " 873: 'name',\n",
       " 874: 'Cellular',\n",
       " 875: 'MedSupport',\n",
       " 876: 'policy',\n",
       " 877: 'policies',\n",
       " 878: 'Female',\n",
       " 879: 'Domestic',\n",
       " 880: 'Partner',\n",
       " 881: 'information',\n",
       " 882: 'Condition',\n",
       " 883: 'am',\n",
       " 884: 'pm',\n",
       " 885: 'Confinement',\n",
       " 886: 'Dates',\n",
       " 887: 'Teri',\n",
       " 888: 'Willochell',\n",
       " 889: 'phycian',\n",
       " 890: 'considerations',\n",
       " 891: 'each',\n",
       " 892: 'such',\n",
       " 893: 'violation',\n",
       " 894: 'number',\n",
       " 895: 'indicated',\n",
       " 896: 'above',\n",
       " 897: 'My',\n",
       " 898: 'Member',\n",
       " 899: 'Relationship',\n",
       " 900: 'person',\n",
       " 901: 'granting',\n",
       " 902: 'authority',\n",
       " 903: 'birth',\n",
       " 904: 'Including',\n",
       " 905: 'L',\n",
       " 906: 'lesser',\n",
       " 907: 'If',\n",
       " 908: 'yes',\n",
       " 909: 'please',\n",
       " 910: 'provide',\n",
       " 911: 'tha',\n",
       " 912: 'diagnosis',\n",
       " 913: 'Treatment',\n",
       " 914: 'advise',\n",
       " 915: 'stop',\n",
       " 916: 'working',\n",
       " 917: 'what',\n",
       " 918: 'Willochel',\n",
       " 919: 'Specially',\n",
       " 920: 'internal',\n",
       " 921: 'medicine',\n",
       " 922: 'MD',\n",
       " 923: 'MedExpress',\n",
       " 924: 'Urgent',\n",
       " 925: 'Care',\n",
       " 926: 'Route',\n",
       " 927: 'HIPPA',\n",
       " 928: 'Socia',\n",
       " 929: 'Location',\n",
       " 930: 'Norwin',\n",
       " 931: 'Huntingdon',\n",
       " 932: 'Holder',\n",
       " 933: 'Sex',\n",
       " 934: 'COMP',\n",
       " 935: 'Clinical',\n",
       " 936: 'Report',\n",
       " 937: 'BP',\n",
       " 938: 'mmHg',\n",
       " 939: 'PULSE',\n",
       " 940: 'bpm',\n",
       " 941: 'RESP',\n",
       " 942: 'TEMP',\n",
       " 943: 'WEIGTH',\n",
       " 944: 'ft',\n",
       " 945: 'BMI',\n",
       " 946: 'LMP',\n",
       " 947: 'PMP',\n",
       " 948: 'SAT',\n",
       " 949: 'CONTUSION',\n",
       " 950: 'Meds',\n",
       " 951: 'ACTIVE',\n",
       " 952: 'acetaminophen',\n",
       " 953: 'albuterol',\n",
       " 954: 'bulk',\n",
       " 955: 'Dilantin',\n",
       " 956: 'gabapentin',\n",
       " 957: 'Humalog',\n",
       " 958: 'Lamictal',\n",
       " 959: 'Lyrica',\n",
       " 960: 'Neurontin',\n",
       " 961: 'valacyclovir',\n",
       " 962: 'Procedures',\n",
       " 963: 'FOOT',\n",
       " 964: 'MIN',\n",
       " 965: 'THREE',\n",
       " 966: 'ESTAB',\n",
       " 967: 'URGENT',\n",
       " 968: 'CARE',\n",
       " 969: 'Pocket',\n",
       " 970: 'Met',\n",
       " 971: 'Estimated',\n",
       " 972: 'charges',\n",
       " 973: 'MSO',\n",
       " 974: 'LLC',\n",
       " 975: 'Med',\n",
       " 976: 'Express',\n",
       " 977: 'UC',\n",
       " 978: 'HUNINGTON',\n",
       " 979: 'Merchant',\n",
       " 980: 'Transaction',\n",
       " 981: 'PURCHASE',\n",
       " 982: 'Approval',\n",
       " 983: 'code',\n",
       " 984: 'Record',\n",
       " 985: 'Visa',\n",
       " 986: 'Trace',\n",
       " 987: 'reference',\n",
       " 988: 'Cardholder',\n",
       " 989: 'identifier',\n",
       " 990: 'Application',\n",
       " 991: 'label',\n",
       " 992: 'TVR',\n",
       " 993: 'AID',\n",
       " 994: 'Subtotal',\n",
       " 995: 'Sales',\n",
       " 996: 'customer',\n",
       " 997: 'copy',\n",
       " 998: 'send',\n",
       " 999: 'payments',\n",
       " ...}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_word_tokens=len(sorted(list(word2int)))\n",
    "num_char_tokens=len(sorted(list(char2int)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vecotrize hierarichal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_char_input_data, decoder_word_input_data, decoder_word_target_data = vectorize_hier_data(input_texts, \n",
    "                                                                                                target_texts, \n",
    "                                                                                                max_words_seq_len, \n",
    "                                                                                                max_chars_seq_len, \n",
    "                                                                                                num_char_tokens, \n",
    "                                                                                                num_word_tokens, \n",
    "                                                                                                word2int, \n",
    "                                                                                                char2int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load char encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/keras/engine/saving.py:269: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "encoder_char_model = load_model(encoder_char_model_file.format(max_sent_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Hierarichal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 91)     8281        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) [(None, None, 512),  712704      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 512)          0           bidirectional_1[0][1]            \n",
      "                                                                 bidirectional_1[0][3]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 512)          0           bidirectional_1[0][2]            \n",
      "                                                                 bidirectional_1[0][4]            \n",
      "==================================================================================================\n",
      "Total params: 720,985\n",
      "Trainable params: 712,704\n",
      "Non-trainable params: 8,281\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_char_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder-decoder  model:\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 15, 20)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 20)           0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 20)           0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 20)           0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 20)           0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 20)           0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 20)           0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, 20)           0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_8 (Lambda)               (None, 20)           0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_9 (Lambda)               (None, 20)           0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_10 (Lambda)              (None, 20)           0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_11 (Lambda)              (None, 20)           0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_12 (Lambda)              (None, 20)           0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_13 (Lambda)              (None, 20)           0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_14 (Lambda)              (None, 20)           0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_15 (Lambda)              (None, 20)           0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "model_2 (Model)                 multiple             720985      lambda_1[0][0]                   \n",
      "                                                                 lambda_2[0][0]                   \n",
      "                                                                 lambda_3[0][0]                   \n",
      "                                                                 lambda_4[0][0]                   \n",
      "                                                                 lambda_5[0][0]                   \n",
      "                                                                 lambda_6[0][0]                   \n",
      "                                                                 lambda_7[0][0]                   \n",
      "                                                                 lambda_8[0][0]                   \n",
      "                                                                 lambda_9[0][0]                   \n",
      "                                                                 lambda_10[0][0]                  \n",
      "                                                                 lambda_11[0][0]                  \n",
      "                                                                 lambda_12[0][0]                  \n",
      "                                                                 lambda_13[0][0]                  \n",
      "                                                                 lambda_14[0][0]                  \n",
      "                                                                 lambda_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 1024)         0           model_2[1][1]                    \n",
      "                                                                 model_2[1][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 1024)         0           model_2[2][1]                    \n",
      "                                                                 model_2[2][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 1024)         0           model_2[3][1]                    \n",
      "                                                                 model_2[3][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 1024)         0           model_2[4][1]                    \n",
      "                                                                 model_2[4][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 1024)         0           model_2[5][1]                    \n",
      "                                                                 model_2[5][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 1024)         0           model_2[6][1]                    \n",
      "                                                                 model_2[6][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 1024)         0           model_2[7][1]                    \n",
      "                                                                 model_2[7][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 1024)         0           model_2[8][1]                    \n",
      "                                                                 model_2[8][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 1024)         0           model_2[9][1]                    \n",
      "                                                                 model_2[9][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 1024)         0           model_2[10][1]                   \n",
      "                                                                 model_2[10][2]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)    (None, 1024)         0           model_2[11][1]                   \n",
      "                                                                 model_2[11][2]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)    (None, 1024)         0           model_2[12][1]                   \n",
      "                                                                 model_2[12][2]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)    (None, 1024)         0           model_2[13][1]                   \n",
      "                                                                 model_2[13][2]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_17 (Concatenate)    (None, 1024)         0           model_2[14][1]                   \n",
      "                                                                 model_2[14][2]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_18 (Concatenate)    (None, 1024)         0           model_2[15][1]                   \n",
      "                                                                 model_2[15][2]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 1, 1024)      0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 1, 1024)      0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 1, 1024)      0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reshape_4 (Reshape)             (None, 1, 1024)      0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reshape_5 (Reshape)             (None, 1, 1024)      0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reshape_6 (Reshape)             (None, 1, 1024)      0           concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reshape_7 (Reshape)             (None, 1, 1024)      0           concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "reshape_8 (Reshape)             (None, 1, 1024)      0           concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "reshape_9 (Reshape)             (None, 1, 1024)      0           concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "reshape_10 (Reshape)            (None, 1, 1024)      0           concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "reshape_11 (Reshape)            (None, 1, 1024)      0           concatenate_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "reshape_12 (Reshape)            (None, 1, 1024)      0           concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "reshape_13 (Reshape)            (None, 1, 1024)      0           concatenate_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "reshape_14 (Reshape)            (None, 1, 1024)      0           concatenate_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "reshape_15 (Reshape)            (None, 1, 1024)      0           concatenate_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_19 (Concatenate)    (None, 15, 1024)     0           reshape_1[0][0]                  \n",
      "                                                                 reshape_2[0][0]                  \n",
      "                                                                 reshape_3[0][0]                  \n",
      "                                                                 reshape_4[0][0]                  \n",
      "                                                                 reshape_5[0][0]                  \n",
      "                                                                 reshape_6[0][0]                  \n",
      "                                                                 reshape_7[0][0]                  \n",
      "                                                                 reshape_8[0][0]                  \n",
      "                                                                 reshape_9[0][0]                  \n",
      "                                                                 reshape_10[0][0]                 \n",
      "                                                                 reshape_11[0][0]                 \n",
      "                                                                 reshape_12[0][0]                 \n",
      "                                                                 reshape_13[0][0]                 \n",
      "                                                                 reshape_14[0][0]                 \n",
      "                                                                 reshape_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            (None, 15)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) [(None, 15, 512), (N 2623488     concatenate_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 15, 1024)     1031168     input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_20 (Concatenate)    (None, 512)          0           bidirectional_2[0][1]            \n",
      "                                                                 bidirectional_2[0][3]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_21 (Concatenate)    (None, 512)          0           bidirectional_2[0][2]            \n",
      "                                                                 bidirectional_2[0][4]            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   [(None, 15, 512), (N 3147776     embedding_3[0][0]                \n",
      "                                                                 concatenate_20[0][0]             \n",
      "                                                                 concatenate_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dot_3 (Dot)                     (None, 15, 15)       0           lstm_4[0][0]                     \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 15, 15)       0           dot_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dot_4 (Dot)                     (None, 15, 512)      0           activation_1[0][0]               \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_22 (Concatenate)    (None, 15, 1024)     0           dot_4[0][0]                      \n",
      "                                                                 lstm_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 15, 1007)     1032175     concatenate_22[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 8,555,592\n",
      "Trainable params: 8,547,311\n",
      "Non-trainable params: 8,281\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:70: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=[<tf.Tenso...)`\n"
     ]
    }
   ],
   "source": [
    "model, encoder_model, decoder_model = build_hier_model(encoder_char_model=encoder_char_model, \n",
    "                              max_words_seq_len=max_words_seq_len,\n",
    "                              max_char_seq_len=max_chars_seq_len,\n",
    "                              num_word_tokens=num_word_tokens,\n",
    "                              num_char_tokens=num_char_tokens, \n",
    "                              latent_dim=latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 15, 20)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 20)           0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 20)           0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 20)           0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 20)           0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 20)           0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 20)           0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, 20)           0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_8 (Lambda)               (None, 20)           0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_9 (Lambda)               (None, 20)           0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_10 (Lambda)              (None, 20)           0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_11 (Lambda)              (None, 20)           0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_12 (Lambda)              (None, 20)           0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_13 (Lambda)              (None, 20)           0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_14 (Lambda)              (None, 20)           0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_15 (Lambda)              (None, 20)           0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "model_2 (Model)                 multiple             720985      lambda_1[0][0]                   \n",
      "                                                                 lambda_2[0][0]                   \n",
      "                                                                 lambda_3[0][0]                   \n",
      "                                                                 lambda_4[0][0]                   \n",
      "                                                                 lambda_5[0][0]                   \n",
      "                                                                 lambda_6[0][0]                   \n",
      "                                                                 lambda_7[0][0]                   \n",
      "                                                                 lambda_8[0][0]                   \n",
      "                                                                 lambda_9[0][0]                   \n",
      "                                                                 lambda_10[0][0]                  \n",
      "                                                                 lambda_11[0][0]                  \n",
      "                                                                 lambda_12[0][0]                  \n",
      "                                                                 lambda_13[0][0]                  \n",
      "                                                                 lambda_14[0][0]                  \n",
      "                                                                 lambda_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 1024)         0           model_2[1][1]                    \n",
      "                                                                 model_2[1][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 1024)         0           model_2[2][1]                    \n",
      "                                                                 model_2[2][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 1024)         0           model_2[3][1]                    \n",
      "                                                                 model_2[3][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 1024)         0           model_2[4][1]                    \n",
      "                                                                 model_2[4][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 1024)         0           model_2[5][1]                    \n",
      "                                                                 model_2[5][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 1024)         0           model_2[6][1]                    \n",
      "                                                                 model_2[6][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 1024)         0           model_2[7][1]                    \n",
      "                                                                 model_2[7][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 1024)         0           model_2[8][1]                    \n",
      "                                                                 model_2[8][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 1024)         0           model_2[9][1]                    \n",
      "                                                                 model_2[9][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 1024)         0           model_2[10][1]                   \n",
      "                                                                 model_2[10][2]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)    (None, 1024)         0           model_2[11][1]                   \n",
      "                                                                 model_2[11][2]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)    (None, 1024)         0           model_2[12][1]                   \n",
      "                                                                 model_2[12][2]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)    (None, 1024)         0           model_2[13][1]                   \n",
      "                                                                 model_2[13][2]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_17 (Concatenate)    (None, 1024)         0           model_2[14][1]                   \n",
      "                                                                 model_2[14][2]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_18 (Concatenate)    (None, 1024)         0           model_2[15][1]                   \n",
      "                                                                 model_2[15][2]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 1, 1024)      0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 1, 1024)      0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 1, 1024)      0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reshape_4 (Reshape)             (None, 1, 1024)      0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reshape_5 (Reshape)             (None, 1, 1024)      0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reshape_6 (Reshape)             (None, 1, 1024)      0           concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reshape_7 (Reshape)             (None, 1, 1024)      0           concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "reshape_8 (Reshape)             (None, 1, 1024)      0           concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "reshape_9 (Reshape)             (None, 1, 1024)      0           concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "reshape_10 (Reshape)            (None, 1, 1024)      0           concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "reshape_11 (Reshape)            (None, 1, 1024)      0           concatenate_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "reshape_12 (Reshape)            (None, 1, 1024)      0           concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "reshape_13 (Reshape)            (None, 1, 1024)      0           concatenate_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "reshape_14 (Reshape)            (None, 1, 1024)      0           concatenate_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "reshape_15 (Reshape)            (None, 1, 1024)      0           concatenate_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_19 (Concatenate)    (None, 15, 1024)     0           reshape_1[0][0]                  \n",
      "                                                                 reshape_2[0][0]                  \n",
      "                                                                 reshape_3[0][0]                  \n",
      "                                                                 reshape_4[0][0]                  \n",
      "                                                                 reshape_5[0][0]                  \n",
      "                                                                 reshape_6[0][0]                  \n",
      "                                                                 reshape_7[0][0]                  \n",
      "                                                                 reshape_8[0][0]                  \n",
      "                                                                 reshape_9[0][0]                  \n",
      "                                                                 reshape_10[0][0]                 \n",
      "                                                                 reshape_11[0][0]                 \n",
      "                                                                 reshape_12[0][0]                 \n",
      "                                                                 reshape_13[0][0]                 \n",
      "                                                                 reshape_14[0][0]                 \n",
      "                                                                 reshape_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) [(None, 15, 512), (N 2623488     concatenate_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_20 (Concatenate)    (None, 512)          0           bidirectional_2[0][1]            \n",
      "                                                                 bidirectional_2[0][3]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_21 (Concatenate)    (None, 512)          0           bidirectional_2[0][2]            \n",
      "                                                                 bidirectional_2[0][4]            \n",
      "==================================================================================================\n",
      "Total params: 3,344,473\n",
      "Trainable params: 3,336,192\n",
      "Non-trainable params: 8,281\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 15)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 15, 1024)     1031168     input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_9 (InputLayer)            (None, 512)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_10 (InputLayer)           (None, 512)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   [(None, 15, 512), (N 3147776     embedding_3[0][0]                \n",
      "                                                                 input_9[0][0]                    \n",
      "                                                                 input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            (None, 15, 512)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dot_3 (Dot)                     (None, 15, 15)       0           lstm_4[1][0]                     \n",
      "                                                                 input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 15, 15)       0           dot_3[1][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dot_4 (Dot)                     (None, 15, 512)      0           activation_1[1][0]               \n",
      "                                                                 input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_22 (Concatenate)    (None, 15, 1024)     0           dot_4[1][0]                      \n",
      "                                                                 lstm_4[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 15, 1007)     1032175     concatenate_22[1][0]             \n",
      "==================================================================================================\n",
      "Total params: 5,211,119\n",
      "Trainable params: 5,211,119\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Hierarichal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3200 samples, validate on 800 samples\n",
      "Epoch 1/10\n",
      "3200/3200 [==============================] - 78s 25ms/step - loss: 2.4927 - categorical_accuracy: 0.4382 - val_loss: 2.1828 - val_categorical_accuracy: 0.4782\n",
      "\n",
      "Epoch 00001: val_categorical_accuracy improved from -inf to 0.47820, saving model to best_hier_model-15-20.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/keras/engine/network.py:872: UserWarning: Layer lstm_4 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_20/concat:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'concatenate_21/concat:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n",
      "3200/3200 [==============================] - 44s 14ms/step - loss: 0.6844 - categorical_accuracy: 0.7060 - val_loss: 1.3884 - val_categorical_accuracy: 0.5663\n",
      "\n",
      "Epoch 00002: val_categorical_accuracy improved from 0.47820 to 0.56628, saving model to best_hier_model-15-20.hdf5\n",
      "Epoch 3/10\n",
      "3200/3200 [==============================] - 45s 14ms/step - loss: 0.3560 - categorical_accuracy: 0.7568 - val_loss: 1.0802 - val_categorical_accuracy: 0.6481\n",
      "\n",
      "Epoch 00003: val_categorical_accuracy improved from 0.56628 to 0.64807, saving model to best_hier_model-15-20.hdf5\n",
      "Epoch 4/10\n",
      "3200/3200 [==============================] - 44s 14ms/step - loss: 0.2349 - categorical_accuracy: 0.7847 - val_loss: 0.9327 - val_categorical_accuracy: 0.6540\n",
      "\n",
      "Epoch 00004: val_categorical_accuracy improved from 0.64807 to 0.65403, saving model to best_hier_model-15-20.hdf5\n",
      "Epoch 5/10\n",
      "3200/3200 [==============================] - 46s 14ms/step - loss: 0.1894 - categorical_accuracy: 0.7920 - val_loss: 0.8922 - val_categorical_accuracy: 0.6972\n",
      "\n",
      "Epoch 00005: val_categorical_accuracy improved from 0.65403 to 0.69723, saving model to best_hier_model-15-20.hdf5\n",
      "Epoch 6/10\n",
      "3200/3200 [==============================] - 44s 14ms/step - loss: 0.1771 - categorical_accuracy: 0.8086 - val_loss: 0.8254 - val_categorical_accuracy: 0.6911\n",
      "\n",
      "Epoch 00006: val_categorical_accuracy did not improve from 0.69723\n",
      "Epoch 7/10\n",
      "3200/3200 [==============================] - 44s 14ms/step - loss: 0.1451 - categorical_accuracy: 0.8056 - val_loss: 0.8478 - val_categorical_accuracy: 0.6752\n",
      "\n",
      "Epoch 00007: val_categorical_accuracy did not improve from 0.69723\n",
      "Epoch 8/10\n",
      "3200/3200 [==============================] - 44s 14ms/step - loss: 0.1164 - categorical_accuracy: 0.8046 - val_loss: 0.7682 - val_categorical_accuracy: 0.7034\n",
      "\n",
      "Epoch 00008: val_categorical_accuracy improved from 0.69723 to 0.70341, saving model to best_hier_model-15-20.hdf5\n",
      "Epoch 9/10\n",
      "3200/3200 [==============================] - 44s 14ms/step - loss: 0.1035 - categorical_accuracy: 0.8177 - val_loss: 0.7723 - val_categorical_accuracy: 0.7183\n",
      "\n",
      "Epoch 00009: val_categorical_accuracy improved from 0.70341 to 0.71828, saving model to best_hier_model-15-20.hdf5\n",
      "Epoch 10/10\n",
      "3200/3200 [==============================] - 45s 14ms/step - loss: 0.1014 - categorical_accuracy: 0.8174 - val_loss: 0.8728 - val_categorical_accuracy: 0.6879\n",
      "\n",
      "Epoch 00010: val_categorical_accuracy did not improve from 0.71828\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2546ba63c8>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 10\n",
    "lr = 0.01\n",
    "\n",
    "model.compile(optimizer=optimizers.Adam(lr=lr), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "#filepath=\"weights-improvement-{epoch:02d}-{val_categorical_accuracy:.2f}.hdf5\"\n",
    "filepath=\"best_hier_model-{}-{}.hdf5\".format(max_words_seq_len,max_chars_seq_len) # Save only the best model for inference step, as saving the epoch and metric might confuse the inference function which model to use\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_categorical_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "tbCallBack = TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "callbacks_list = [checkpoint, tbCallBack]\n",
    "#callbacks_list = [checkpoint, tbCallBack, lrate]\n",
    "model.fit([encoder_char_input_data, decoder_word_input_data], decoder_word_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=callbacks_list,\n",
    "          validation_split=0.2,\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\t Medical Provider Roles UNK Treating \\n UNK UNK UNK UNK UNK UNK UNK UNK '"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_gt_sequence(decoder_word_input_data[idx:idx+1], int2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Medicl', 'Providr', 'Royles', ':', 'Teating']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(input_texts[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\tMedical Provider Roles: Treating\\n'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_texts[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\t', 'Medical', 'Provider', 'Roles', ':', 'Treating', '\\n']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_lst =  word_tokenize(target_texts[idx])\n",
    "words_lst.insert(0, '\\t')\n",
    "words_lst.append('\\n')\n",
    "words_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Medical Provider Roles UNK Treating \\n UNK UNK UNK UNK UNK UNK UNK UNK UNK '"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_gt_sequence(np.argmax(decoder_word_target_data[idx:idx+1], axis=-1), int2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Medical Provider Roles UNK UNK \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n '"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = model.predict([encoder_char_input_data[idx:idx+1], decoder_word_input_data[idx-1:idx]])\n",
    "y.shape\n",
    "#sampled_token_index = np.argmax(y[0, -1, :])\n",
    "d = np.argmax(y, axis=-1)\n",
    "d.shape\n",
    "decode_gt_sequence(d, int2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Medical Provider Roles UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n"
     ]
    }
   ],
   "source": [
    "input_seq = encoder_char_input_data[idx:idx+1]\n",
    "decoded_sentence, _ = decode_sequence(input_seq, encoder_model, decoder_model, max_words_seq_len, num_word_tokens, int2word)\n",
    "print(decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: lCaim Tpyze: VBAicciwden -A ccidental Injry\n",
      "GT sentence: Claim Type: VB Accident - Accidental Injury\n",
      "Decoded sentence: Claim Type UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Polcyholder/Owner Information\n",
      "GT sentence: Policyholder/Owner Information\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: First Name:\n",
      "GT sentence: First Name:\n",
      "Decoded sentence: First Name UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Middle Nmae/Initial:\n",
      "GT sentence: Middle Name/Initial:\n",
      "Decoded sentence: Middle UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Las tName:\n",
      "GT sentence: Last Name:\n",
      "Decoded sentence: Last Name UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Sjocmial Seurity Number:\n",
      "GT sentence: Social Security Number:\n",
      "Decoded sentence: Individual Family UNK UNK  \n",
      "-\n",
      "Input sentence: Brth Datje:\n",
      "GT sentence: Birth Date:\n",
      "Decoded sentence: Birth Date UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Gender\n",
      "GT sentence: Gender:\n",
      "Decoded sentence: Gender UNK  \n",
      "-\n",
      "Input sentence: Lagnuage Preference\n",
      "\n",
      "GT sentence: Language Preference:\n",
      "Decoded sentence: Surgery Information  \n",
      "-\n",
      "Input sentence: Address Line 1:\n",
      "GT sentence: Address Line 1:\n",
      "Decoded sentence: Address Line UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: City:\n",
      "GT sentence: City:\n",
      "Decoded sentence: City UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: State/Provicne:\n",
      "GT sentence: State/Province:\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: PostalCode:\n",
      "GT sentence: Postal Code:\n",
      "Decoded sentence: Postal Code UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Country:\n",
      "GT sentence: Country:\n",
      "Decoded sentence: Country UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Be tPhone Nuber to be Rneached During the Day\n",
      "GT sentence: Best Phone Number to be Reached During the Day:\n",
      "Decoded sentence: Best Phone Number to the patient UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Email Adhdress:\n",
      "GT sentence: Email Address:\n",
      "Decoded sentence: Email Address UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Pge 1 of 1\n",
      "GT sentence: Page 1 of 1\n",
      "Decoded sentence: Page UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: RADIOLOGYg\n",
      "GT sentence: RADIOLOGY\n",
      "Decoded sentence: PLEASE VISIT  \n",
      "-\n",
      "Input sentence: REPORT\n",
      "GT sentence: REPORT\n",
      "Decoded sentence: STATEMENT  \n",
      "-\n",
      "Input sentence: www.rays.ne\n",
      "GT sentence: www.rays.net\n",
      "Decoded sentence:  \n",
      "-\n",
      "Input sentence: Patient MRNAccesion N.o Ref. Physician\n",
      "GT sentence: Patient MRN Accession No. Ref. Physician\n",
      "Decoded sentence: Patient MRN UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: UNKNOW\n",
      "GT sentence: UNKNOWN\n",
      "Decoded sentence: MARCO HEALTHCARE  \n",
      "-\n",
      "Input sentence: Stud y\n",
      "GT sentence: Study\n",
      "Decoded sentence: Gender UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Stud yDate:\n",
      "GT sentence: Study Date:\n",
      "Decoded sentence: Study Date UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: ospital Codhe: 0226 \n",
      "GT sentence: Hospital Code: 2026\n",
      "Decoded sentence: Hospital Code UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: DOB:\n",
      "GT sentence: DOB:\n",
      "Decoded sentence: DOB UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: 43F.\n",
      "GT sentence: 43F.\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Tecnique: 3 viewsleft wrist\n",
      "GT sentence: Technique: 3 views left wrist\n",
      "Decoded sentence: Technique UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Cormartison: None avaiabie\n",
      "GT sentence: Cormarison: None availabie\n",
      "Decoded sentence: Comparison UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Comparison: Noen available\n",
      "GT sentence: Comparison: None available\n",
      "Decoded sentence: Comparison UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: FINDING:\n",
      "GT sentence: FINDINGS:\n",
      "Decoded sentence: FINDINGS UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: IMPRESSION:\n",
      "GT sentence: IMPRESSION:\n",
      "Decoded sentence: IMPRESSION UNK UNK UNK  \n",
      "-\n",
      "Input sentence: 2. o aucte losseous abnormality iedntifiesd.\n",
      "GT sentence: 2. No acute osseous abnormality identified.\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Daytime Pone:\n",
      "GT sentence: Daytime Phone:\n",
      "Decoded sentence: Daytime Phone UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Claim cEvent Informaton\n",
      "GT sentence: Claim Event Information\n",
      "Decoded sentence: Claim Event Information  \n",
      "-\n",
      "Input sentence: Stopepd Wrkuing?: Yes\n",
      "GT sentence: Stopped Working?: Yes\n",
      "Decoded sentence: Stopped Working UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Dateu LastP hysically at Work:\n",
      "GT sentence: Date Last Physically at Work:\n",
      "Decoded sentence: Date Last Physically at Work UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Hours Wkorked on Lstt Day :\n",
      "\n",
      "GT sentence: Hours Worked on Last Day: 8\n",
      "Decoded sentence: Hours Worked on Last Day UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Huors Scheduled to Work on Last Day:\n",
      "GT sentence: Hours Scheduled to Work on Last Day:\n",
      "Decoded sentence: Hours Scheduled to Work on Last Day UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Dat eFirst Missde Wrk:\n",
      "GT sentence: Date First Missed Work:\n",
      "Decoded sentence: Date First Missed Work UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Rfeturnde to Work?: No\n",
      "GT sentence: Returned to Work?: No\n",
      "Decoded sentence: Returned to Work UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Accipdetn Wokr Reltedh: Yses\n",
      "GT sentence: Accident Work Related: Yes\n",
      "Decoded sentence: Accident Work Related UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Time of Accident:\n",
      "GT sentence: Time of Accident:\n",
      "Decoded sentence: Time of Accident UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Accident Dtae:\n",
      "GT sentence: Accident Date:\n",
      "Decoded sentence: Accident Date UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Diagonsis Cde: Arthiscopi csrugery lefet wrsit.\n",
      "GT sentence: Diagnosis Code: Arthiscopic surgery left wrist.\n",
      "Decoded sentence: Diagnosis Code UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Surgeyr Infromantin\n",
      "GT sentence: Surgery Information\n",
      "Decoded sentence: Surgery Information  \n",
      "-\n",
      "Input sentence: Is Surgery Required: Yes\n",
      "GT sentence: Is Surgery Required: Yes\n",
      "Decoded sentence: Is Surgery Required UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Surger Date:\n",
      "GT sentence: Surgery Date:\n",
      "Decoded sentence: Surgery Date UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Inptient/Outpatient Iandicator: Outpatient\n",
      "GT sentence: Inpatient/Outpatient Indicator: Outpatient\n",
      "Decoded sentence: Medical Provider Specialty UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Medical Provider Information - hPysicaan\n",
      "GT sentence: Medical Provider Information - Physician\n",
      "Decoded sentence: Medical Provider Information UNK UNK  \n",
      "-\n",
      "Input sentence: Medicl Providr Royles: Teating\n",
      "GT sentence: Medical Provider Roles: Treating\n",
      "Decoded sentence: Medical Provider Roles UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Provider First Name: Patrick\n",
      "GT sentence: Provider First Name: Patrick\n",
      "Decoded sentence: Provider First Name UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Provider vLaslt Namde Emersno\n",
      "GT sentence: Provider Last Name: Emerson\n",
      "Decoded sentence: Provider Last Name UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Adders Lin 1 :\n",
      "GT sentence: Address Line 1 :\n",
      "Decoded sentence: Address Line UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Ci\n",
      "GT sentence: City:\n",
      "Decoded sentence: City UNK  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: State/Provincwe:\n",
      "GT sentence: State/Province:\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Posgtal Code\n",
      "\n",
      "GT sentence: Postal Code:\n",
      "Decoded sentence: Postal Code UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Cuountry:\n",
      "GT sentence: Country:\n",
      "Decoded sentence: Country UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Businss Teblephone:\n",
      "GT sentence: Business Telephone:\n",
      "Decoded sentence: Business Telephone UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Busness Fax\n",
      "GT sentence: Business Fax\n",
      "Decoded sentence: Business Fax  \n",
      "-\n",
      "Input sentence: Date xf First Visit:\n",
      "GT sentence: Date of First Visit:\n",
      "Decoded sentence: Date of First Visit UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Date of Next Visit:\n",
      "GT sentence: Date of Next Visit:\n",
      "Decoded sentence: Date of Next Visit UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: edrila Provider Information - Hospiktliztaion\n",
      "GT sentence: Medical Provider Information - Hospitalization\n",
      "Decoded sentence: Medical Provider Information UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Hostitahln Name:\n",
      "GT sentence: Hospital Name:\n",
      "Decoded sentence: Hospital Name UNK  \n",
      "-\n",
      "Input sentence: Addressw Litne 1:\n",
      "GT sentence: Address Line 1:\n",
      "Decoded sentence: Address Line UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Citwy:\n",
      "GT sentence: City:\n",
      "Decoded sentence: City UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: State/rPgovicne:\n",
      "GT sentence: State/Province:\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Postpl Codeh:\n",
      "GT sentence: Postal Code:\n",
      "Decoded sentence: Postal Code UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Country:\n",
      "GT sentence: Country:\n",
      "Decoded sentence: Country UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Date of Visitc/Admission:v\n",
      "GT sentence: Date of Visit/Admission:\n",
      "Decoded sentence: Date of UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Date of Dischareg:\n",
      "GT sentence: Date of Discharge:\n",
      "Decoded sentence: Date of birth UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: rowcedur:e Left wrist arthicopic surgery\n",
      "GT sentence: Procedure: Left wrist arthiscopic surgery\n",
      "Decoded sentence: Procedure UNK feel wrist arthiscopic surgery  \n",
      "-\n",
      "Input sentence: Emplyoment Infoarmation\n",
      "GT sentence: Employment Information\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: Employer zName:\n",
      "GT sentence: Employer Name:\n",
      "Decoded sentence: Employer Name UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Policy Numbe:r\n",
      "GT sentence: Policy Number:\n",
      "Decoded sentence: Employer Name UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Elecronc Submikssio\n",
      "GT sentence: Electronic Submission\n",
      "Decoded sentence: Electronic Submission  \n",
      "-\n",
      "Input sentence: Claim Even tIdnetiifer:\n",
      "GT sentence: Claim Event Identifier:\n",
      "Decoded sentence: Claim Event Identifier UNK  \n",
      "-\n",
      "Input sentence: Suibmission Date:\n",
      "GT sentence: Submission Date:\n",
      "Decoded sentence: Submission Date UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Electronically Signde Indicator: zYes\n",
      "GT sentence: Electronically Signed Indicator: Yes\n",
      "Decoded sentence: Electronically Signed Indicator UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Fraud Statements evieweed anwd Elcctronically\n",
      "GT sentence: Fraud Statements Reviewed and Electronically\n",
      "Decoded sentence: Fraud Statements Reviewed and Electronically  \n",
      "-\n",
      "Input sentence: Signed Date\n",
      "GT sentence: Signed Date:\n",
      "Decoded sentence: Signed Date UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: unpum\n",
      "GT sentence: unum\n",
      "Decoded sentence: unum  \n",
      "-\n",
      "Input sentence: The Benesfits Center\n",
      "GT sentence: The Benefits Center\n",
      "Decoded sentence: The Benefits Center  \n",
      "-\n",
      "Input sentence: (Not for FMLA Request)s\n",
      "GT sentence: (Not for FMLA Requests)\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Electronially Signed 02/28/2018\n",
      "GT sentence: Electronically Signed 02/28/2018\n",
      "Decoded sentence: Electronically Signed  \n",
      "-\n",
      "Input sentence: Isnureds’ Signautre Date Sind\n",
      "GT sentence: Insured’s Signature Date Signed\n",
      "Decoded sentence: Insured ’ s Signature  \n",
      "-\n",
      "Input sentence: PrinkteudN ame Social Secrityq Numbe\n",
      "\n",
      "GT sentence: Printed Name Social Security Number\n",
      "Decoded sentence: Printed Name  \n",
      "-\n",
      "Input sentence: C-111 (11/14)\n",
      "GT sentence: CL-1116 (11/14)\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Unum\n",
      "GT sentence: Unum\n",
      "Decoded sentence: MRN  \n",
      "-\n",
      "Input sentence: Confirmation pof Coevrage\n",
      "GT sentence: Confirmation of Coverage\n",
      "Decoded sentence: Confirmation of Coverage  \n",
      "-\n",
      "Input sentence: Eploe:\n",
      "GT sentence: Employer:\n",
      "Decoded sentence: Employer UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Goup Poliyc :#\n",
      "GT sentence: Group Policy #:\n",
      "Decoded sentence: Group Policy UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Custoemer  Poilc y#:\n",
      "GT sentence: Customer  Policy #:\n",
      "Decoded sentence: Customer Policy UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: EE Name:\n",
      "GT sentence: EE Name:\n",
      "Decoded sentence: EE Name UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Inusrce dCoverage Tyep Coverage Efefctisve ate\n",
      "GT sentence: Insured Coverage Type Coverage Effective Date\n",
      "Decoded sentence: Insured Coverage Type Coverage Effective  \n",
      "-\n",
      "Input sentence: Employeeb Off-Jbo Ancco Januuary 1, 2017\n",
      "GT sentence: Employee Off-Job Acc January 1, 2017\n",
      "Decoded sentence: Employee UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Emplouyee Wellness Benefit January 1, 2s017\n",
      "GT sentence: Employee Wellness Benefit January 1, 2017\n",
      "Decoded sentence: Employee Wellness Benefit January UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: wTeotal Monthly Premium:\n",
      "GT sentence: Total Monthly Premium:\n",
      "Decoded sentence: Total Monthly Premium UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Totalj Empliyee Montly Pyroll Deducion:\n",
      "GT sentence: Total Employee Montly Payroll Deduction:\n",
      "Decoded sentence: Total Employee Weekly Payroll Deduction UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Name:\n",
      "GT sentence: Name:\n",
      "Decoded sentence: Employer Name UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n"
     ]
    }
   ],
   "source": [
    "# Sample output from train data\n",
    "decoded_sentences = []\n",
    "target_texts_ =  []\n",
    "for seq_index in range(100):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "\n",
    "    input_seq = encoder_char_input_data[seq_index: seq_index + 1]\n",
    "    target_seq = np.argmax(decoder_word_target_data[seq_index: seq_index + 1], axis=-1)\n",
    "    #print(target_seq)\n",
    "    \n",
    "    decoded_sentence, _ = decode_sequence(input_seq, encoder_model, decoder_model, max_words_seq_len, num_word_tokens, int2word)\n",
    "    target_text = target_texts[seq_index][1:-1]\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('GT sentence:', target_text)\n",
    "    print('Decoded sentence:', decoded_sentence)   \n",
    "    decoded_sentences.append(decoded_sentence)\n",
    "    target_texts_.append(target_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: text\n",
      "GT sentence: Claim Type: VB Accident - Accidental Injury\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Policyholder/Owner Information\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: First Name:\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Middle Name/Initial:\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Last Name:\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence:  \n",
      "GT sentence: Social Security Number:\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Birth Date:\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Gender:\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Language Preference:\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: Fai\n",
      "GT sentence: Address Line 1:\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: 10\n",
      "GT sentence: City:\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: 7521509\n",
      "GT sentence: State/Province:\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: (FISTDEOO)\n",
      "GT sentence: Postal Code:\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: at\n",
      "GT sentence: Country:\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: 11/3/2017\n",
      "GT sentence: Best Phone Number to be Reached During the Day:\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: 5:23:19\n",
      "GT sentence: Email Address:\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: from\n",
      "GT sentence: Page 1 of 1\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: -9373834004\n",
      "GT sentence: RADIOLOGY\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: Req\n",
      "GT sentence: REPORT\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: IC\n",
      "GT sentence: www.rays.net\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: 2017:1030525109:292E.\n",
      "GT sentence: Patient MRN Accession No. Ref. Physician\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: Page\n",
      "GT sentence: UNKNOWN\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: 4\n",
      "GT sentence: Study\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: of\n",
      "GT sentence: Study Date:\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: 5\n",
      "GT sentence: Hospital Code: 2026\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: (C)\n",
      "GT sentence: DOB:\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: 43F.\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Technique: 3 views left wrist\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Cormarison: None availabie\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence:  \n",
      "GT sentence: Comparison: None available\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: FINDINGS:\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: IMPRESSION:\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: 2. No acute osseous abnormality identified.\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence:  \n",
      "GT sentence: Daytime Phone:\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Claim Event Information\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Stopped Working?: Yes\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Date Last Physically at Work:\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: 11/3/2017\n",
      "GT sentence: Hours Worked on Last Day: 8\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: FRI\n",
      "GT sentence: Hours Scheduled to Work on Last Day:\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: 8:26\n",
      "GT sentence: Date First Missed Work:\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: FAX\n",
      "GT sentence: Returned to Work?: No\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: 2373834004\n",
      "GT sentence: Accident Work Related: Yes\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: Kjooas00s\n",
      "GT sentence: Time of Accident:\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Accident Date:\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Diagnosis Code: Arthiscopic surgery left wrist.\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Surgery Information\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: as3-ursasy3\n",
      "GT sentence: Is Surgery Required: Yes\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: 11:30:11\n",
      "GT sentence: Surgery Date:\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: 11/2/2017\n",
      "GT sentence: Inpatient/Outpatient Indicator: Outpatient\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: vis\n",
      "GT sentence: Medical Provider Information - Physician\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Medical Provider Roles: Treating\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Provider First Name: Patrick\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Provider Last Name: Emerson\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: ®\n",
      "GT sentence: Address Line 1 :\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: ®\n",
      "GT sentence: City:\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: &\n",
      "GT sentence: State/Province:\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: ACCIDENT\n",
      "GT sentence: Postal Code:\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: CLAIM\n",
      "GT sentence: Country:\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: FORM\n",
      "GT sentence: Business Telephone:\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Business Fax\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: uu\n",
      "GT sentence: Date of First Visit:\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: num’\n",
      "GT sentence: Date of Next Visit:\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: Tha\n",
      "GT sentence: Medical Provider Information - Hospitalization\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: Benelits\n",
      "GT sentence: Hospital Name:\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: Canter\n",
      "GT sentence: Address Line 1:\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: City:\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: P.O.\n",
      "GT sentence: State/Province:\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: Bax\n",
      "GT sentence: Postal Code:\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: 100158,\n",
      "GT sentence: Country:\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: Calumbin,\n",
      "GT sentence: Date of Visit/Admission:\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: EC\n",
      "GT sentence: Date of Discharge:\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: 20202-3150\n",
      "GT sentence: Procedure: Left wrist arthiscopic surgery\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Employment Information\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: Tol-frea:\n",
      "GT sentence: Employer Name:\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: 1-800-635-5587\n",
      "GT sentence: Policy Number:\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: Fax:\n",
      "GT sentence: Electronic Submission\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: 1-800-447-2488\n",
      "GT sentence: Claim Event Identifier:\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Submission Date:\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: Gall\n",
      "GT sentence: Electronically Signed Indicator: Yes\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: toll-free\n",
      "GT sentence: Fraud Statements Reviewed and Electronically\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: Monday\n",
      "GT sentence: Signed Date:\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: through\n",
      "GT sentence: unum\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: Friday,\n",
      "GT sentence: The Benefits Center\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: 8\n",
      "GT sentence: (Not for FMLA Requests)\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: a.m.\n",
      "GT sentence: Electronically Signed 02/28/2018\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: lo\n",
      "GT sentence: Insured’s Signature Date Signed\n",
      "Decoded sentence: Procedure Description  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: 8\n",
      "GT sentence: Printed Name Social Security Number\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: p.m,\n",
      "GT sentence: CL-1116 (11/14)\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: Eagtarn\n",
      "GT sentence: Unum\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: Time.\n",
      "GT sentence: Confirmation of Coverage\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Employer:\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Group Policy #:\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Customer  Policy #:\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence:  \n",
      "GT sentence: EE Name:\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Insured Coverage Type Coverage Effective Date\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Employee Off-Job Acc January 1, 2017\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Employee Wellness Benefit January 1, 2017\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence:  \n",
      "GT sentence: Total Monthly Premium:\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Total Employee Montly Payroll Deduction:\n",
      "Decoded sentence: Procedure Description  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Name:\n",
      "Decoded sentence: Procedure Description  \n"
     ]
    }
   ],
   "source": [
    "input_texts = ['text',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'Fai',\n",
    "'10',\n",
    "'7521509',\n",
    "'(FISTDEOO)',\n",
    "'at',\n",
    "'11/3/2017',\n",
    "'5:23:19',\n",
    "'from',\n",
    "'-9373834004',\n",
    "'Req',\n",
    "'IC',\n",
    "'2017:1030525109:292E.',\n",
    "'Page',\n",
    "'4',\n",
    "'of',\n",
    "'5',\n",
    "'(C)',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'11/3/2017',\n",
    "'FRI',\n",
    "'8:26',\n",
    "'FAX',\n",
    "'2373834004',\n",
    "'Kjooas00s',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'as3-ursasy3',\n",
    "'11:30:11',\n",
    "'11/2/2017',\n",
    "'vis',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'®',\n",
    "'®',\n",
    "'&',\n",
    "'ACCIDENT',\n",
    "'CLAIM',\n",
    "'FORM',\n",
    "'',\n",
    "'uu',\n",
    "'num’',\n",
    "'Tha',\n",
    "'Benelits',\n",
    "'Canter',\n",
    "'',\n",
    "'P.O.',\n",
    "'Bax',\n",
    "'100158,',\n",
    "'Calumbin,',\n",
    "'EC',\n",
    "'20202-3150',\n",
    "'',\n",
    "'Tol-frea:',\n",
    "'1-800-635-5587',\n",
    "'Fax:',\n",
    "'1-800-447-2488',\n",
    "'',\n",
    "'Gall',\n",
    "'toll-free',\n",
    "'Monday',\n",
    "'through',\n",
    "'Friday,',\n",
    "'8',\n",
    "'a.m.',\n",
    "'lo',\n",
    "'8',\n",
    "'p.m,',\n",
    "'Eagtarn',\n",
    "'Time.',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'[',\n",
    "'ATTENDING',\n",
    "'PHYSICIAN',\n",
    "'STATEMENT',\n",
    "']',\n",
    "'',\n",
    "'',\n",
    "'IneurexiPolicyt',\n",
    "'alcar',\n",
    "'Hama',\n",
    "'(Lael',\n",
    "'Name,',\n",
    "'Flis!',\n",
    "'Nama,',\n",
    "'MI,',\n",
    "'Suffix)',\n",
    "'Data',\n",
    "'of',\n",
    "'Risth',\n",
    "'{msmidrfyy)',\n",
    "'-',\n",
    "'',\n",
    "'',\n",
    "'Faupi',\n",
    "'Nana',\n",
    "'{Laut',\n",
    "'Hume,',\n",
    "'Flial',\n",
    "'Numa,',\n",
    "'1',\n",
    "'Sut)',\n",
    "'Dats',\n",
    "'al',\n",
    "'Bln',\n",
    "'rAvad)',\n",
    "'Ul',\n",
    "'_',\n",
    "'',\n",
    "'-[ECIpENT',\n",
    "'DETAILS',\n",
    "']',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'a',\n",
    "'thls',\n",
    "'Gundilan',\n",
    "'the',\n",
    "'result',\n",
    "'of',\n",
    "'a',\n",
    "'acddental',\n",
    "'inury?',\n",
    "'ves',\n",
    "'O',\n",
    "'No',\n",
    "'if',\n",
    "'yas,',\n",
    "'dale',\n",
    "'of',\n",
    "'accident',\n",
    "'qre/ddlyy)',\n",
    "'[1',\n",
    "'0]',\n",
    "'[z]e',\n",
    "'[=]',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'Is',\n",
    "'Mig',\n",
    "'condition',\n",
    "'Lhe',\n",
    "'result',\n",
    "'of',\n",
    "'hefer',\n",
    "'employment',\n",
    "'£1',\n",
    "'Yes',\n",
    "'pNo',\n",
    "'[1',\n",
    "'Unknown',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'Plaaze',\n",
    "'verily',\n",
    "'treatment',\n",
    "'for',\n",
    "'the',\n",
    "'accident',\n",
    "'lalad',\n",
    "'above.',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'Dalaw',\n",
    "'of',\n",
    "'Diagnosis',\n",
    "'Diagncsis',\n",
    "'Description',\n",
    "'Prosadure',\n",
    "'Procedure',\n",
    "'Dascription',\n",
    "'',\n",
    "'Branden',\n",
    "'(Including',\n",
    "'|',\n",
    "'Cudo',\n",
    "'(GD)',\n",
    "'ous',\n",
    "'',\n",
    "'Confinement)',\n",
    "'eR',\n",
    "'ap',\n",
    "'HAS',\n",
    "'TTT',\n",
    "'',\n",
    "'BEEF',\n",
    "'eR',\n",
    "'',\n",
    "'wiz]',\n",
    "'.',\n",
    "'S33,5XxA',\n",
    "'Hh',\n",
    "'rioes',\n",
    "'ey',\n",
    "'race',\n",
    "'Word',\n",
    "'',\n",
    "'awqd]',\n",
    "'',\n",
    "'weak',\n",
    "'3',\n",
    "'n',\n",
    "'[aveny',\n",
    "'[d',\n",
    "'',\n",
    "'wifi',\n",
    "'Wl',\n",
    "'',\n",
    "'oa',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'Has',\n",
    "'lhe',\n",
    "'pallet',\n",
    "'bean',\n",
    "'trastad',\n",
    "'for',\n",
    "'tha',\n",
    "'same',\n",
    "'ar',\n",
    "'&',\n",
    "'S(tilar',\n",
    "'candillan',\n",
    "'by',\n",
    "'anolher',\n",
    "'phyalelan',\n",
    "'In',\n",
    "'tha',\n",
    "'past?',\n",
    "'[1',\n",
    "'Yen',\n",
    "'Bho',\n",
    "'',\n",
    "'M',\n",
    "'yor,',\n",
    "'pioona',\n",
    "'provid',\n",
    "'tha',\n",
    "'fares:',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'Diageosis:',\n",
    "'Tramiment',\n",
    "'Daten:',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'id',\n",
    "'ya.1',\n",
    "'#dving',\n",
    "'Lhe',\n",
    "'patient',\n",
    "'to',\n",
    "'clap',\n",
    "'working?',\n",
    "'RECEIVED',\n",
    "'',\n",
    "'It',\n",
    "'yes,',\n",
    "'B8',\n",
    "'of',\n",
    "'what',\n",
    "'cate?',\n",
    "'(mmidkyy)',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'[23]',\n",
    "'[117]',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'[Ih',\n",
    "'cielih',\n",
    "'fa',\n",
    "'rotated',\n",
    "'to',\n",
    "'normal',\n",
    "'prepnency,',\n",
    "'please',\n",
    "'grovida',\n",
    "'tha',\n",
    "'idliawing:',\n",
    "'NOV',\n",
    "'',\n",
    "'Expecigd',\n",
    "'Delivery',\n",
    "'Dale',\n",
    "'(mimicd/yy)',\n",
    "'Aclual',\n",
    "'Delivery',\n",
    "'Dale',\n",
    "'{mmiddlyy',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'Phyeiclan',\n",
    "'informaiton',\n",
    "'HUMAN',\n",
    "'REGOURCITE',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'FRAUD',\n",
    "'NOTICE:',\n",
    "'Any',\n",
    "'person',\n",
    "'wha',\n",
    "'knowingly',\n",
    "'files',\n",
    "'&',\n",
    "'statement',\n",
    "'of',\n",
    "'clalm',\n",
    "'containing',\n",
    "'FALSE',\n",
    "'or',\n",
    "'misleading',\n",
    "'information',\n",
    "'8',\n",
    "'',\n",
    "'subject',\n",
    "'to',\n",
    "'criminal',\n",
    "'and',\n",
    "'elvil',\n",
    "'penallies.',\n",
    "'This',\n",
    "'includes',\n",
    "'Attending',\n",
    "'Physician',\n",
    "'portions',\n",
    "'of',\n",
    "'the',\n",
    "'claim',\n",
    "'farm.',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'CS',\n",
    "'yma',\n",
    "'SEAS',\n",
    "'Ta',\n",
    "'hve',\n",
    "'glan',\n",
    "'=',\n",
    "'',\n",
    "'The',\n",
    "'above',\n",
    "'statements',\n",
    "'ara',\n",
    "'trun',\n",
    "'And',\n",
    "'rompints',\n",
    "'to',\n",
    "'tho',\n",
    "'bot',\n",
    "'of',\n",
    "'my',\n",
    "'knowledge',\n",
    "'and',\n",
    "'bolluf.',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'Physician',\n",
    "'Name',\n",
    "'(Lea!',\n",
    "'Name,',\n",
    "'Firat',\n",
    "'Name,',\n",
    "'MI,',\n",
    "'Suita)',\n",
    "'Plases',\n",
    "'Print',\n",
    "'Co',\n",
    "'FHman',\n",
    "'log',\n",
    "'Mm',\n",
    "'',\n",
    "'/',\n",
    "'‘',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'Medical',\n",
    "'Speclaty',\n",
    "'[Tr',\n",
    "'eactal-',\n",
    "']',\n",
    "'|',\n",
    "'D',\n",
    "'of',\n",
    "'r',\n",
    "'of',\n",
    "'Ch',\n",
    "'2',\n",
    "'',\n",
    "'2',\n",
    "'Le',\n",
    "'',\n",
    "'',\n",
    "'==',\n",
    "'Zoi!',\n",
    "'M',\n",
    "'o',\n",
    "'“Fanart',\n",
    "'',\n",
    "'',\n",
    "'=',\n",
    "'Balfrone',\n",
    "'ie',\n",
    "'2',\n",
    "'Sle',\n",
    "'iu',\n",
    "'',\n",
    "'il',\n",
    "'HY',\n",
    "'BY',\n",
    "'1942',\n",
    "'Fax',\n",
    "'Number',\n",
    "'yz—',\n",
    "'43',\n",
    "'-8',\n",
    "'7775',\n",
    "'Fhyalafans',\n",
    "'Tax',\n",
    "'ID',\n",
    "'Number.',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'Aro',\n",
    "'you',\n",
    "'refateq',\n",
    "'to',\n",
    "'hiv',\n",
    "'pollen?',\n",
    "'0',\n",
    "'Yoe',\n",
    "'LlMo',\n",
    "'|',\n",
    "'yes,',\n",
    "'wal',\n",
    "'iv',\n",
    "'the',\n",
    "'relelianshipT',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'Physlclan',\n",
    "'Slgnature',\n",
    "'Date',\n",
    "'',\n",
    "'CL-1023',\n",
    "'-2717',\n",
    "'=',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'—',]\n",
    "\n",
    "encoder_char_input_data, decoder_word_input_data, decoder_word_target_data = vectorize_hier_data(input_texts, \n",
    "                                                                                                [], \n",
    "                                                                                                max_words_seq_len, \n",
    "                                                                                                max_chars_seq_len, \n",
    "                                                                                                num_char_tokens, \n",
    "                                                                                                num_word_tokens, \n",
    "                                                                                                word2int, \n",
    "                                                                                                char2int)\n",
    "\n",
    "# Sample output from train data\n",
    "decoded_sentences = []\n",
    "target_texts_ =  []\n",
    "for seq_index in range(100):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "\n",
    "    input_seq = encoder_char_input_data[seq_index: seq_index + 1]\n",
    "    target_seq = np.argmax(decoder_word_target_data[seq_index: seq_index + 1], axis=-1)\n",
    "    #print(target_seq)\n",
    "    \n",
    "    decoded_sentence, _ = decode_sequence(input_seq, encoder_model, decoder_model, max_words_seq_len, num_word_tokens, int2word)\n",
    "    target_text = target_texts[seq_index][1:-1]\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('GT sentence:', target_text)\n",
    "    print('Decoded sentence:', decoded_sentence)   \n",
    "    decoded_sentences.append(decoded_sentence)\n",
    "    target_texts_.append(target_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
