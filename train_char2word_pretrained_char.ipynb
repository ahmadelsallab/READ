{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Bidirectional, Concatenate, GRU, Dot, TimeDistributed, Activation, Embedding, Lambda, Concatenate, Reshape\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, LearningRateScheduler\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from keras.models import load_model\n",
    "from nltk.tokenize import word_tokenize\n",
    "from autocorrect import spell\n",
    "import re\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit gpu allocation. allow_growth, or gpu_fraction\n",
    "def gpu_alloc():\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_alloc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_WER_sent(gt, pred):\n",
    "    '''\n",
    "    calculate_WER('calculating wer between two sentences', 'calculate wer between two sentences')\n",
    "    '''\n",
    "    gt_words = gt.lower().split(' ')\n",
    "    pred_words = pred.lower().split(' ')\n",
    "    d = np.zeros(((len(gt_words) + 1), (len(pred_words) + 1)), dtype=np.uint8)\n",
    "    # d = d.reshape((len(gt_words)+1, len(pred_words)+1))\n",
    "\n",
    "    # Initializing error matrix\n",
    "    for i in range(len(gt_words) + 1):\n",
    "        for j in range(len(pred_words) + 1):\n",
    "            if i == 0:\n",
    "                d[0][j] = j\n",
    "            elif j == 0:\n",
    "                d[i][0] = i\n",
    "\n",
    "    # computation\n",
    "    for i in range(1, len(gt_words) + 1):\n",
    "        for j in range(1, len(pred_words) + 1):\n",
    "            if gt_words[i - 1] == pred_words[j - 1]:\n",
    "                d[i][j] = d[i - 1][j - 1]\n",
    "            else:\n",
    "                substitution = d[i - 1][j - 1] + 1\n",
    "                insertion = d[i][j - 1] + 1\n",
    "                deletion = d[i - 1][j] + 1\n",
    "                d[i][j] = min(substitution, insertion, deletion)\n",
    "    return d[len(gt_words)][len(pred_words)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_WER(gt, pred):\n",
    "    '''\n",
    "\n",
    "    :param gt: list of sentences of the ground truth\n",
    "    :param pred: list of sentences of the predictions\n",
    "    both lists must have the same length\n",
    "    :return: accumulated WER\n",
    "    '''\n",
    "#    assert len(gt) == len(pred)\n",
    "    WER = 0\n",
    "    nb_w = 0\n",
    "    for i in range(len(gt)):\n",
    "        #print(gt[i])\n",
    "        #print(pred[i])\n",
    "        WER += calculate_WER_sent(gt[i], pred[i])\n",
    "        nb_w += len(gt[i])\n",
    "\n",
    "    return WER / nb_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Artificial noisy spelling mistakes\n",
    "def noise_maker(sentence, threshold):\n",
    "    '''Relocate, remove, or add characters to create spelling mistakes'''\n",
    "    letters = ['a','b','c','d','e','f','g','h','i','j','k','l','m',\n",
    "           'n','o','p','q','r','s','t','u','v','w','x','y','z',]\n",
    "    noisy_sentence = []\n",
    "    i = 0\n",
    "    while i < len(sentence):\n",
    "        random = np.random.uniform(0, 1, 1)\n",
    "        # Most characters will be correct since the threshold value is high\n",
    "        if random < threshold:\n",
    "            noisy_sentence.append(sentence[i])\n",
    "        else:\n",
    "            new_random = np.random.uniform(0, 1, 1)\n",
    "            # ~33% chance characters will swap locations\n",
    "            if new_random > 0.67:\n",
    "                if i == (len(sentence) - 1):\n",
    "                    # If last character in sentence, it will not be typed\n",
    "                    continue\n",
    "                else:\n",
    "                    # if any other character, swap order with following character\n",
    "                    noisy_sentence.append(sentence[i + 1])\n",
    "                    noisy_sentence.append(sentence[i])\n",
    "                    i += 1\n",
    "            # ~33% chance an extra lower case letter will be added to the sentence\n",
    "            elif new_random < 0.33:\n",
    "                random_letter = np.random.choice(letters, 1)[0]\n",
    "                noisy_sentence.append(random_letter)\n",
    "                noisy_sentence.append(sentence[i])\n",
    "            # ~33% chance a character will not be typed\n",
    "            else:\n",
    "                pass\n",
    "        i += 1\n",
    "\n",
    "    return ''.join(noisy_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_with_gt(file_name, num_samples, max_sent_len, min_sent_len, delimiter='\\t', gt_index=1, prediction_index=0):\n",
    "    '''Load data from txt file, with each line has: <TXT><TAB><GT>. The  target to the decoder muxt have \\t as the start trigger and \\n as the stop trigger.'''\n",
    "    cnt = 0  \n",
    "    input_texts = []\n",
    "    gt_texts = []\n",
    "    target_texts = []\n",
    "    for row in open(file_name, encoding='utf8'):\n",
    "        if cnt < num_samples :\n",
    "            #print(row)\n",
    "            sents = row.split(delimiter)\n",
    "            if (len(sents) < 2):\n",
    "                continue             \n",
    "            input_text = sents[prediction_index]\n",
    "            \n",
    "            target_text = '\\t' + sents[gt_index].strip() + '\\n'\n",
    "            if len(input_text) > min_sent_len and len(input_text) < max_sent_len and len(target_text) > min_sent_len and len(target_text) < max_sent_len:\n",
    "                cnt += 1\n",
    "                \n",
    "                input_texts.append(input_text)\n",
    "                target_texts.append(target_text)\n",
    "                gt_texts.append(sents[gt_index])\n",
    "    return input_texts, target_texts, gt_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_with_noise(file_name, num_samples, noise_threshold, max_sent_len, min_sent_len):\n",
    "    '''Load data from txt file, with each line has: <TXT>. The GT is just a noisy version of TXT. The  target to the decoder muxt have \\t as the start trigger and \\n as the stop trigger.'''\n",
    "    cnt = 0  \n",
    "    input_texts = []\n",
    "    gt_texts = []\n",
    "    target_texts = []\n",
    "    while cnt < num_samples :\n",
    "        for row in open(file_name, encoding='utf8'):\n",
    "        #for row in open(file_name):\n",
    "            if cnt < num_samples :\n",
    "                sents = row.split(\"\\t\")\n",
    "                if (len(sents) < 2):\n",
    "                    continue                 \n",
    "                input_text = noise_maker(sents[1], noise_threshold)\n",
    "                input_text = input_text[:-1]\n",
    "\n",
    "                target_text = '\\t' + sents[1].strip() + '\\n'            \n",
    "                if len(input_text) > min_sent_len and len(input_text) < max_sent_len and len(target_text) > min_sent_len and len(target_text) < max_sent_len:\n",
    "                    cnt += 1\n",
    "                    input_texts.append(input_text)\n",
    "                    target_texts.append(target_text)\n",
    "                    gt_texts.append(target_text[1:-1])\n",
    "                    \n",
    "    return input_texts, target_texts, gt_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_words_vocab(all_texts):\n",
    "    '''Build vocab dictionary to victorize chars into ints'''\n",
    "    vocab_to_int = {}\n",
    "    count = 0 # Start index for any char will be 1, as 0 is masked by the Embedding/Masking layer\n",
    "    codes = ['UNK', ' ', '\\t','\\n']# Start 'UNK' at the first entry, to keep its index=0 to be masked\n",
    "    for code in codes:\n",
    "        if code not in vocab_to_int:\n",
    "            vocab_to_int[code] = count\n",
    "            count += 1    \n",
    "    \n",
    "    for sentence in all_texts:       \n",
    "        for word in word_tokenize(sentence):\n",
    "            word = process_word(word)\n",
    "            if word not in vocab_to_int:\n",
    "                vocab_to_int[word] = count\n",
    "                count += 1\n",
    "\n",
    "\n",
    "    '''''Build inverse translation from int to word'''\n",
    "    int_to_vocab = {}\n",
    "    for word, value in vocab_to_int.items():\n",
    "        int_to_vocab[value] = word\n",
    "        \n",
    "    return vocab_to_int, int_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_chars_vocab(all_texts):\n",
    "    '''Build vocab dictionary to victorize chars into ints'''\n",
    "    vocab_to_int = {}\n",
    "    count = 0 # Start index for any char will be 1, as 0 is masked by the Embedding/Masking layer\n",
    "    codes = ['UNK', ' ', '\\t','\\n']# Start 'UNK' at the first entry, to keep its index=0 to be masked\n",
    "    for code in codes:\n",
    "        if code not in vocab_to_int:\n",
    "            vocab_to_int[code] = count\n",
    "            count += 1    \n",
    "    \n",
    "    for sentence in all_texts:\n",
    "        for char in sentence:\n",
    "            if char not in vocab_to_int:\n",
    "                vocab_to_int[char] = count\n",
    "                count += 1\n",
    "\n",
    "\n",
    "    '''''Build inverse translation from int to char'''\n",
    "    int_to_vocab = {}\n",
    "    for character, value in vocab_to_int.items():\n",
    "        int_to_vocab[value] = character\n",
    "        \n",
    "    return vocab_to_int, int_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_char_data(input_texts, target_texts, max_encoder_seq_length, num_encoder_tokens, vocab_to_int):\n",
    "    '''Prepares the input text and targets into the proper seq2seq numpy arrays'''\n",
    "    encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length),\n",
    "    dtype='float32')\n",
    "    decoder_input_data = np.zeros(\n",
    "        (len(input_texts), max_decoder_seq_length),\n",
    "        dtype='float32')\n",
    "    decoder_target_data = np.zeros(\n",
    "        (len(input_texts), max_decoder_seq_length, num_encoder_tokens),\n",
    "        dtype='float32')\n",
    "\n",
    "    for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "        for t, char in enumerate(input_text):\n",
    "            # c0..cn\n",
    "            encoder_input_data[i, t] = vocab_to_int[char]\n",
    "        for t, char in enumerate(target_text):\n",
    "            # c0'..cm'\n",
    "            # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "            decoder_input_data[i, t] = vocab_to_int[char]\n",
    "            if t > 0:\n",
    "                # decoder_target_data will be ahead by one timestep\n",
    "                # and will not include the start character.\n",
    "                decoder_target_data[i, t - 1, vocab_to_int[char]] = 1.\n",
    "                \n",
    "    return encoder_input_data, decoder_input_data, decoder_target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_hier_data(input_texts, target_texts, max_words_seq_length, max_chars_seq_length, num_char_tokens, num_word_tokens, word2int, char2int):\n",
    "\n",
    "    '''Prepares the input text and targets into the proper seq2seq numpy arrays'''\n",
    "    # \n",
    "    encoder_char_input_data = np.zeros(\n",
    "    (len(input_texts), max_words_seq_length, max_chars_seq_length),\n",
    "    dtype='float32')\n",
    "    \n",
    "    decoder_word_input_data = np.zeros(\n",
    "        (len(input_texts), max_words_seq_length),\n",
    "        dtype='float32')\n",
    "    \n",
    "    decoder_word_target_data = np.zeros(\n",
    "        (len(input_texts), max_words_seq_length, num_word_tokens),\n",
    "        dtype='float32')\n",
    "\n",
    "    for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "        words_lst = word_tokenize(input_text)\n",
    "        if(len(words_lst) > max_words_seq_length):\n",
    "            continue\n",
    "        for j, word in enumerate(words_lst):\n",
    "            if(len(word) > max_chars_seq_length):\n",
    "                continue\n",
    "            for k, char in enumerate(word):\n",
    "                # c0..cn\n",
    "                if(char in char2int):\n",
    "                    encoder_char_input_data[i, j, k] = char2int[char]\n",
    "                    \n",
    "        words_lst = word_tokenize(target_text)# word_tokenize removes the \\t and \\n, we need them to start and end a sequence\n",
    "        words_lst.insert(0, '\\t')\n",
    "        words_lst.append('\\n')        \n",
    "        if(len(words_lst) > max_words_seq_length):\n",
    "            continue                \n",
    "        for j, word in enumerate(words_lst):\n",
    "            processed_word = process_word(word)\n",
    "            if not processed_word in word2int:\n",
    "                continue\n",
    "            # c0'..cm'\n",
    "            # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "            decoder_word_input_data[i, j] = word2int[processed_word]\n",
    "            if j > 0:\n",
    "                # decoder_target_data will be ahead by one timestep\n",
    "                # and will not include the start character.\n",
    "                decoder_word_target_data[i, j - 1, word2int[processed_word]] = 1.\n",
    "                \n",
    "    return encoder_char_input_data, decoder_word_input_data, decoder_word_target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_gt_sequence(input_seq, int_to_vocab):\n",
    "\n",
    "    decoded_sentence = ''\n",
    "    for i in range(input_seq.shape[1]):\n",
    "        \n",
    "        # Sample a token\n",
    "        sampled_token_index = input_seq[0][i]\n",
    "        sampled_word = int_to_vocab[sampled_token_index]\n",
    "        decoded_sentence += sampled_word + ' '\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq, encoder_model, decoder_model, max_words_seq_len, num_decoder_tokens, int_to_vocab):\n",
    "    # Encode the input as state vectors.\n",
    "    encoder_outputs, h, c  = encoder_model.predict(input_seq)\n",
    "    states_value = [h,c]\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, max_words_seq_len))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0] = vocab_to_int['\\t']\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    #print(input_seq)\n",
    "    attention_density = []\n",
    "\n",
    "    special_chars = ['\\\\', '/', '-', '—' , ':', '[', ']', ',', '.', '\"', ';', '%', '~', '(', ')', '{', '}', '$']\n",
    "    i = 0\n",
    "    while not stop_condition:\n",
    "        #print(target_seq)\n",
    "        output_tokens, attention, h, c  = decoder_model.predict(\n",
    "            [target_seq, encoder_outputs] + states_value)\n",
    "        #print(attention.shape)\n",
    "        attention_density.append(attention[0][0])# attention is max_sent_len x 1 since we have num_time_steps = 1 for the output\n",
    "        # Sample a token\n",
    "        #print(output_tokens.shape)\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        \n",
    "        #print(sampled_token_index)\n",
    "        sampled_char = int_to_vocab[sampled_token_index]\n",
    "       \n",
    "        \n",
    "        #orig_char = int_to_vocab[int(input_seq[:,i][0])]\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(word_tokenize(decoded_sentence)) > max_words_seq_len):\n",
    "            stop_condition = True\n",
    "            sampled_char = ''\n",
    "\n",
    "        # Copy digits as it, since the spelling corrector is not good at digit corrections\n",
    "        '''\n",
    "        if(orig_char.isdigit() or orig_char in special_chars):\n",
    "            decoded_sentence += orig_char            \n",
    "        else:\n",
    "            if(sampled_char.isdigit() or sampled_char in special_chars):\n",
    "                decoded_sentence += ''\n",
    "            else:\n",
    "                decoded_sentence += sampled_char\n",
    "        '''\n",
    "        decoded_sentence += sampled_char + ' '\n",
    "\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        #target_seq = np.zeros((1, max_words_seq_len))\n",
    "        if i < max_words_seq_len:\n",
    "            target_seq[0, i] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "        i += 1\n",
    "        #if i > 48:\n",
    "        #    i = 0\n",
    "        \n",
    "\n",
    "    attention_density = np.array(attention_density)\n",
    "    return decoded_sentence, attention_density\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_char_sequence(input_seq, encoder_model, decoder_model, num_decoder_tokens, int_to_vocab):\n",
    "    # Encode the input as state vectors.\n",
    "    encoder_outputs, h, c  = encoder_model.predict(input_seq)\n",
    "    states_value = [h,c]\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0] = vocab_to_int['\\t']\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    #print(input_seq)\n",
    "    attention_density = []\n",
    "    i = 0\n",
    "    special_chars = ['\\\\', '/', '-', '—' , ':', '[', ']', ',', '.', '\"', ';', '%', '~', '(', ')', '{', '}', '$']\n",
    "    while not stop_condition:\n",
    "        #print(target_seq)\n",
    "        output_tokens, attention, h, c  = decoder_model.predict(\n",
    "            [target_seq, encoder_outputs] + states_value)\n",
    "        #print(attention.shape)\n",
    "        attention_density.append(attention[0][0])# attention is max_sent_len x 1 since we have num_time_steps = 1 for the output\n",
    "        # Sample a token\n",
    "        #print(output_tokens.shape)\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        \n",
    "        #print(sampled_token_index)\n",
    "        sampled_char = int_to_vocab[sampled_token_index]\n",
    "        orig_char = int_to_vocab[int(input_seq[:,i][0])]\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "            sampled_char = ''\n",
    "\n",
    "        # Copy digits as it, since the spelling corrector is not good at digit corrections\n",
    "        if(orig_char.isdigit() or orig_char in special_chars):\n",
    "            decoded_sentence += orig_char            \n",
    "        else:\n",
    "            if(sampled_char.isdigit() or sampled_char in special_chars):\n",
    "                decoded_sentence += ''\n",
    "            else:\n",
    "                decoded_sentence += sampled_char\n",
    "        \n",
    "\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "        \n",
    "        i += 1\n",
    "        if(i > 48):\n",
    "            i = 0\n",
    "    attention_density = np.array(attention_density)\n",
    "    return decoded_sentence, attention_density\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_char_model(num_encoder_tokens, latent_dim):\n",
    "    # Define an input sequence and process it.\n",
    "    encoder_inputs = Input(shape=(None,), dtype='float32')\n",
    "    encoder_inputs_ = Embedding(num_encoder_tokens, num_encoder_tokens,                           \n",
    "                            weights=[np.eye(num_encoder_tokens)],\n",
    "                            mask_zero=True, trainable=False)(encoder_inputs)    \n",
    "    #encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "    encoder = Bidirectional(LSTM(latent_dim, return_state=True, return_sequences=True)) # Bi LSTM\n",
    "    encoder_outputs, state_f_h, state_f_c, state_b_h, state_b_c = encoder(encoder_inputs_)# Bi LSTM\n",
    "    state_h = Concatenate()([state_f_h, state_b_h])# Bi LSTM\n",
    "    state_c = Concatenate()([state_f_c, state_b_c])# Bi LSTM\n",
    "\n",
    "    # We discard `encoder_outputs` and only keep the states.\n",
    "    encoder_states = [state_h, state_c]# Bi GRU, LSTM, BHi LSTM\n",
    "    #print(encoder_states)\n",
    "    \n",
    "    decoder_inputs = Input(shape=(None,))\n",
    "    decoder_inputs_ = Embedding(num_encoder_tokens, num_encoder_tokens,                           \n",
    "                            weights=[np.eye(num_encoder_tokens)],\n",
    "                            mask_zero=True, trainable=False)(decoder_inputs)    \n",
    "    # We set up our decoder to return full output sequences,\n",
    "    # and to return internal states as well. We don't use the\n",
    "    # return states in the training model, but we will use them in inference.\n",
    "    decoder_lstm = LSTM(latent_dim*2, return_sequences=True, return_state=True)# Bi LSTM\n",
    "    \n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs_, initial_state=encoder_states)\n",
    "\n",
    "    # Equation (7) with 'dot' score from Section 3.1 in the paper.\n",
    "    # Note that we reuse Softmax-activation layer instead of writing tensor calculation\n",
    "    #print(decoder_outputs)\n",
    "    #print(encoder_outputs)\n",
    "    att_dot = Dot(axes=[2, 2])\n",
    "    attention = att_dot([decoder_outputs, encoder_outputs])\n",
    "    att_activation = Activation('softmax', name='attention')\n",
    "    attention = att_activation(attention)\n",
    "    #print('attention', attention)\n",
    "    context_dot = Dot(axes=[2,1])\n",
    "    context = context_dot([attention, encoder_outputs])\n",
    "    #print('context', context)\n",
    "    att_context_concat = Concatenate()\n",
    "    decoder_combined_context = att_context_concat([context, decoder_outputs])\n",
    "    #print('decoder_combined_context', decoder_combined_context)\n",
    "\n",
    "    # Has another weight + tanh layer as described in equation (5) of the paper\n",
    "    #decoder_outputs = TimeDistributed(Dense(64, activation=\"tanh\"))(decoder_combined_context)\n",
    "    #decoder_outputs = TimeDistributed(Dense(num_encoder_tokens, activation=\"softmax\"))(decoder_outputs)\n",
    "\n",
    "    decoder_dense = Dense(num_encoder_tokens, activation='softmax')\n",
    "    #decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_outputs = decoder_dense(decoder_combined_context)\n",
    "\n",
    "    # Define the model that will turn\n",
    "    # `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "    #model = Model(decoder_inputs, decoder_outputs)\n",
    "    #print('encoder-decoder  model:')\n",
    "    print(model.summary()) \n",
    "    \n",
    "    #print(encoder_inputs)\n",
    "    #print(encoder_outputs)\n",
    "    #print(encoder_states)\n",
    "    #encoder_model = Model(encoder_inputs, [encoder_outputs, encoder_states])\n",
    "    encoder_model = Model(input=encoder_inputs, output=[encoder_outputs] + encoder_states)\n",
    "    print(encoder_outputs.shape)\n",
    "    encoder_embedding_output = Lambda(lambda x: x[:,-1,:])(encoder_outputs)\n",
    "    encoder_word_embedding_model = Model(input=encoder_inputs, output=encoder_embedding_output)\n",
    "\n",
    "    #decoder_state_input_h = Input(shape=(latent_dim,))# LSTM\n",
    "    #decoder_state_input_c = Input(shape=(latent_dim,))# LSTM\n",
    "    decoder_encoder_inputs = Input(shape=(None, latent_dim*2,))\n",
    "    decoder_state_input_h = Input(shape=(latent_dim*2,))# Bi LSTM\n",
    "    decoder_state_input_c = Input(shape=(latent_dim*2,)) # Bi LSTM\n",
    "    #decoder_state_input = Input(shape=(latent_dim*2,)) # Bi GRU\n",
    "\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    #decoder_states_inputs = [decoder_state_input] # Bi GRU\n",
    "\n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs_, initial_state=decoder_states_inputs)\n",
    "\n",
    "    #decoder_outputs, state = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "\n",
    "    decoder_states = [state_h, state_c]\n",
    "    \n",
    "    # Equation (7) with 'dot' score from Section 3.1 in the paper.\n",
    "    # Note that we reuse Softmax-activation layer instead of writing tensor calculation\n",
    "    \n",
    "    attention = att_dot([decoder_outputs, decoder_encoder_inputs])\n",
    "    \n",
    "    attention = att_activation(attention)\n",
    "    #print('attention', attention)\n",
    "    \n",
    "    context = context_dot([attention, decoder_encoder_inputs])\n",
    "    #print('context', context)\n",
    "    \n",
    "    \n",
    "    \n",
    "    decoder_combined_context = att_context_concat([context, decoder_outputs])\n",
    "    #print('decoder_combined_context', decoder_combined_context)\n",
    "\n",
    "    # Has another weight + tanh layer as described in equation (5) of the paper\n",
    "    #decoder_outputs = TimeDistributed(Dense(64, activation=\"tanh\"))(decoder_combined_context)\n",
    "    #decoder_outputs = TimeDistributed(Dense(num_encoder_tokens, activation=\"softmax\"))(decoder_outputs)\n",
    "    \n",
    "    #decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "    #decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_outputs = decoder_dense(decoder_combined_context)\n",
    "    \n",
    "    decoder_model = Model(\n",
    "        [decoder_inputs, decoder_encoder_inputs] + decoder_states_inputs,\n",
    "        [decoder_outputs, attention] + decoder_states)\n",
    "    \n",
    "    return model, encoder_model, decoder_model, encoder_word_embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_hier_model(encoder_word_embedding_model, max_words_seq_len, max_char_seq_len, num_word_tokens, num_char_tokens, latent_dim):\n",
    "    # Define an input sequence and process it.\n",
    "\n",
    "    inputs = Input(shape=(max_words_seq_len, max_char_seq_len,), dtype='float32')\n",
    "    decoder_inputs_words = Input(shape=(max_words_seq_len,), dtype='float32')\n",
    "    words_states = []\n",
    "    '''\n",
    "    for w in range(max_words_seq_len):\n",
    "        \n",
    "        encoder_char_inputs = Lambda(lambda x: x[:,w,:])(inputs)\n",
    "        _, h, c = encoder_char_model(encoder_char_inputs)\n",
    "        encoder_chars_states = Concatenate()([h,c])\n",
    "        #print(encoder_chars_states)\n",
    "        encoder_chars_states = Reshape((1,latent_dim*4))(encoder_chars_states)\n",
    "        words_states.append(encoder_chars_states)\n",
    "    \n",
    "    input_words = Concatenate(axis=-2)(words_states)\n",
    "\n",
    "    '''\n",
    "    #input_words = TimeDistributed(Dense(10))(inputs)\n",
    "\n",
    "    input_words = TimeDistributed(encoder_word_embedding_model)(inputs)\n",
    "\n",
    "    encoder_inputs_ = input_words   \n",
    "    #encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "    encoder = Bidirectional(LSTM(latent_dim, return_state=True, return_sequences=True)) # Bi LSTM\n",
    "    encoder_outputs, state_f_h, state_f_c, state_b_h, state_b_c = encoder(encoder_inputs_)# Bi LSTM\n",
    "    state_h = Concatenate()([state_f_h, state_b_h])# Bi LSTM\n",
    "    state_c = Concatenate()([state_f_c, state_b_c])# Bi LSTM\n",
    "\n",
    "    # We discard `encoder_outputs` and only keep the states.\n",
    "    encoder_states = [state_h, state_c]# Bi GRU, LSTM, BHi LSTM\n",
    "    \n",
    "    decoder_inputs = decoder_inputs_words\n",
    "    decoder_inputs_ = Embedding(num_word_tokens, latent_dim*4,                           \n",
    "                            #weights=[np.eye(num_word_tokens)],\n",
    "                            mask_zero=True, trainable=True)(decoder_inputs)    \n",
    "    # We set up our decoder to return full output sequences,\n",
    "    # and to return internal states as well. We don't use the\n",
    "    # return states in the training model, but we will use them in inference.\n",
    "    decoder_lstm = LSTM(latent_dim*2, return_sequences=True, return_state=True)# Bi LSTM\n",
    "    \n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs_, initial_state=encoder_states)\n",
    "\n",
    "    att_dot = Dot(axes=[2, 2])\n",
    "    attention = att_dot([decoder_outputs, encoder_outputs])\n",
    "    att_activation = Activation('softmax')\n",
    "    attention = att_activation(attention)\n",
    "    #print('attention', attention)\n",
    "    context_dot = Dot(axes=[2,1])\n",
    "    context = context_dot([attention, encoder_outputs])\n",
    "    #print('context', context)\n",
    "    att_context_concat = Concatenate()\n",
    "    decoder_combined_context = att_context_concat([context, decoder_outputs])\n",
    "    #print('decoder_combined_context', decoder_combined_context)\n",
    "\n",
    "    # Has another weight + tanh layer as described in equation (5) of the paper\n",
    "    #decoder_outputs = TimeDistributed(Dense(64, activation=\"tanh\"))(decoder_combined_context)\n",
    "    #decoder_outputs = TimeDistributed(Dense(num_encoder_tokens, activation=\"softmax\"))(decoder_outputs)\n",
    "\n",
    "    decoder_dense = Dense(num_word_tokens, activation='softmax')\n",
    "    #decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_outputs = decoder_dense(decoder_combined_context)\n",
    "\n",
    "    # Define the model that will turn\n",
    "    # `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "    model = Model([inputs, decoder_inputs_words], decoder_outputs)\n",
    "    #model = Model(decoder_inputs, decoder_outputs)\n",
    "    print('encoder-decoder  model:')\n",
    "    print(model.summary()) \n",
    "    \n",
    "    #encoder_model = Model(encoder_inputs, [encoder_outputs, encoder_states])\n",
    "    encoder_model = Model(input=inputs, output=[encoder_outputs] + encoder_states)\n",
    "\n",
    "    #decoder_state_input_h = Input(shape=(latent_dim,))# LSTM\n",
    "    #decoder_state_input_c = Input(shape=(latent_dim,))# LSTM\n",
    "    decoder_encoder_inputs = Input(shape=(max_words_seq_len, latent_dim*2,))\n",
    "    decoder_state_input_h = Input(shape=(latent_dim*2,))# Bi LSTM\n",
    "    decoder_state_input_c = Input(shape=(latent_dim*2,)) # Bi LSTM\n",
    "    #decoder_state_input = Input(shape=(latent_dim*2,)) # Bi GRU\n",
    "\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    #decoder_states_inputs = [decoder_state_input] # Bi GRU\n",
    "\n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs_, initial_state=decoder_states_inputs)\n",
    "\n",
    "    #decoder_outputs, state = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "\n",
    "    decoder_states = [state_h, state_c]\n",
    "    \n",
    "    # Equation (7) with 'dot' score from Section 3.1 in the paper.\n",
    "    # Note that we reuse Softmax-activation layer instead of writing tensor calculation\n",
    "    \n",
    "    attention = att_dot([decoder_outputs, decoder_encoder_inputs])\n",
    "    \n",
    "    attention = att_activation(attention)\n",
    "    #print('attention', attention)\n",
    "    \n",
    "    context = context_dot([attention, decoder_encoder_inputs])\n",
    "    #print('context', context)\n",
    "    \n",
    "    \n",
    "    \n",
    "    decoder_combined_context = att_context_concat([context, decoder_outputs])\n",
    "    #print('decoder_combined_context', decoder_combined_context)\n",
    "\n",
    "    # Has another weight + tanh layer as described in equation (5) of the paper\n",
    "    #decoder_outputs = TimeDistributed(Dense(64, activation=\"tanh\"))(decoder_combined_context)\n",
    "    #decoder_outputs = TimeDistributed(Dense(num_encoder_tokens, activation=\"softmax\"))(decoder_outputs)\n",
    "    \n",
    "    #decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "    #decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_outputs = decoder_dense(decoder_combined_context)\n",
    "    \n",
    "    decoder_model = Model(\n",
    "        [decoder_inputs_words, decoder_encoder_inputs] + decoder_states_inputs,\n",
    "        [decoder_outputs, attention] + decoder_states)\n",
    "    \n",
    "    return model, encoder_model, decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder_word_embedding_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmodel, encoder_model, decoder_model = build_hier_model(encoder_word_embedding_model=encoder_word_embedding_model, \\n                              max_words_seq_len=max_words_seq_len,\\n                              max_char_seq_len=max_chars_seq_len,\\n                              num_word_tokens=num_word_tokens,\\n                              num_char_tokens=num_char_tokens, \\n                              latent_dim=latent_dim)\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "model, encoder_model, decoder_model = build_hier_model(encoder_word_embedding_model=encoder_word_embedding_model, \n",
    "                              max_words_seq_len=max_words_seq_len,\n",
    "                              max_char_seq_len=max_chars_seq_len,\n",
    "                              num_word_tokens=num_word_tokens,\n",
    "                              num_char_tokens=num_char_tokens, \n",
    "                              latent_dim=latent_dim)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n_, encoder_char_model, _ = build_char_model(num_encoder_tokens=28, latent_dim=256)\\nhier_model = build_hier_model(encoder_char_model=encoder_char_model, max_words_seq_len=40, max_char_seq_len=20, num_word_tokens=1000, num_char_tokens=28, latent_dim=256)\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "_, encoder_char_model, _ = build_char_model(num_encoder_tokens=28, latent_dim=256)\n",
    "hier_model = build_hier_model(encoder_char_model=encoder_char_model, max_words_seq_len=40, max_char_seq_len=20, num_word_tokens=1000, num_char_tokens=28, latent_dim=256)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nx = Input(shape=(50,))\\nx = Reshape((1,50))(x)\\nprint(x)\\nl = []\\nl.append(x)\\nl.append(x)\\nprint(l)\\ny = Concatenate(axis=-2)(l)\\nprint(y)\\nz = Reshape((-1,2,50))(y)\\nprint(z)\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "x = Input(shape=(50,))\n",
    "x = Reshape((1,50))(x)\n",
    "print(x)\n",
    "l = []\n",
    "l.append(x)\n",
    "l.append(x)\n",
    "print(l)\n",
    "y = Concatenate(axis=-2)(l)\n",
    "print(y)\n",
    "z = Reshape((-1,2,50))(y)\n",
    "print(z)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def visualize_attention(text, encoder_model, decoder_model, max_encoder_seq_length, num_decoder_tokens, vocab_to_int, int_to_vocab):\n",
    "\n",
    "    encoder_input_data = np.zeros((1, max_encoder_seq_length), dtype='float32')\n",
    "    \n",
    "    for t, char in enumerate(text):\n",
    "        # c0..cn\n",
    "        encoder_input_data[0, t] = vocab_to_int[char]\n",
    "\n",
    "    input_seq = encoder_input_data[0:1]\n",
    "\n",
    "    decoded_sentence, attention_density = decode_sequence(input_seq, encoder_model, decoder_model, num_decoder_tokens, int_to_vocab)\n",
    "\n",
    "    plt.clf()\n",
    "    plt.figure(figsize=(28,12))\n",
    "    \n",
    "    ax = sns.heatmap(attention_density[:, : len(text) + 2],\n",
    "        xticklabels=[w for w in text],\n",
    "        yticklabels=[w for w in decoded_sentence])\n",
    "\n",
    "    ax.invert_yaxis()\n",
    "    plt.show()\n",
    "    \n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_word(word):\n",
    "    # Try to correct the word from known dict\n",
    "    #word = spell(word)\n",
    "    # Option 1: Replace special chars and digits\n",
    "    #processed_word = re.sub(r'[\\\\\\/\\-\\—\\:\\[\\]\\,\\.\\\"\\;\\%\\~\\(\\)\\{\\}\\$\\#\\?\\●\\@\\+\\-\\*\\d]', r'', w.lower())\n",
    "    \n",
    "    # Option 2: skip all words with special chars or digits\n",
    "    if(len(re.findall(r'[\\\\\\/\\-\\—\\:\\[\\]\\,\\.\\\"\\;\\%\\~\\(\\)\\{\\}\\$\\#\\?\\●\\@\\+\\-\\*\\d]', word.lower())) == 0):\n",
    "        #processed_word = word.lower()\n",
    "        processed_word = word\n",
    "    else:\n",
    "        processed_word = 'UNK'\n",
    "\n",
    "    # Skip stop words\n",
    "    #stop_words = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]        \n",
    "    stop_words = []\n",
    "    if processed_word in stop_words:\n",
    "        processed_word = 'UNK'\n",
    "        \n",
    "    return processed_word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-train char model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../dat/'\n",
    "max_sent_len = 50\n",
    "min_sent_len = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1000\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "files_list = ['all_ocr_data_2.txt', 'field_class_21.txt', 'field_class_32.txt', 'field_class_30.txt']\n",
    "desired_file_sizes = [num_samples, num_samples, num_samples, num_samples]\n",
    "noise_threshold = 0.9\n",
    "\n",
    "for file_name, num_file_samples in zip(files_list, desired_file_sizes):\n",
    "    tess_correction_data = os.path.join(data_path, file_name)\n",
    "    input_texts_OCR, target_texts_OCR, gt_OCR = load_data_with_noise(tess_correction_data, num_file_samples, noise_threshold, max_sent_len, min_sent_len)\n",
    "\n",
    "    input_texts += input_texts_OCR\n",
    "    target_texts += target_texts_OCR\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram of lenghts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chars_per_words_lengths = []\n",
    "words_per_sents_lengths = []\n",
    "\n",
    "# Chars per word should be on all text\n",
    "for text in (input_texts_OCR+target_texts_OCR):\n",
    "    words = word_tokenize(text)\n",
    "    #words_per_sents_lengths.append(len(words))\n",
    "    for word in words:\n",
    "        chars_per_words_lengths.append(len(word))\n",
    "\n",
    "# Words in sent should be on target only        \n",
    "for text in target_texts_OCR:\n",
    "    words = word_tokenize(text)\n",
    "    words_per_sents_lengths.append(len(words))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f21e4737320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "h_w = plt.hist(words_per_sents_lengths, bins=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f21e46f5a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "h_c = plt.hist(chars_per_words_lengths, bins=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build char vocab (all text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts = target_texts + input_texts\n",
    "vocab_to_int, int_to_vocab = build_chars_vocab(all_texts)\n",
    "np.savez('vocab_char-{}'.format(max_sent_len), vocab_to_int=vocab_to_int, int_to_vocab=int_to_vocab, max_sent_len=max_sent_len, min_sent_len=min_sent_len )\n",
    "char2int = vocab_to_int\n",
    "int2char = int_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_characters = sorted(list(vocab_to_int))\n",
    "target_characters = sorted(list(vocab_to_int))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 4000\n",
      "Number of unique input tokens: 91\n",
      "Number of unique output tokens: 91\n",
      "Max sequence length for inputs: 49\n",
      "Max sequence length for outputs: 49\n"
     ]
    }
   ],
   "source": [
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\t': 2,\n",
       " '\\n': 3,\n",
       " ' ': 1,\n",
       " '#': 67,\n",
       " '$': 80,\n",
       " '%': 85,\n",
       " '&': 73,\n",
       " \"'\": 83,\n",
       " '(': 64,\n",
       " ')': 65,\n",
       " '*': 77,\n",
       " '+': 76,\n",
       " ',': 69,\n",
       " '-': 21,\n",
       " '.': 48,\n",
       " '/': 29,\n",
       " '0': 54,\n",
       " '1': 43,\n",
       " '2': 53,\n",
       " '3': 57,\n",
       " '4': 56,\n",
       " '5': 74,\n",
       " '6': 55,\n",
       " '7': 70,\n",
       " '8': 61,\n",
       " '9': 72,\n",
       " ':': 13,\n",
       " ';': 75,\n",
       " '=': 89,\n",
       " '?': 60,\n",
       " '@': 81,\n",
       " 'A': 16,\n",
       " 'B': 15,\n",
       " 'C': 4,\n",
       " 'D': 40,\n",
       " 'E': 46,\n",
       " 'F': 33,\n",
       " 'G': 41,\n",
       " 'H': 52,\n",
       " 'I': 22,\n",
       " 'J': 68,\n",
       " 'K': 50,\n",
       " 'L': 37,\n",
       " 'M': 36,\n",
       " 'N': 35,\n",
       " 'O': 30,\n",
       " 'P': 26,\n",
       " 'Q': 78,\n",
       " 'R': 45,\n",
       " 'S': 38,\n",
       " 'T': 9,\n",
       " 'U': 49,\n",
       " 'UNK': 0,\n",
       " 'V': 14,\n",
       " 'W': 51,\n",
       " 'X': 79,\n",
       " 'Y': 47,\n",
       " 'Z': 71,\n",
       " '^': 86,\n",
       " '_': 90,\n",
       " 'a': 6,\n",
       " 'b': 39,\n",
       " 'c': 17,\n",
       " 'd': 18,\n",
       " 'e': 12,\n",
       " 'f': 32,\n",
       " 'g': 42,\n",
       " 'h': 28,\n",
       " 'i': 7,\n",
       " 'j': 23,\n",
       " 'k': 59,\n",
       " 'l': 5,\n",
       " 'm': 8,\n",
       " 'n': 19,\n",
       " 'o': 27,\n",
       " 'p': 11,\n",
       " 'q': 58,\n",
       " 'r': 25,\n",
       " 's': 34,\n",
       " 't': 20,\n",
       " 'u': 24,\n",
       " 'v': 44,\n",
       " 'w': 31,\n",
       " 'x': 62,\n",
       " 'y': 10,\n",
       " 'z': 63,\n",
       " '|': 82,\n",
       " '’': 66,\n",
       " '•': 84,\n",
       " '●': 87,\n",
       " 'ﬁ': 88}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char2int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'UNK',\n",
       " 1: ' ',\n",
       " 2: '\\t',\n",
       " 3: '\\n',\n",
       " 4: 'C',\n",
       " 5: 'l',\n",
       " 6: 'a',\n",
       " 7: 'i',\n",
       " 8: 'm',\n",
       " 9: 'T',\n",
       " 10: 'y',\n",
       " 11: 'p',\n",
       " 12: 'e',\n",
       " 13: ':',\n",
       " 14: 'V',\n",
       " 15: 'B',\n",
       " 16: 'A',\n",
       " 17: 'c',\n",
       " 18: 'd',\n",
       " 19: 'n',\n",
       " 20: 't',\n",
       " 21: '-',\n",
       " 22: 'I',\n",
       " 23: 'j',\n",
       " 24: 'u',\n",
       " 25: 'r',\n",
       " 26: 'P',\n",
       " 27: 'o',\n",
       " 28: 'h',\n",
       " 29: '/',\n",
       " 30: 'O',\n",
       " 31: 'w',\n",
       " 32: 'f',\n",
       " 33: 'F',\n",
       " 34: 's',\n",
       " 35: 'N',\n",
       " 36: 'M',\n",
       " 37: 'L',\n",
       " 38: 'S',\n",
       " 39: 'b',\n",
       " 40: 'D',\n",
       " 41: 'G',\n",
       " 42: 'g',\n",
       " 43: '1',\n",
       " 44: 'v',\n",
       " 45: 'R',\n",
       " 46: 'E',\n",
       " 47: 'Y',\n",
       " 48: '.',\n",
       " 49: 'U',\n",
       " 50: 'K',\n",
       " 51: 'W',\n",
       " 52: 'H',\n",
       " 53: '2',\n",
       " 54: '0',\n",
       " 55: '6',\n",
       " 56: '4',\n",
       " 57: '3',\n",
       " 58: 'q',\n",
       " 59: 'k',\n",
       " 60: '?',\n",
       " 61: '8',\n",
       " 62: 'x',\n",
       " 63: 'z',\n",
       " 64: '(',\n",
       " 65: ')',\n",
       " 66: '’',\n",
       " 67: '#',\n",
       " 68: 'J',\n",
       " 69: ',',\n",
       " 70: '7',\n",
       " 71: 'Z',\n",
       " 72: '9',\n",
       " 73: '&',\n",
       " 74: '5',\n",
       " 75: ';',\n",
       " 76: '+',\n",
       " 77: '*',\n",
       " 78: 'Q',\n",
       " 79: 'X',\n",
       " 80: '$',\n",
       " 81: '@',\n",
       " 82: '|',\n",
       " 83: \"'\",\n",
       " 84: '•',\n",
       " 85: '%',\n",
       " 86: '^',\n",
       " 87: '●',\n",
       " 88: 'ﬁ',\n",
       " 89: '=',\n",
       " 90: '_'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int2char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(int_to_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize char data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data, decoder_input_data, decoder_target_data = vectorize_char_data(input_texts=input_texts,\n",
    "                                                                             target_texts=target_texts, \n",
    "                                                                             max_encoder_seq_length=max_encoder_seq_length, \n",
    "                                                                             num_encoder_tokens=num_encoder_tokens, \n",
    "                                                                             vocab_to_int=vocab_to_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build char model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 256 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 91)     8281        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) [(None, None, 512),  712704      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, None, 91)     8281        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 512)          0           bidirectional_1[0][1]            \n",
      "                                                                 bidirectional_1[0][3]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 512)          0           bidirectional_1[0][2]            \n",
      "                                                                 bidirectional_1[0][4]            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, None, 512),  1236992     embedding_2[0][0]                \n",
      "                                                                 concatenate_1[0][0]              \n",
      "                                                                 concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, None, None)   0           lstm_2[0][0]                     \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention (Activation)          (None, None, None)   0           dot_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dot_2 (Dot)                     (None, None, 512)    0           attention[0][0]                  \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, None, 1024)   0           dot_2[0][0]                      \n",
      "                                                                 lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 91)     93275       concatenate_3[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 2,059,533\n",
      "Trainable params: 2,042,971\n",
      "Non-trainable params: 16,562\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "(?, ?, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:63: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=[<tf.Tenso...)`\n",
      "/opt/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:66: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"la...)`\n"
     ]
    }
   ],
   "source": [
    "model, encoder_model, decoder_model, encoder_word_embedding_model = build_char_model(latent_dim=latent_dim, num_encoder_tokens=num_encoder_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 91)     8281        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) [(None, None, 512),  712704      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 512)          0           bidirectional_1[0][1]            \n",
      "                                                                 bidirectional_1[0][3]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 512)          0           bidirectional_1[0][2]            \n",
      "                                                                 bidirectional_1[0][4]            \n",
      "==================================================================================================\n",
      "Total params: 720,985\n",
      "Trainable params: 712,704\n",
      "Non-trainable params: 8,281\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit char model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3200 samples, validate on 800 samples\n",
      "Epoch 1/10\n",
      "3200/3200 [==============================] - 19s 6ms/step - loss: 3.4368 - categorical_accuracy: 0.1263 - val_loss: 2.8445 - val_categorical_accuracy: 0.2150\n",
      "\n",
      "Epoch 00001: val_categorical_accuracy improved from -inf to 0.21497, saving model to best_model_char-50.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/keras/engine/network.py:872: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_1/concat:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'concatenate_2/concat:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n",
      "3200/3200 [==============================] - 18s 6ms/step - loss: 2.2253 - categorical_accuracy: 0.3476 - val_loss: 2.0023 - val_categorical_accuracy: 0.4033\n",
      "\n",
      "Epoch 00002: val_categorical_accuracy improved from 0.21497 to 0.40333, saving model to best_model_char-50.hdf5\n",
      "Epoch 3/10\n",
      "3200/3200 [==============================] - 18s 6ms/step - loss: 1.1842 - categorical_accuracy: 0.6423 - val_loss: 1.0497 - val_categorical_accuracy: 0.6696\n",
      "\n",
      "Epoch 00003: val_categorical_accuracy improved from 0.40333 to 0.66957, saving model to best_model_char-50.hdf5\n",
      "Epoch 4/10\n",
      "3200/3200 [==============================] - 18s 6ms/step - loss: 0.4599 - categorical_accuracy: 0.8366 - val_loss: 0.5711 - val_categorical_accuracy: 0.8035\n",
      "\n",
      "Epoch 00004: val_categorical_accuracy improved from 0.66957 to 0.80350, saving model to best_model_char-50.hdf5\n",
      "Epoch 5/10\n",
      "3200/3200 [==============================] - 17s 5ms/step - loss: 0.2348 - categorical_accuracy: 0.8957 - val_loss: 0.4111 - val_categorical_accuracy: 0.8425\n",
      "\n",
      "Epoch 00005: val_categorical_accuracy improved from 0.80350 to 0.84245, saving model to best_model_char-50.hdf5\n",
      "Epoch 6/10\n",
      "3200/3200 [==============================] - 17s 5ms/step - loss: 0.1644 - categorical_accuracy: 0.9137 - val_loss: 0.3802 - val_categorical_accuracy: 0.8503\n",
      "\n",
      "Epoch 00006: val_categorical_accuracy improved from 0.84245 to 0.85030, saving model to best_model_char-50.hdf5\n",
      "Epoch 7/10\n",
      "3200/3200 [==============================] - 21s 7ms/step - loss: 0.1599 - categorical_accuracy: 0.9152 - val_loss: 0.3466 - val_categorical_accuracy: 0.8563\n",
      "\n",
      "Epoch 00007: val_categorical_accuracy improved from 0.85030 to 0.85627, saving model to best_model_char-50.hdf5\n",
      "Epoch 8/10\n",
      "3200/3200 [==============================] - 24s 8ms/step - loss: 0.1106 - categorical_accuracy: 0.9278 - val_loss: 0.3188 - val_categorical_accuracy: 0.8669\n",
      "\n",
      "Epoch 00008: val_categorical_accuracy improved from 0.85627 to 0.86690, saving model to best_model_char-50.hdf5\n",
      "Epoch 9/10\n",
      "3200/3200 [==============================] - 25s 8ms/step - loss: 0.0691 - categorical_accuracy: 0.9396 - val_loss: 0.2922 - val_categorical_accuracy: 0.8750\n",
      "\n",
      "Epoch 00009: val_categorical_accuracy improved from 0.86690 to 0.87498, saving model to best_model_char-50.hdf5\n",
      "Epoch 10/10\n",
      "3200/3200 [==============================] - 25s 8ms/step - loss: 0.0427 - categorical_accuracy: 0.9473 - val_loss: 0.2579 - val_categorical_accuracy: 0.8848\n",
      "\n",
      "Epoch 00010: val_categorical_accuracy improved from 0.87498 to 0.88477, saving model to best_model_char-50.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f21e44f6eb8>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 10  \n",
    "lr = 0.01\n",
    "\n",
    "model.compile(optimizer=optimizers.Adam(lr=lr), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "#filepath=\"weights-improvement-{epoch:02d}-{val_categorical_accuracy:.2f}.hdf5\"\n",
    "filepath=\"best_model_char-{}.hdf5\".format(max_sent_len) # Save only the best model for inference step, as saving the epoch and metric might confuse the inference function which model to use\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_categorical_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "tbCallBack = TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "callbacks_list = [checkpoint, tbCallBack]\n",
    "#callbacks_list = [checkpoint, tbCallBack, lrate]\n",
    "\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          #validation_data = ([test_encoder_input_data, test_decoder_input_data], test_decoder_target_data),\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=callbacks_list,\n",
    "          validation_split=0.2,\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/keras/engine/network.py:872: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_4:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'input_5:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "encoder_char_model_file = 'encoder_char_model-{}.hdf5'\n",
    "decoder_char_model_file = 'decoder_char_model-{}.hdf5'\n",
    "encoder_model.save('encoder_char_model-{}.hdf5'.format(max_sent_len))\n",
    "decoder_model.save('decoder_char_model-{}.hdf5'.format(max_sent_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Hierarichal model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build word vocab (target_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words_seq_len=15\n",
    "max_chars_seq_len=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts = target_texts\n",
    "vocab_to_int, int_to_vocab = build_words_vocab(all_texts)\n",
    "word2int = vocab_to_int\n",
    "int2word = int_to_vocab\n",
    "np.savez('vocab_hier-{}-{}'.format(max_words_seq_len, max_chars_seq_len), char2int=char2int, int2char=int2char, word2int=word2int, int2word=int2word, max_words_seq_len=max_words_seq_len, max_char_seq_len=max_chars_seq_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'UNK': 0,\n",
       " ' ': 1,\n",
       " '\\t': 2,\n",
       " '\\n': 3,\n",
       " 'Claim': 4,\n",
       " 'Type': 5,\n",
       " 'VB': 6,\n",
       " 'Accident': 7,\n",
       " 'Accidental': 8,\n",
       " 'Injury': 9,\n",
       " 'Information': 10,\n",
       " 'First': 11,\n",
       " 'Name': 12,\n",
       " 'Middle': 13,\n",
       " 'Last': 14,\n",
       " 'Social': 15,\n",
       " 'Security': 16,\n",
       " 'Number': 17,\n",
       " 'Birth': 18,\n",
       " 'Date': 19,\n",
       " 'Gender': 20,\n",
       " 'Language': 21,\n",
       " 'Preference': 22,\n",
       " 'Address': 23,\n",
       " 'Line': 24,\n",
       " 'City': 25,\n",
       " 'Postal': 26,\n",
       " 'Code': 27,\n",
       " 'Country': 28,\n",
       " 'Best': 29,\n",
       " 'Phone': 30,\n",
       " 'to': 31,\n",
       " 'be': 32,\n",
       " 'Reached': 33,\n",
       " 'During': 34,\n",
       " 'the': 35,\n",
       " 'Day': 36,\n",
       " 'Email': 37,\n",
       " 'Page': 38,\n",
       " 'of': 39,\n",
       " 'RADIOLOGY': 40,\n",
       " 'REPORT': 41,\n",
       " 'Patient': 42,\n",
       " 'MRN': 43,\n",
       " 'Accession': 44,\n",
       " 'No': 45,\n",
       " 'Ref': 46,\n",
       " 'Physician': 47,\n",
       " 'UNKNOWN': 48,\n",
       " 'Study': 49,\n",
       " 'Hospital': 50,\n",
       " 'DOB': 51,\n",
       " 'Technique': 52,\n",
       " 'views': 53,\n",
       " 'left': 54,\n",
       " 'wrist': 55,\n",
       " 'Cormarison': 56,\n",
       " 'None': 57,\n",
       " 'availabie': 58,\n",
       " 'Comparison': 59,\n",
       " 'available': 60,\n",
       " 'FINDINGS': 61,\n",
       " 'IMPRESSION': 62,\n",
       " 'acute': 63,\n",
       " 'osseous': 64,\n",
       " 'abnormality': 65,\n",
       " 'identified': 66,\n",
       " 'Daytime': 67,\n",
       " 'Event': 68,\n",
       " 'Stopped': 69,\n",
       " 'Working': 70,\n",
       " 'Yes': 71,\n",
       " 'Physically': 72,\n",
       " 'at': 73,\n",
       " 'Work': 74,\n",
       " 'Hours': 75,\n",
       " 'Worked': 76,\n",
       " 'on': 77,\n",
       " 'Scheduled': 78,\n",
       " 'Missed': 79,\n",
       " 'Returned': 80,\n",
       " 'Related': 81,\n",
       " 'Time': 82,\n",
       " 'Diagnosis': 83,\n",
       " 'Arthiscopic': 84,\n",
       " 'surgery': 85,\n",
       " 'Surgery': 86,\n",
       " 'Is': 87,\n",
       " 'Required': 88,\n",
       " 'Indicator': 89,\n",
       " 'Outpatient': 90,\n",
       " 'Medical': 91,\n",
       " 'Provider': 92,\n",
       " 'Roles': 93,\n",
       " 'Treating': 94,\n",
       " 'Patrick': 95,\n",
       " 'Emerson': 96,\n",
       " 'Business': 97,\n",
       " 'Telephone': 98,\n",
       " 'Fax': 99,\n",
       " 'Visit': 100,\n",
       " 'Next': 101,\n",
       " 'Hospitalization': 102,\n",
       " 'Discharge': 103,\n",
       " 'Procedure': 104,\n",
       " 'Left': 105,\n",
       " 'arthiscopic': 106,\n",
       " 'Employment': 107,\n",
       " 'Employer': 108,\n",
       " 'Policy': 109,\n",
       " 'Electronic': 110,\n",
       " 'Submission': 111,\n",
       " 'Identifier': 112,\n",
       " 'Electronically': 113,\n",
       " 'Signed': 114,\n",
       " 'Fraud': 115,\n",
       " 'Statements': 116,\n",
       " 'Reviewed': 117,\n",
       " 'and': 118,\n",
       " 'unum': 119,\n",
       " 'The': 120,\n",
       " 'Benefits': 121,\n",
       " 'Center': 122,\n",
       " 'Not': 123,\n",
       " 'for': 124,\n",
       " 'FMLA': 125,\n",
       " 'Requests': 126,\n",
       " 'Insured': 127,\n",
       " '’': 128,\n",
       " 's': 129,\n",
       " 'Signature': 130,\n",
       " 'Printed': 131,\n",
       " 'Unum': 132,\n",
       " 'Confirmation': 133,\n",
       " 'Coverage': 134,\n",
       " 'Group': 135,\n",
       " 'Customer': 136,\n",
       " 'EE': 137,\n",
       " 'Effective': 138,\n",
       " 'Employee': 139,\n",
       " 'Acc': 140,\n",
       " 'January': 141,\n",
       " 'Wellness': 142,\n",
       " 'Benefit': 143,\n",
       " 'Total': 144,\n",
       " 'Monthly': 145,\n",
       " 'Premium': 146,\n",
       " 'Montly': 147,\n",
       " 'Payroll': 148,\n",
       " 'Deduction': 149,\n",
       " 'Account': 150,\n",
       " 'F': 151,\n",
       " 'Exam': 152,\n",
       " 'Referring': 153,\n",
       " 'Phys': 154,\n",
       " 'STEPHEN': 155,\n",
       " 'GELOVICH': 156,\n",
       " 'Tax': 157,\n",
       " 'MRI': 158,\n",
       " 'OF': 159,\n",
       " 'THE': 160,\n",
       " 'LEFT': 161,\n",
       " 'WRIST': 162,\n",
       " 'WITHOUT': 163,\n",
       " 'CONTRAST': 164,\n",
       " 'COMPARISON': 165,\n",
       " 'These': 166,\n",
       " 'results': 167,\n",
       " 'were': 168,\n",
       " 'faxed': 169,\n",
       " 'Gelovich': 170,\n",
       " 'signed': 171,\n",
       " 'by': 172,\n",
       " 'Stephen': 173,\n",
       " 'Bravo': 174,\n",
       " 'Dependent': 175,\n",
       " 'Detail': 176,\n",
       " 'Zachary': 177,\n",
       " 'Jager': 178,\n",
       " 'Billed': 179,\n",
       " 'Amounts': 180,\n",
       " 'Contract': 181,\n",
       " 'Adjustment': 182,\n",
       " 'Allowed': 183,\n",
       " 'Amount': 184,\n",
       " 'Covered': 185,\n",
       " 'Reason': 186,\n",
       " 'Deductible': 187,\n",
       " 'Other': 188,\n",
       " 'Carrier': 189,\n",
       " 'Paid': 190,\n",
       " 'Responsibility': 191,\n",
       " 'Totals': 192,\n",
       " 'For': 193,\n",
       " 'Appeals': 194,\n",
       " 'Rights': 195,\n",
       " 'Important': 196,\n",
       " 'about': 197,\n",
       " 'Your': 198,\n",
       " 'Appeal': 199,\n",
       " 'All': 200,\n",
       " 'Languages': 201,\n",
       " 'Contact': 202,\n",
       " 'Did': 203,\n",
       " 'You': 204,\n",
       " 'Know': 205,\n",
       " 'Specialty': 206,\n",
       " 'Orthopedic': 207,\n",
       " 'Surgeon': 208,\n",
       " 'Unknown': 209,\n",
       " 'Kari': 210,\n",
       " 'Lund': 211,\n",
       " 'Orthopedist': 212,\n",
       " 'Dan': 213,\n",
       " 'Palmer': 214,\n",
       " 'On': 215,\n",
       " '&': 216,\n",
       " 'May': 217,\n",
       " 'Spouse': 218,\n",
       " 'Child': 219,\n",
       " 'BLACK': 220,\n",
       " 'HILLS': 221,\n",
       " 'ORTHOPEDIC': 222,\n",
       " 'CENTER': 223,\n",
       " 'PC': 224,\n",
       " 'LAST': 225,\n",
       " 'PMT': 226,\n",
       " 'AMOUNT': 227,\n",
       " 'DUE': 228,\n",
       " 'DATE': 229,\n",
       " 'PAGE': 230,\n",
       " 'STATEMENT': 231,\n",
       " 'Ins': 232,\n",
       " 'Description': 233,\n",
       " 'E': 234,\n",
       " 'm': 235,\n",
       " 'New': 236,\n",
       " 'Moderat': 237,\n",
       " 'S': 238,\n",
       " 'Clo': 239,\n",
       " 'Tx': 240,\n",
       " 'Phalangealfx': 241,\n",
       " 'Finger': 242,\n",
       " 'Splint': 243,\n",
       " 'Offic': 244,\n",
       " 'Cons': 245,\n",
       " 'Moderate': 246,\n",
       " 'Sever': 247,\n",
       " 'Rad': 248,\n",
       " 'Mini': 249,\n",
       " 'Views': 250,\n",
       " 'Applic': 251,\n",
       " 'Hand': 252,\n",
       " 'Lower': 253,\n",
       " 'Forearm': 254,\n",
       " 'Fiberglass': 255,\n",
       " 'gauntlet': 256,\n",
       " 'Cast': 257,\n",
       " 'Yrs': 258,\n",
       " 'Charge': 259,\n",
       " 'Pmt': 260,\n",
       " 'Pat': 261,\n",
       " 'Adjust': 262,\n",
       " 'Current': 263,\n",
       " 'Days': 264,\n",
       " 'Balance': 265,\n",
       " 'Pending': 266,\n",
       " 'Now': 267,\n",
       " 'Due': 268,\n",
       " 'Message': 269,\n",
       " 'Make': 270,\n",
       " 'Checks': 271,\n",
       " 'Payable': 272,\n",
       " 'To': 273,\n",
       " 'Statement': 274,\n",
       " 'Billing': 275,\n",
       " 'Questions': 276,\n",
       " 'Choice': 277,\n",
       " 'Health': 278,\n",
       " 'Administrators': 279,\n",
       " 'Forwarding': 280,\n",
       " 'Service': 281,\n",
       " 'Requested': 282,\n",
       " 'REGIONAL': 283,\n",
       " 'HEALTH': 284,\n",
       " 'INC': 285,\n",
       " 'Participant': 286,\n",
       " 'ID': 287,\n",
       " 'Original': 288,\n",
       " 'Print': 289,\n",
       " 'Website': 290,\n",
       " 'DEA': 291,\n",
       " 'Individual': 292,\n",
       " 'Summary': 293,\n",
       " 'By': 294,\n",
       " 'Plan': 295,\n",
       " 'Status': 296,\n",
       " 'Period': 297,\n",
       " 'DEDUCTIBLE': 298,\n",
       " 'OUT': 299,\n",
       " 'POCKET': 300,\n",
       " 'Regional': 301,\n",
       " 'Inc': 302,\n",
       " 'EXPLANATION': 303,\n",
       " 'BENEFITS': 304,\n",
       " 'Retain': 305,\n",
       " 'Purposes': 306,\n",
       " 'Sign': 307,\n",
       " 'up': 308,\n",
       " 'paperless': 309,\n",
       " 'Family': 310,\n",
       " 'Out': 311,\n",
       " 'Network': 312,\n",
       " 'Karl': 313,\n",
       " 'Services': 314,\n",
       " 'exam': 315,\n",
       " 'hand': 316,\n",
       " 'Modifiers': 317,\n",
       " 'TC': 318,\n",
       " 'RT': 319,\n",
       " 'This': 320,\n",
       " 'Qualified': 321,\n",
       " 'sign': 322,\n",
       " 'language': 323,\n",
       " 'interpreters': 324,\n",
       " 'written': 325,\n",
       " 'in': 326,\n",
       " 'other': 327,\n",
       " 'languages': 328,\n",
       " 'Jacquelin': 329,\n",
       " 'Brainard': 330,\n",
       " 'Compliance': 331,\n",
       " 'Officer': 332,\n",
       " 'or': 333,\n",
       " 'mail': 334,\n",
       " 'phone': 335,\n",
       " 'Department': 336,\n",
       " 'Human': 337,\n",
       " 'Complaint': 338,\n",
       " 'forms': 339,\n",
       " 'are': 340,\n",
       " 'DAKOTA': 341,\n",
       " 'EZ': 342,\n",
       " 'Ways': 343,\n",
       " 'Pay': 344,\n",
       " 'Automated': 345,\n",
       " 'Attendant': 346,\n",
       " 'hours': 347,\n",
       " 'a': 348,\n",
       " 'day': 349,\n",
       " 'Payments': 350,\n",
       " 'Please': 351,\n",
       " 'Call': 352,\n",
       " 'Upon': 353,\n",
       " 'Receipt': 354,\n",
       " 'Improved': 355,\n",
       " 'Online': 356,\n",
       " 'Experience': 357,\n",
       " '|': 358,\n",
       " 'Update': 359,\n",
       " 'Info': 360,\n",
       " 'About': 361,\n",
       " 'See': 362,\n",
       " 'Details': 363,\n",
       " 'Back': 364,\n",
       " 'ACCOUNT': 365,\n",
       " 'NO': 366,\n",
       " 'SHOW': 367,\n",
       " 'PAID': 368,\n",
       " 'HERE': 369,\n",
       " 'MAKE': 370,\n",
       " 'CHECKS': 371,\n",
       " 'TO': 372,\n",
       " 'PROC': 373,\n",
       " 'CODE': 374,\n",
       " 'UNITS': 375,\n",
       " 'DETAILS': 376,\n",
       " 'SERVICES': 377,\n",
       " 'CHARGES': 378,\n",
       " 'INSUR': 379,\n",
       " 'PENDING': 380,\n",
       " 'PATIENT': 381,\n",
       " 'BALANCE': 382,\n",
       " 'EXAM': 383,\n",
       " 'HAND': 384,\n",
       " 'THORAC': 385,\n",
       " 'SPINE': 386,\n",
       " 'COMMERCIAL': 387,\n",
       " 'NON': 388,\n",
       " 'ALLOWED': 389,\n",
       " 'CT': 390,\n",
       " 'ABD': 391,\n",
       " 'PELV': 392,\n",
       " 'PAYMENT': 393,\n",
       " 'CHEST': 394,\n",
       " 'VIEWS': 395,\n",
       " 'HARGES': 396,\n",
       " 'Over': 397,\n",
       " 'DIGIT': 398,\n",
       " 'Of': 399,\n",
       " 'Today': 400,\n",
       " \"'s\": 401,\n",
       " 'Ethnicity': 402,\n",
       " 'Hispanic': 403,\n",
       " 'Latino': 404,\n",
       " 'Preferred': 405,\n",
       " 'English': 406,\n",
       " 'visit': 407,\n",
       " 'with': 408,\n",
       " 'Suzanne': 409,\n",
       " 'Newsom': 410,\n",
       " 'CNP': 411,\n",
       " '•': 412,\n",
       " 'Lethargy': 413,\n",
       " 'cough': 414,\n",
       " 'Vitals': 415,\n",
       " 'lbs': 416,\n",
       " 'kg': 417,\n",
       " 'Wt': 418,\n",
       " 'Temp': 419,\n",
       " 'HR': 420,\n",
       " 'Oxygen': 421,\n",
       " 'sat': 422,\n",
       " 'Allergies': 423,\n",
       " 'Amoxicillin': 424,\n",
       " 'rash': 425,\n",
       " 'possible': 426,\n",
       " 'hives': 427,\n",
       " 'Active': 428,\n",
       " 'Diagnoses': 429,\n",
       " 'Include': 430,\n",
       " 'Acute': 431,\n",
       " 'frontal': 432,\n",
       " 'sinusitis': 433,\n",
       " 'unspecified': 434,\n",
       " 'Dizziness': 435,\n",
       " 'giddiness': 436,\n",
       " 'Medication': 437,\n",
       " 'List': 438,\n",
       " 'medications': 439,\n",
       " 'you': 440,\n",
       " 'Taking': 441,\n",
       " 'Zyrtec': 442,\n",
       " 'Childrens': 443,\n",
       " 'Allergy': 444,\n",
       " 'Notes': 445,\n",
       " 'Tests': 446,\n",
       " 'Labs': 447,\n",
       " 'Illumigene': 448,\n",
       " 'MYCO': 449,\n",
       " 'http': 450,\n",
       " 'BASIC': 451,\n",
       " 'METABOLIC': 452,\n",
       " 'SODIUM': 453,\n",
       " 'Range': 454,\n",
       " 'POTASSIUM': 455,\n",
       " 'CHLORIDE': 456,\n",
       " 'GLUCOSE': 457,\n",
       " 'BUN': 458,\n",
       " 'CREATININE': 459,\n",
       " 'CALCIUM': 460,\n",
       " 'CREA': 461,\n",
       " 'RATIO': 462,\n",
       " 'Ratio': 463,\n",
       " 'ANION': 464,\n",
       " 'GAP': 465,\n",
       " 'Calc': 466,\n",
       " 'CBC': 467,\n",
       " 'DIFF': 468,\n",
       " 'WBC': 469,\n",
       " 'RBC': 470,\n",
       " 'HGB': 471,\n",
       " 'HCT': 472,\n",
       " 'MCV': 473,\n",
       " 'fL': 474,\n",
       " 'MCH': 475,\n",
       " 'pg': 476,\n",
       " 'MCHC': 477,\n",
       " 'MPV': 478,\n",
       " 'PLATELETS': 479,\n",
       " 'NEUTROPHILS': 480,\n",
       " 'LYMPHOCYTES': 481,\n",
       " 'MONOCYTES': 482,\n",
       " 'Conditions': 483,\n",
       " 'Problem': 484,\n",
       " 'Idiopathic': 485,\n",
       " 'urticaria': 486,\n",
       " 'document': 487,\n",
       " 'wish': 488,\n",
       " 'keep': 489,\n",
       " 'Policyholder': 490,\n",
       " 'Owner': 491,\n",
       " 'Eastside': 492,\n",
       " 'Acct': 493,\n",
       " 'Jasminder': 494,\n",
       " 'Singh': 495,\n",
       " 'Dev': 496,\n",
       " 'PA': 497,\n",
       " 'EXCUSE': 498,\n",
       " 'east': 499,\n",
       " 'side': 500,\n",
       " 'medical': 501,\n",
       " 'center': 502,\n",
       " 'April': 503,\n",
       " 'Weekly': 504,\n",
       " 'MEDICAL': 505,\n",
       " 'Ph': 506,\n",
       " 'MR': 507,\n",
       " 'Primary': 508,\n",
       " 'Thoracic': 509,\n",
       " 'Strain': 510,\n",
       " 'have': 511,\n",
       " 'strained': 512,\n",
       " 'your': 513,\n",
       " 'thoracic': 514,\n",
       " 'spine': 515,\n",
       " 'IF': 516,\n",
       " 'ANY': 517,\n",
       " 'FOLLOWING': 518,\n",
       " 'OCCURS': 519,\n",
       " 'feel': 520,\n",
       " 'weakness': 521,\n",
       " 'arms': 522,\n",
       " 'legs': 523,\n",
       " 'severe': 524,\n",
       " 'increase': 525,\n",
       " 'pain': 526,\n",
       " 'Lumbosacral': 527,\n",
       " 'weak': 528,\n",
       " 'becomes': 529,\n",
       " 'more': 530,\n",
       " 'Follow': 531,\n",
       " 'Up': 532,\n",
       " 'What': 533,\n",
       " 'Do': 534,\n",
       " 'Take': 535,\n",
       " 'all': 536,\n",
       " 'as': 537,\n",
       " 'directed': 538,\n",
       " 'Additional': 539,\n",
       " 'Prescriptions': 540,\n",
       " 'Written': 541,\n",
       " 'Prescriber': 542,\n",
       " 'Paper': 543,\n",
       " 'Prescription': 544,\n",
       " 'given': 545,\n",
       " 'patient': 546,\n",
       " 'Preventative': 547,\n",
       " 'Instructions': 548,\n",
       " 'knee': 549,\n",
       " 'injury': 550,\n",
       " 'David': 551,\n",
       " 'Bruce': 552,\n",
       " 'Identiﬁer': 553,\n",
       " 'June': 554,\n",
       " 'Explanation': 555,\n",
       " 'Gap': 556,\n",
       " 'no': 557,\n",
       " 'concussion': 558,\n",
       " 'Assistant': 559,\n",
       " 'devin': 560,\n",
       " 'conrad': 561,\n",
       " 'September': 562,\n",
       " 'ACCIDENT': 563,\n",
       " 'CLAIM': 564,\n",
       " 'FORM': 565,\n",
       " 'ATTENDING': 566,\n",
       " 'PHYSICIAN': 567,\n",
       " 'PLEASE': 568,\n",
       " 'PRINT': 569,\n",
       " 'PART': 570,\n",
       " 'I': 571,\n",
       " 'BE': 572,\n",
       " 'COMPLETED': 573,\n",
       " 'BY': 574,\n",
       " 'ICD': 575,\n",
       " 'first': 576,\n",
       " 'unable': 577,\n",
       " 'work': 578,\n",
       " 'Expected': 579,\n",
       " 'Delivery': 580,\n",
       " 'Actual': 581,\n",
       " 'Unable': 582,\n",
       " 'Vaginal': 583,\n",
       " 'per': 584,\n",
       " 'Continued': 585,\n",
       " 'Facility': 586,\n",
       " 'State': 587,\n",
       " 'Zip': 588,\n",
       " 'Performed': 589,\n",
       " 'Surgical': 590,\n",
       " 'CPT': 591,\n",
       " 'Attending': 592,\n",
       " 'Degree': 593,\n",
       " 'A': 594,\n",
       " 'check': 595,\n",
       " 'type': 596,\n",
       " 'claim': 597,\n",
       " 'filing': 598,\n",
       " 'B': 599,\n",
       " 'Suffix': 600,\n",
       " 'MI': 601,\n",
       " 'Spanish': 602,\n",
       " 'Short': 603,\n",
       " 'Term': 604,\n",
       " 'Disability': 605,\n",
       " 'Long': 606,\n",
       " 'Life': 607,\n",
       " 'Insurance': 608,\n",
       " 'Voluntary': 609,\n",
       " 'Was': 610,\n",
       " 'this': 611,\n",
       " 'motor': 612,\n",
       " 'vehicle': 613,\n",
       " 'accident': 614,\n",
       " 'Physicians': 615,\n",
       " 'Hospitals': 616,\n",
       " 'Considerations': 617,\n",
       " 'Folder': 618,\n",
       " 'Contents': 619,\n",
       " 'Claimant': 620,\n",
       " 'Unauthorized': 621,\n",
       " 'access': 622,\n",
       " 'is': 623,\n",
       " 'strictly': 624,\n",
       " 'probihited': 625,\n",
       " 'Male': 626,\n",
       " 'Reach': 627,\n",
       " 'Return': 628,\n",
       " 'Sprained': 629,\n",
       " 'Ankle': 630,\n",
       " 'Practitioner': 631,\n",
       " 'Monica': 632,\n",
       " 'Shaffer': 633,\n",
       " 'Johnson': 634,\n",
       " 'Ave': 635,\n",
       " 'Bridgeport': 636,\n",
       " 'WV': 637,\n",
       " 'US': 638,\n",
       " 'Accountability': 639,\n",
       " 'Act': 640,\n",
       " 'HIPAA': 641,\n",
       " 'Privacy': 642,\n",
       " 'Rule': 643,\n",
       " 'banks': 644,\n",
       " 'governmental': 645,\n",
       " 'entities': 646,\n",
       " 'communicable': 647,\n",
       " 'disease': 648,\n",
       " 'CL': 649,\n",
       " 'GREGORY': 650,\n",
       " 'we': 651,\n",
       " 'can': 652,\n",
       " 'assist': 653,\n",
       " 'prohibited': 654,\n",
       " 'otherwise': 655,\n",
       " 'permitted': 656,\n",
       " 'law': 657,\n",
       " 'EMS': 658,\n",
       " 'Christopher': 659,\n",
       " 'Bartruff': 660,\n",
       " 'health': 661,\n",
       " 'park': 662,\n",
       " 'blvd': 663,\n",
       " 'Naples': 664,\n",
       " 'FL': 665,\n",
       " 'NCH': 666,\n",
       " 'Emergency': 667,\n",
       " 'Healthcare': 668,\n",
       " 'System': 669,\n",
       " 'Napies': 670,\n",
       " 'North': 671,\n",
       " 'Collier': 672,\n",
       " 'Northeast': 673,\n",
       " 'ED': 674,\n",
       " 'Chief': 675,\n",
       " 'ICD=CC': 676,\n",
       " 'Numbers': 677,\n",
       " 'Feeling': 678,\n",
       " 'Suicidal': 679,\n",
       " 'Help': 680,\n",
       " 'With': 681,\n",
       " 'Where': 682,\n",
       " 'When': 683,\n",
       " 'Comments': 684,\n",
       " 'Dear': 685,\n",
       " 'However': 686,\n",
       " 'Estimado': 687,\n",
       " 'Paciente': 688,\n",
       " 'has': 689,\n",
       " 'been': 690,\n",
       " 'these': 691,\n",
       " 'instructions': 692,\n",
       " 'HOWEVER': 693,\n",
       " 'Education': 694,\n",
       " 'Materials': 695,\n",
       " 'Peds': 696,\n",
       " 'Upper': 697,\n",
       " 'Extremity': 698,\n",
       " 'Contusion': 699,\n",
       " 'Home': 700,\n",
       " 'care': 701,\n",
       " 'any': 702,\n",
       " 'Special': 703,\n",
       " 'note': 704,\n",
       " 'parents': 705,\n",
       " 'seek': 706,\n",
       " 'advice': 707,\n",
       " 'Bruising': 708,\n",
       " 'that': 709,\n",
       " 'gets': 710,\n",
       " 'worse': 711,\n",
       " 'Numbness': 712,\n",
       " 'tingling': 713,\n",
       " 'injured': 714,\n",
       " 'arm': 715,\n",
       " 'Trauma': 716,\n",
       " 'Watch': 717,\n",
       " 'following': 718,\n",
       " 'symptoms': 719,\n",
       " 'Headache': 720,\n",
       " 'Nausea': 721,\n",
       " 'vomiting': 722,\n",
       " 'Sensitivity': 723,\n",
       " 'light': 724,\n",
       " 'noise': 725,\n",
       " 'Unusual': 726,\n",
       " 'sleepiness': 727,\n",
       " 'grogginess': 728,\n",
       " 'Trouble': 729,\n",
       " 'falling': 730,\n",
       " 'asleep': 731,\n",
       " 'Personality': 732,\n",
       " 'changes': 733,\n",
       " 'Vision': 734,\n",
       " 'Memory': 735,\n",
       " 'loss': 736,\n",
       " 'Confusion': 737,\n",
       " 'walking': 738,\n",
       " 'clumsiness': 739,\n",
       " 'Loss': 740,\n",
       " 'consciousness': 741,\n",
       " 'even': 742,\n",
       " 'short': 743,\n",
       " 'time': 744,\n",
       " 'Inability': 745,\n",
       " 'awakened': 746,\n",
       " 'Stiff': 747,\n",
       " 'neck': 748,\n",
       " 'Weakness': 749,\n",
       " 'numbness': 750,\n",
       " 'part': 751,\n",
       " 'body': 752,\n",
       " 'Seizures': 753,\n",
       " 'General': 754,\n",
       " 'Avoid': 755,\n",
       " 'lifting': 756,\n",
       " 'strenuous': 757,\n",
       " 'activities': 758,\n",
       " 'Pain': 759,\n",
       " 'doesn': 760,\n",
       " 't': 761,\n",
       " 'get': 762,\n",
       " 'better': 763,\n",
       " 'worsens': 764,\n",
       " 'increased': 765,\n",
       " 'swelling': 766,\n",
       " 'bruising': 767,\n",
       " 'Sick': 768,\n",
       " 'appearance': 769,\n",
       " 'behaviors': 770,\n",
       " 'worry': 771,\n",
       " 'Excuse': 772,\n",
       " 'From': 773,\n",
       " 'School': 774,\n",
       " 'excuse': 775,\n",
       " 'from': 776,\n",
       " 'until': 777,\n",
       " 'Caregiver': 778,\n",
       " 'Document': 779,\n",
       " 'Released': 780,\n",
       " 'reminders': 781,\n",
       " 'regarding': 782,\n",
       " 'prescriptions': 783,\n",
       " 'provided': 784,\n",
       " 'leaflets': 785,\n",
       " 'COLLIER': 786,\n",
       " 'COUNTRY': 787,\n",
       " 'DIGITECH': 788,\n",
       " 'COMPUTER': 789,\n",
       " 'BILLING': 790,\n",
       " 'ON': 791,\n",
       " 'BEHALF': 792,\n",
       " 'BEDFORD': 793,\n",
       " 'RD': 794,\n",
       " 'BLDG': 795,\n",
       " 'FLOOR': 796,\n",
       " 'CHAPPAQUA': 797,\n",
       " 'NY': 798,\n",
       " 'VISIT': 799,\n",
       " 'HTTPS': 800,\n",
       " 'PAY': 801,\n",
       " 'THIS': 802,\n",
       " 'INVOICE': 803,\n",
       " 'COUNTY': 804,\n",
       " 'NAPLES': 805,\n",
       " 'N': 806,\n",
       " 'NORTH': 807,\n",
       " 'HOSPITAL': 808,\n",
       " 'PARK': 809,\n",
       " 'BLVD': 810,\n",
       " 'BAKER': 811,\n",
       " 'MARCO': 812,\n",
       " 'HEALTHCARE': 813,\n",
       " 'NORTHEAST': 814,\n",
       " 'EMERGENCY': 815,\n",
       " 'DEPARTMENT': 816,\n",
       " 'Lockbox': 817,\n",
       " 'Processing': 818,\n",
       " 'Atlanta': 819,\n",
       " 'GA': 820,\n",
       " 'SYSTEM': 821,\n",
       " 'FOR': 822,\n",
       " 'AADC': 823,\n",
       " 'RE': 824,\n",
       " 'thirty': 825,\n",
       " 'days': 826,\n",
       " 'receipt': 827,\n",
       " 'letter': 828,\n",
       " 'Accounting': 829,\n",
       " 'SOUTHWEST': 830,\n",
       " 'FLORIDA': 831,\n",
       " 'MANAGEMENT': 832,\n",
       " 'CINCINNATI': 833,\n",
       " 'OH': 834,\n",
       " 'PHONE': 835,\n",
       " 'TH_AR_LTR': 836,\n",
       " 'SERVICE': 837,\n",
       " 'NUMBER': 838,\n",
       " 'JASON': 839,\n",
       " 'SANTANA': 840,\n",
       " 'Sincerely': 841,\n",
       " 'PLANTATION': 842,\n",
       " 'Allstate': 843,\n",
       " 'Youre': 844,\n",
       " 'good': 845,\n",
       " 'hands': 846,\n",
       " 'Florida': 847,\n",
       " 'PIP': 848,\n",
       " 'Central': 849,\n",
       " 'CLINTON': 850,\n",
       " 'IA': 851,\n",
       " 'February': 852,\n",
       " 'INSURED': 853,\n",
       " 'LOSS': 854,\n",
       " 'FAX': 855,\n",
       " 'OFFICE': 856,\n",
       " 'HOURS': 857,\n",
       " 'VYPHAPHONE': 858,\n",
       " 'INTHALANGSY': 859,\n",
       " 'EXT': 860,\n",
       " \"'re\": 861,\n",
       " 'Jacksonville': 862,\n",
       " 'DALLAS': 863,\n",
       " 'TX': 864,\n",
       " 'Some': 865,\n",
       " 'specifics': 866,\n",
       " 'request': 867,\n",
       " 'sincerely': 868,\n",
       " 'CAITLEN': 869,\n",
       " 'CARROLL': 870,\n",
       " 'Ext': 871,\n",
       " 'date': 872,\n",
       " 'name': 873,\n",
       " 'Cellular': 874,\n",
       " 'MedSupport': 875,\n",
       " 'policy': 876,\n",
       " 'policies': 877,\n",
       " 'Female': 878,\n",
       " 'Domestic': 879,\n",
       " 'Partner': 880,\n",
       " 'information': 881,\n",
       " 'Condition': 882,\n",
       " 'am': 883,\n",
       " 'pm': 884,\n",
       " 'Confinement': 885,\n",
       " 'Dates': 886,\n",
       " 'Teri': 887,\n",
       " 'Willochell': 888,\n",
       " 'phycian': 889,\n",
       " 'considerations': 890,\n",
       " 'each': 891,\n",
       " 'such': 892,\n",
       " 'violation': 893,\n",
       " 'number': 894,\n",
       " 'indicated': 895,\n",
       " 'above': 896,\n",
       " 'My': 897,\n",
       " 'Member': 898,\n",
       " 'Relationship': 899,\n",
       " 'person': 900,\n",
       " 'granting': 901,\n",
       " 'authority': 902,\n",
       " 'birth': 903,\n",
       " 'Including': 904,\n",
       " 'L': 905,\n",
       " 'lesser': 906,\n",
       " 'If': 907,\n",
       " 'yes': 908,\n",
       " 'please': 909,\n",
       " 'provide': 910,\n",
       " 'tha': 911,\n",
       " 'diagnosis': 912,\n",
       " 'Treatment': 913,\n",
       " 'advise': 914,\n",
       " 'stop': 915,\n",
       " 'working': 916,\n",
       " 'what': 917,\n",
       " 'Willochel': 918,\n",
       " 'Specially': 919,\n",
       " 'internal': 920,\n",
       " 'medicine': 921,\n",
       " 'MD': 922,\n",
       " 'MedExpress': 923,\n",
       " 'Urgent': 924,\n",
       " 'Care': 925,\n",
       " 'Route': 926,\n",
       " 'HIPPA': 927,\n",
       " 'Socia': 928,\n",
       " 'Location': 929,\n",
       " 'Norwin': 930,\n",
       " 'Huntingdon': 931,\n",
       " 'Holder': 932,\n",
       " 'Sex': 933,\n",
       " 'COMP': 934,\n",
       " 'Clinical': 935,\n",
       " 'Report': 936,\n",
       " 'BP': 937,\n",
       " 'mmHg': 938,\n",
       " 'PULSE': 939,\n",
       " 'bpm': 940,\n",
       " 'RESP': 941,\n",
       " 'TEMP': 942,\n",
       " 'WEIGTH': 943,\n",
       " 'ft': 944,\n",
       " 'BMI': 945,\n",
       " 'LMP': 946,\n",
       " 'PMP': 947,\n",
       " 'SAT': 948,\n",
       " 'CONTUSION': 949,\n",
       " 'Meds': 950,\n",
       " 'ACTIVE': 951,\n",
       " 'acetaminophen': 952,\n",
       " 'albuterol': 953,\n",
       " 'bulk': 954,\n",
       " 'Dilantin': 955,\n",
       " 'gabapentin': 956,\n",
       " 'Humalog': 957,\n",
       " 'Lamictal': 958,\n",
       " 'Lyrica': 959,\n",
       " 'Neurontin': 960,\n",
       " 'valacyclovir': 961,\n",
       " 'Procedures': 962,\n",
       " 'FOOT': 963,\n",
       " 'MIN': 964,\n",
       " 'THREE': 965,\n",
       " 'ESTAB': 966,\n",
       " 'URGENT': 967,\n",
       " 'CARE': 968,\n",
       " 'Pocket': 969,\n",
       " 'Met': 970,\n",
       " 'Estimated': 971,\n",
       " 'charges': 972,\n",
       " 'MSO': 973,\n",
       " 'LLC': 974,\n",
       " 'Med': 975,\n",
       " 'Express': 976,\n",
       " 'UC': 977,\n",
       " 'HUNINGTON': 978,\n",
       " 'Merchant': 979,\n",
       " 'Transaction': 980,\n",
       " 'PURCHASE': 981,\n",
       " 'Approval': 982,\n",
       " 'code': 983,\n",
       " 'Record': 984,\n",
       " 'Visa': 985,\n",
       " 'Trace': 986,\n",
       " 'reference': 987,\n",
       " 'Cardholder': 988,\n",
       " 'identifier': 989,\n",
       " 'Application': 990,\n",
       " 'label': 991,\n",
       " 'TVR': 992,\n",
       " 'AID': 993,\n",
       " 'Subtotal': 994,\n",
       " 'Sales': 995,\n",
       " 'customer': 996,\n",
       " 'copy': 997,\n",
       " 'send': 998,\n",
       " 'payments': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'UNK',\n",
       " 1: ' ',\n",
       " 2: '\\t',\n",
       " 3: '\\n',\n",
       " 4: 'Claim',\n",
       " 5: 'Type',\n",
       " 6: 'VB',\n",
       " 7: 'Accident',\n",
       " 8: 'Accidental',\n",
       " 9: 'Injury',\n",
       " 10: 'Information',\n",
       " 11: 'First',\n",
       " 12: 'Name',\n",
       " 13: 'Middle',\n",
       " 14: 'Last',\n",
       " 15: 'Social',\n",
       " 16: 'Security',\n",
       " 17: 'Number',\n",
       " 18: 'Birth',\n",
       " 19: 'Date',\n",
       " 20: 'Gender',\n",
       " 21: 'Language',\n",
       " 22: 'Preference',\n",
       " 23: 'Address',\n",
       " 24: 'Line',\n",
       " 25: 'City',\n",
       " 26: 'Postal',\n",
       " 27: 'Code',\n",
       " 28: 'Country',\n",
       " 29: 'Best',\n",
       " 30: 'Phone',\n",
       " 31: 'to',\n",
       " 32: 'be',\n",
       " 33: 'Reached',\n",
       " 34: 'During',\n",
       " 35: 'the',\n",
       " 36: 'Day',\n",
       " 37: 'Email',\n",
       " 38: 'Page',\n",
       " 39: 'of',\n",
       " 40: 'RADIOLOGY',\n",
       " 41: 'REPORT',\n",
       " 42: 'Patient',\n",
       " 43: 'MRN',\n",
       " 44: 'Accession',\n",
       " 45: 'No',\n",
       " 46: 'Ref',\n",
       " 47: 'Physician',\n",
       " 48: 'UNKNOWN',\n",
       " 49: 'Study',\n",
       " 50: 'Hospital',\n",
       " 51: 'DOB',\n",
       " 52: 'Technique',\n",
       " 53: 'views',\n",
       " 54: 'left',\n",
       " 55: 'wrist',\n",
       " 56: 'Cormarison',\n",
       " 57: 'None',\n",
       " 58: 'availabie',\n",
       " 59: 'Comparison',\n",
       " 60: 'available',\n",
       " 61: 'FINDINGS',\n",
       " 62: 'IMPRESSION',\n",
       " 63: 'acute',\n",
       " 64: 'osseous',\n",
       " 65: 'abnormality',\n",
       " 66: 'identified',\n",
       " 67: 'Daytime',\n",
       " 68: 'Event',\n",
       " 69: 'Stopped',\n",
       " 70: 'Working',\n",
       " 71: 'Yes',\n",
       " 72: 'Physically',\n",
       " 73: 'at',\n",
       " 74: 'Work',\n",
       " 75: 'Hours',\n",
       " 76: 'Worked',\n",
       " 77: 'on',\n",
       " 78: 'Scheduled',\n",
       " 79: 'Missed',\n",
       " 80: 'Returned',\n",
       " 81: 'Related',\n",
       " 82: 'Time',\n",
       " 83: 'Diagnosis',\n",
       " 84: 'Arthiscopic',\n",
       " 85: 'surgery',\n",
       " 86: 'Surgery',\n",
       " 87: 'Is',\n",
       " 88: 'Required',\n",
       " 89: 'Indicator',\n",
       " 90: 'Outpatient',\n",
       " 91: 'Medical',\n",
       " 92: 'Provider',\n",
       " 93: 'Roles',\n",
       " 94: 'Treating',\n",
       " 95: 'Patrick',\n",
       " 96: 'Emerson',\n",
       " 97: 'Business',\n",
       " 98: 'Telephone',\n",
       " 99: 'Fax',\n",
       " 100: 'Visit',\n",
       " 101: 'Next',\n",
       " 102: 'Hospitalization',\n",
       " 103: 'Discharge',\n",
       " 104: 'Procedure',\n",
       " 105: 'Left',\n",
       " 106: 'arthiscopic',\n",
       " 107: 'Employment',\n",
       " 108: 'Employer',\n",
       " 109: 'Policy',\n",
       " 110: 'Electronic',\n",
       " 111: 'Submission',\n",
       " 112: 'Identifier',\n",
       " 113: 'Electronically',\n",
       " 114: 'Signed',\n",
       " 115: 'Fraud',\n",
       " 116: 'Statements',\n",
       " 117: 'Reviewed',\n",
       " 118: 'and',\n",
       " 119: 'unum',\n",
       " 120: 'The',\n",
       " 121: 'Benefits',\n",
       " 122: 'Center',\n",
       " 123: 'Not',\n",
       " 124: 'for',\n",
       " 125: 'FMLA',\n",
       " 126: 'Requests',\n",
       " 127: 'Insured',\n",
       " 128: '’',\n",
       " 129: 's',\n",
       " 130: 'Signature',\n",
       " 131: 'Printed',\n",
       " 132: 'Unum',\n",
       " 133: 'Confirmation',\n",
       " 134: 'Coverage',\n",
       " 135: 'Group',\n",
       " 136: 'Customer',\n",
       " 137: 'EE',\n",
       " 138: 'Effective',\n",
       " 139: 'Employee',\n",
       " 140: 'Acc',\n",
       " 141: 'January',\n",
       " 142: 'Wellness',\n",
       " 143: 'Benefit',\n",
       " 144: 'Total',\n",
       " 145: 'Monthly',\n",
       " 146: 'Premium',\n",
       " 147: 'Montly',\n",
       " 148: 'Payroll',\n",
       " 149: 'Deduction',\n",
       " 150: 'Account',\n",
       " 151: 'F',\n",
       " 152: 'Exam',\n",
       " 153: 'Referring',\n",
       " 154: 'Phys',\n",
       " 155: 'STEPHEN',\n",
       " 156: 'GELOVICH',\n",
       " 157: 'Tax',\n",
       " 158: 'MRI',\n",
       " 159: 'OF',\n",
       " 160: 'THE',\n",
       " 161: 'LEFT',\n",
       " 162: 'WRIST',\n",
       " 163: 'WITHOUT',\n",
       " 164: 'CONTRAST',\n",
       " 165: 'COMPARISON',\n",
       " 166: 'These',\n",
       " 167: 'results',\n",
       " 168: 'were',\n",
       " 169: 'faxed',\n",
       " 170: 'Gelovich',\n",
       " 171: 'signed',\n",
       " 172: 'by',\n",
       " 173: 'Stephen',\n",
       " 174: 'Bravo',\n",
       " 175: 'Dependent',\n",
       " 176: 'Detail',\n",
       " 177: 'Zachary',\n",
       " 178: 'Jager',\n",
       " 179: 'Billed',\n",
       " 180: 'Amounts',\n",
       " 181: 'Contract',\n",
       " 182: 'Adjustment',\n",
       " 183: 'Allowed',\n",
       " 184: 'Amount',\n",
       " 185: 'Covered',\n",
       " 186: 'Reason',\n",
       " 187: 'Deductible',\n",
       " 188: 'Other',\n",
       " 189: 'Carrier',\n",
       " 190: 'Paid',\n",
       " 191: 'Responsibility',\n",
       " 192: 'Totals',\n",
       " 193: 'For',\n",
       " 194: 'Appeals',\n",
       " 195: 'Rights',\n",
       " 196: 'Important',\n",
       " 197: 'about',\n",
       " 198: 'Your',\n",
       " 199: 'Appeal',\n",
       " 200: 'All',\n",
       " 201: 'Languages',\n",
       " 202: 'Contact',\n",
       " 203: 'Did',\n",
       " 204: 'You',\n",
       " 205: 'Know',\n",
       " 206: 'Specialty',\n",
       " 207: 'Orthopedic',\n",
       " 208: 'Surgeon',\n",
       " 209: 'Unknown',\n",
       " 210: 'Kari',\n",
       " 211: 'Lund',\n",
       " 212: 'Orthopedist',\n",
       " 213: 'Dan',\n",
       " 214: 'Palmer',\n",
       " 215: 'On',\n",
       " 216: '&',\n",
       " 217: 'May',\n",
       " 218: 'Spouse',\n",
       " 219: 'Child',\n",
       " 220: 'BLACK',\n",
       " 221: 'HILLS',\n",
       " 222: 'ORTHOPEDIC',\n",
       " 223: 'CENTER',\n",
       " 224: 'PC',\n",
       " 225: 'LAST',\n",
       " 226: 'PMT',\n",
       " 227: 'AMOUNT',\n",
       " 228: 'DUE',\n",
       " 229: 'DATE',\n",
       " 230: 'PAGE',\n",
       " 231: 'STATEMENT',\n",
       " 232: 'Ins',\n",
       " 233: 'Description',\n",
       " 234: 'E',\n",
       " 235: 'm',\n",
       " 236: 'New',\n",
       " 237: 'Moderat',\n",
       " 238: 'S',\n",
       " 239: 'Clo',\n",
       " 240: 'Tx',\n",
       " 241: 'Phalangealfx',\n",
       " 242: 'Finger',\n",
       " 243: 'Splint',\n",
       " 244: 'Offic',\n",
       " 245: 'Cons',\n",
       " 246: 'Moderate',\n",
       " 247: 'Sever',\n",
       " 248: 'Rad',\n",
       " 249: 'Mini',\n",
       " 250: 'Views',\n",
       " 251: 'Applic',\n",
       " 252: 'Hand',\n",
       " 253: 'Lower',\n",
       " 254: 'Forearm',\n",
       " 255: 'Fiberglass',\n",
       " 256: 'gauntlet',\n",
       " 257: 'Cast',\n",
       " 258: 'Yrs',\n",
       " 259: 'Charge',\n",
       " 260: 'Pmt',\n",
       " 261: 'Pat',\n",
       " 262: 'Adjust',\n",
       " 263: 'Current',\n",
       " 264: 'Days',\n",
       " 265: 'Balance',\n",
       " 266: 'Pending',\n",
       " 267: 'Now',\n",
       " 268: 'Due',\n",
       " 269: 'Message',\n",
       " 270: 'Make',\n",
       " 271: 'Checks',\n",
       " 272: 'Payable',\n",
       " 273: 'To',\n",
       " 274: 'Statement',\n",
       " 275: 'Billing',\n",
       " 276: 'Questions',\n",
       " 277: 'Choice',\n",
       " 278: 'Health',\n",
       " 279: 'Administrators',\n",
       " 280: 'Forwarding',\n",
       " 281: 'Service',\n",
       " 282: 'Requested',\n",
       " 283: 'REGIONAL',\n",
       " 284: 'HEALTH',\n",
       " 285: 'INC',\n",
       " 286: 'Participant',\n",
       " 287: 'ID',\n",
       " 288: 'Original',\n",
       " 289: 'Print',\n",
       " 290: 'Website',\n",
       " 291: 'DEA',\n",
       " 292: 'Individual',\n",
       " 293: 'Summary',\n",
       " 294: 'By',\n",
       " 295: 'Plan',\n",
       " 296: 'Status',\n",
       " 297: 'Period',\n",
       " 298: 'DEDUCTIBLE',\n",
       " 299: 'OUT',\n",
       " 300: 'POCKET',\n",
       " 301: 'Regional',\n",
       " 302: 'Inc',\n",
       " 303: 'EXPLANATION',\n",
       " 304: 'BENEFITS',\n",
       " 305: 'Retain',\n",
       " 306: 'Purposes',\n",
       " 307: 'Sign',\n",
       " 308: 'up',\n",
       " 309: 'paperless',\n",
       " 310: 'Family',\n",
       " 311: 'Out',\n",
       " 312: 'Network',\n",
       " 313: 'Karl',\n",
       " 314: 'Services',\n",
       " 315: 'exam',\n",
       " 316: 'hand',\n",
       " 317: 'Modifiers',\n",
       " 318: 'TC',\n",
       " 319: 'RT',\n",
       " 320: 'This',\n",
       " 321: 'Qualified',\n",
       " 322: 'sign',\n",
       " 323: 'language',\n",
       " 324: 'interpreters',\n",
       " 325: 'written',\n",
       " 326: 'in',\n",
       " 327: 'other',\n",
       " 328: 'languages',\n",
       " 329: 'Jacquelin',\n",
       " 330: 'Brainard',\n",
       " 331: 'Compliance',\n",
       " 332: 'Officer',\n",
       " 333: 'or',\n",
       " 334: 'mail',\n",
       " 335: 'phone',\n",
       " 336: 'Department',\n",
       " 337: 'Human',\n",
       " 338: 'Complaint',\n",
       " 339: 'forms',\n",
       " 340: 'are',\n",
       " 341: 'DAKOTA',\n",
       " 342: 'EZ',\n",
       " 343: 'Ways',\n",
       " 344: 'Pay',\n",
       " 345: 'Automated',\n",
       " 346: 'Attendant',\n",
       " 347: 'hours',\n",
       " 348: 'a',\n",
       " 349: 'day',\n",
       " 350: 'Payments',\n",
       " 351: 'Please',\n",
       " 352: 'Call',\n",
       " 353: 'Upon',\n",
       " 354: 'Receipt',\n",
       " 355: 'Improved',\n",
       " 356: 'Online',\n",
       " 357: 'Experience',\n",
       " 358: '|',\n",
       " 359: 'Update',\n",
       " 360: 'Info',\n",
       " 361: 'About',\n",
       " 362: 'See',\n",
       " 363: 'Details',\n",
       " 364: 'Back',\n",
       " 365: 'ACCOUNT',\n",
       " 366: 'NO',\n",
       " 367: 'SHOW',\n",
       " 368: 'PAID',\n",
       " 369: 'HERE',\n",
       " 370: 'MAKE',\n",
       " 371: 'CHECKS',\n",
       " 372: 'TO',\n",
       " 373: 'PROC',\n",
       " 374: 'CODE',\n",
       " 375: 'UNITS',\n",
       " 376: 'DETAILS',\n",
       " 377: 'SERVICES',\n",
       " 378: 'CHARGES',\n",
       " 379: 'INSUR',\n",
       " 380: 'PENDING',\n",
       " 381: 'PATIENT',\n",
       " 382: 'BALANCE',\n",
       " 383: 'EXAM',\n",
       " 384: 'HAND',\n",
       " 385: 'THORAC',\n",
       " 386: 'SPINE',\n",
       " 387: 'COMMERCIAL',\n",
       " 388: 'NON',\n",
       " 389: 'ALLOWED',\n",
       " 390: 'CT',\n",
       " 391: 'ABD',\n",
       " 392: 'PELV',\n",
       " 393: 'PAYMENT',\n",
       " 394: 'CHEST',\n",
       " 395: 'VIEWS',\n",
       " 396: 'HARGES',\n",
       " 397: 'Over',\n",
       " 398: 'DIGIT',\n",
       " 399: 'Of',\n",
       " 400: 'Today',\n",
       " 401: \"'s\",\n",
       " 402: 'Ethnicity',\n",
       " 403: 'Hispanic',\n",
       " 404: 'Latino',\n",
       " 405: 'Preferred',\n",
       " 406: 'English',\n",
       " 407: 'visit',\n",
       " 408: 'with',\n",
       " 409: 'Suzanne',\n",
       " 410: 'Newsom',\n",
       " 411: 'CNP',\n",
       " 412: '•',\n",
       " 413: 'Lethargy',\n",
       " 414: 'cough',\n",
       " 415: 'Vitals',\n",
       " 416: 'lbs',\n",
       " 417: 'kg',\n",
       " 418: 'Wt',\n",
       " 419: 'Temp',\n",
       " 420: 'HR',\n",
       " 421: 'Oxygen',\n",
       " 422: 'sat',\n",
       " 423: 'Allergies',\n",
       " 424: 'Amoxicillin',\n",
       " 425: 'rash',\n",
       " 426: 'possible',\n",
       " 427: 'hives',\n",
       " 428: 'Active',\n",
       " 429: 'Diagnoses',\n",
       " 430: 'Include',\n",
       " 431: 'Acute',\n",
       " 432: 'frontal',\n",
       " 433: 'sinusitis',\n",
       " 434: 'unspecified',\n",
       " 435: 'Dizziness',\n",
       " 436: 'giddiness',\n",
       " 437: 'Medication',\n",
       " 438: 'List',\n",
       " 439: 'medications',\n",
       " 440: 'you',\n",
       " 441: 'Taking',\n",
       " 442: 'Zyrtec',\n",
       " 443: 'Childrens',\n",
       " 444: 'Allergy',\n",
       " 445: 'Notes',\n",
       " 446: 'Tests',\n",
       " 447: 'Labs',\n",
       " 448: 'Illumigene',\n",
       " 449: 'MYCO',\n",
       " 450: 'http',\n",
       " 451: 'BASIC',\n",
       " 452: 'METABOLIC',\n",
       " 453: 'SODIUM',\n",
       " 454: 'Range',\n",
       " 455: 'POTASSIUM',\n",
       " 456: 'CHLORIDE',\n",
       " 457: 'GLUCOSE',\n",
       " 458: 'BUN',\n",
       " 459: 'CREATININE',\n",
       " 460: 'CALCIUM',\n",
       " 461: 'CREA',\n",
       " 462: 'RATIO',\n",
       " 463: 'Ratio',\n",
       " 464: 'ANION',\n",
       " 465: 'GAP',\n",
       " 466: 'Calc',\n",
       " 467: 'CBC',\n",
       " 468: 'DIFF',\n",
       " 469: 'WBC',\n",
       " 470: 'RBC',\n",
       " 471: 'HGB',\n",
       " 472: 'HCT',\n",
       " 473: 'MCV',\n",
       " 474: 'fL',\n",
       " 475: 'MCH',\n",
       " 476: 'pg',\n",
       " 477: 'MCHC',\n",
       " 478: 'MPV',\n",
       " 479: 'PLATELETS',\n",
       " 480: 'NEUTROPHILS',\n",
       " 481: 'LYMPHOCYTES',\n",
       " 482: 'MONOCYTES',\n",
       " 483: 'Conditions',\n",
       " 484: 'Problem',\n",
       " 485: 'Idiopathic',\n",
       " 486: 'urticaria',\n",
       " 487: 'document',\n",
       " 488: 'wish',\n",
       " 489: 'keep',\n",
       " 490: 'Policyholder',\n",
       " 491: 'Owner',\n",
       " 492: 'Eastside',\n",
       " 493: 'Acct',\n",
       " 494: 'Jasminder',\n",
       " 495: 'Singh',\n",
       " 496: 'Dev',\n",
       " 497: 'PA',\n",
       " 498: 'EXCUSE',\n",
       " 499: 'east',\n",
       " 500: 'side',\n",
       " 501: 'medical',\n",
       " 502: 'center',\n",
       " 503: 'April',\n",
       " 504: 'Weekly',\n",
       " 505: 'MEDICAL',\n",
       " 506: 'Ph',\n",
       " 507: 'MR',\n",
       " 508: 'Primary',\n",
       " 509: 'Thoracic',\n",
       " 510: 'Strain',\n",
       " 511: 'have',\n",
       " 512: 'strained',\n",
       " 513: 'your',\n",
       " 514: 'thoracic',\n",
       " 515: 'spine',\n",
       " 516: 'IF',\n",
       " 517: 'ANY',\n",
       " 518: 'FOLLOWING',\n",
       " 519: 'OCCURS',\n",
       " 520: 'feel',\n",
       " 521: 'weakness',\n",
       " 522: 'arms',\n",
       " 523: 'legs',\n",
       " 524: 'severe',\n",
       " 525: 'increase',\n",
       " 526: 'pain',\n",
       " 527: 'Lumbosacral',\n",
       " 528: 'weak',\n",
       " 529: 'becomes',\n",
       " 530: 'more',\n",
       " 531: 'Follow',\n",
       " 532: 'Up',\n",
       " 533: 'What',\n",
       " 534: 'Do',\n",
       " 535: 'Take',\n",
       " 536: 'all',\n",
       " 537: 'as',\n",
       " 538: 'directed',\n",
       " 539: 'Additional',\n",
       " 540: 'Prescriptions',\n",
       " 541: 'Written',\n",
       " 542: 'Prescriber',\n",
       " 543: 'Paper',\n",
       " 544: 'Prescription',\n",
       " 545: 'given',\n",
       " 546: 'patient',\n",
       " 547: 'Preventative',\n",
       " 548: 'Instructions',\n",
       " 549: 'knee',\n",
       " 550: 'injury',\n",
       " 551: 'David',\n",
       " 552: 'Bruce',\n",
       " 553: 'Identiﬁer',\n",
       " 554: 'June',\n",
       " 555: 'Explanation',\n",
       " 556: 'Gap',\n",
       " 557: 'no',\n",
       " 558: 'concussion',\n",
       " 559: 'Assistant',\n",
       " 560: 'devin',\n",
       " 561: 'conrad',\n",
       " 562: 'September',\n",
       " 563: 'ACCIDENT',\n",
       " 564: 'CLAIM',\n",
       " 565: 'FORM',\n",
       " 566: 'ATTENDING',\n",
       " 567: 'PHYSICIAN',\n",
       " 568: 'PLEASE',\n",
       " 569: 'PRINT',\n",
       " 570: 'PART',\n",
       " 571: 'I',\n",
       " 572: 'BE',\n",
       " 573: 'COMPLETED',\n",
       " 574: 'BY',\n",
       " 575: 'ICD',\n",
       " 576: 'first',\n",
       " 577: 'unable',\n",
       " 578: 'work',\n",
       " 579: 'Expected',\n",
       " 580: 'Delivery',\n",
       " 581: 'Actual',\n",
       " 582: 'Unable',\n",
       " 583: 'Vaginal',\n",
       " 584: 'per',\n",
       " 585: 'Continued',\n",
       " 586: 'Facility',\n",
       " 587: 'State',\n",
       " 588: 'Zip',\n",
       " 589: 'Performed',\n",
       " 590: 'Surgical',\n",
       " 591: 'CPT',\n",
       " 592: 'Attending',\n",
       " 593: 'Degree',\n",
       " 594: 'A',\n",
       " 595: 'check',\n",
       " 596: 'type',\n",
       " 597: 'claim',\n",
       " 598: 'filing',\n",
       " 599: 'B',\n",
       " 600: 'Suffix',\n",
       " 601: 'MI',\n",
       " 602: 'Spanish',\n",
       " 603: 'Short',\n",
       " 604: 'Term',\n",
       " 605: 'Disability',\n",
       " 606: 'Long',\n",
       " 607: 'Life',\n",
       " 608: 'Insurance',\n",
       " 609: 'Voluntary',\n",
       " 610: 'Was',\n",
       " 611: 'this',\n",
       " 612: 'motor',\n",
       " 613: 'vehicle',\n",
       " 614: 'accident',\n",
       " 615: 'Physicians',\n",
       " 616: 'Hospitals',\n",
       " 617: 'Considerations',\n",
       " 618: 'Folder',\n",
       " 619: 'Contents',\n",
       " 620: 'Claimant',\n",
       " 621: 'Unauthorized',\n",
       " 622: 'access',\n",
       " 623: 'is',\n",
       " 624: 'strictly',\n",
       " 625: 'probihited',\n",
       " 626: 'Male',\n",
       " 627: 'Reach',\n",
       " 628: 'Return',\n",
       " 629: 'Sprained',\n",
       " 630: 'Ankle',\n",
       " 631: 'Practitioner',\n",
       " 632: 'Monica',\n",
       " 633: 'Shaffer',\n",
       " 634: 'Johnson',\n",
       " 635: 'Ave',\n",
       " 636: 'Bridgeport',\n",
       " 637: 'WV',\n",
       " 638: 'US',\n",
       " 639: 'Accountability',\n",
       " 640: 'Act',\n",
       " 641: 'HIPAA',\n",
       " 642: 'Privacy',\n",
       " 643: 'Rule',\n",
       " 644: 'banks',\n",
       " 645: 'governmental',\n",
       " 646: 'entities',\n",
       " 647: 'communicable',\n",
       " 648: 'disease',\n",
       " 649: 'CL',\n",
       " 650: 'GREGORY',\n",
       " 651: 'we',\n",
       " 652: 'can',\n",
       " 653: 'assist',\n",
       " 654: 'prohibited',\n",
       " 655: 'otherwise',\n",
       " 656: 'permitted',\n",
       " 657: 'law',\n",
       " 658: 'EMS',\n",
       " 659: 'Christopher',\n",
       " 660: 'Bartruff',\n",
       " 661: 'health',\n",
       " 662: 'park',\n",
       " 663: 'blvd',\n",
       " 664: 'Naples',\n",
       " 665: 'FL',\n",
       " 666: 'NCH',\n",
       " 667: 'Emergency',\n",
       " 668: 'Healthcare',\n",
       " 669: 'System',\n",
       " 670: 'Napies',\n",
       " 671: 'North',\n",
       " 672: 'Collier',\n",
       " 673: 'Northeast',\n",
       " 674: 'ED',\n",
       " 675: 'Chief',\n",
       " 676: 'ICD=CC',\n",
       " 677: 'Numbers',\n",
       " 678: 'Feeling',\n",
       " 679: 'Suicidal',\n",
       " 680: 'Help',\n",
       " 681: 'With',\n",
       " 682: 'Where',\n",
       " 683: 'When',\n",
       " 684: 'Comments',\n",
       " 685: 'Dear',\n",
       " 686: 'However',\n",
       " 687: 'Estimado',\n",
       " 688: 'Paciente',\n",
       " 689: 'has',\n",
       " 690: 'been',\n",
       " 691: 'these',\n",
       " 692: 'instructions',\n",
       " 693: 'HOWEVER',\n",
       " 694: 'Education',\n",
       " 695: 'Materials',\n",
       " 696: 'Peds',\n",
       " 697: 'Upper',\n",
       " 698: 'Extremity',\n",
       " 699: 'Contusion',\n",
       " 700: 'Home',\n",
       " 701: 'care',\n",
       " 702: 'any',\n",
       " 703: 'Special',\n",
       " 704: 'note',\n",
       " 705: 'parents',\n",
       " 706: 'seek',\n",
       " 707: 'advice',\n",
       " 708: 'Bruising',\n",
       " 709: 'that',\n",
       " 710: 'gets',\n",
       " 711: 'worse',\n",
       " 712: 'Numbness',\n",
       " 713: 'tingling',\n",
       " 714: 'injured',\n",
       " 715: 'arm',\n",
       " 716: 'Trauma',\n",
       " 717: 'Watch',\n",
       " 718: 'following',\n",
       " 719: 'symptoms',\n",
       " 720: 'Headache',\n",
       " 721: 'Nausea',\n",
       " 722: 'vomiting',\n",
       " 723: 'Sensitivity',\n",
       " 724: 'light',\n",
       " 725: 'noise',\n",
       " 726: 'Unusual',\n",
       " 727: 'sleepiness',\n",
       " 728: 'grogginess',\n",
       " 729: 'Trouble',\n",
       " 730: 'falling',\n",
       " 731: 'asleep',\n",
       " 732: 'Personality',\n",
       " 733: 'changes',\n",
       " 734: 'Vision',\n",
       " 735: 'Memory',\n",
       " 736: 'loss',\n",
       " 737: 'Confusion',\n",
       " 738: 'walking',\n",
       " 739: 'clumsiness',\n",
       " 740: 'Loss',\n",
       " 741: 'consciousness',\n",
       " 742: 'even',\n",
       " 743: 'short',\n",
       " 744: 'time',\n",
       " 745: 'Inability',\n",
       " 746: 'awakened',\n",
       " 747: 'Stiff',\n",
       " 748: 'neck',\n",
       " 749: 'Weakness',\n",
       " 750: 'numbness',\n",
       " 751: 'part',\n",
       " 752: 'body',\n",
       " 753: 'Seizures',\n",
       " 754: 'General',\n",
       " 755: 'Avoid',\n",
       " 756: 'lifting',\n",
       " 757: 'strenuous',\n",
       " 758: 'activities',\n",
       " 759: 'Pain',\n",
       " 760: 'doesn',\n",
       " 761: 't',\n",
       " 762: 'get',\n",
       " 763: 'better',\n",
       " 764: 'worsens',\n",
       " 765: 'increased',\n",
       " 766: 'swelling',\n",
       " 767: 'bruising',\n",
       " 768: 'Sick',\n",
       " 769: 'appearance',\n",
       " 770: 'behaviors',\n",
       " 771: 'worry',\n",
       " 772: 'Excuse',\n",
       " 773: 'From',\n",
       " 774: 'School',\n",
       " 775: 'excuse',\n",
       " 776: 'from',\n",
       " 777: 'until',\n",
       " 778: 'Caregiver',\n",
       " 779: 'Document',\n",
       " 780: 'Released',\n",
       " 781: 'reminders',\n",
       " 782: 'regarding',\n",
       " 783: 'prescriptions',\n",
       " 784: 'provided',\n",
       " 785: 'leaflets',\n",
       " 786: 'COLLIER',\n",
       " 787: 'COUNTRY',\n",
       " 788: 'DIGITECH',\n",
       " 789: 'COMPUTER',\n",
       " 790: 'BILLING',\n",
       " 791: 'ON',\n",
       " 792: 'BEHALF',\n",
       " 793: 'BEDFORD',\n",
       " 794: 'RD',\n",
       " 795: 'BLDG',\n",
       " 796: 'FLOOR',\n",
       " 797: 'CHAPPAQUA',\n",
       " 798: 'NY',\n",
       " 799: 'VISIT',\n",
       " 800: 'HTTPS',\n",
       " 801: 'PAY',\n",
       " 802: 'THIS',\n",
       " 803: 'INVOICE',\n",
       " 804: 'COUNTY',\n",
       " 805: 'NAPLES',\n",
       " 806: 'N',\n",
       " 807: 'NORTH',\n",
       " 808: 'HOSPITAL',\n",
       " 809: 'PARK',\n",
       " 810: 'BLVD',\n",
       " 811: 'BAKER',\n",
       " 812: 'MARCO',\n",
       " 813: 'HEALTHCARE',\n",
       " 814: 'NORTHEAST',\n",
       " 815: 'EMERGENCY',\n",
       " 816: 'DEPARTMENT',\n",
       " 817: 'Lockbox',\n",
       " 818: 'Processing',\n",
       " 819: 'Atlanta',\n",
       " 820: 'GA',\n",
       " 821: 'SYSTEM',\n",
       " 822: 'FOR',\n",
       " 823: 'AADC',\n",
       " 824: 'RE',\n",
       " 825: 'thirty',\n",
       " 826: 'days',\n",
       " 827: 'receipt',\n",
       " 828: 'letter',\n",
       " 829: 'Accounting',\n",
       " 830: 'SOUTHWEST',\n",
       " 831: 'FLORIDA',\n",
       " 832: 'MANAGEMENT',\n",
       " 833: 'CINCINNATI',\n",
       " 834: 'OH',\n",
       " 835: 'PHONE',\n",
       " 836: 'TH_AR_LTR',\n",
       " 837: 'SERVICE',\n",
       " 838: 'NUMBER',\n",
       " 839: 'JASON',\n",
       " 840: 'SANTANA',\n",
       " 841: 'Sincerely',\n",
       " 842: 'PLANTATION',\n",
       " 843: 'Allstate',\n",
       " 844: 'Youre',\n",
       " 845: 'good',\n",
       " 846: 'hands',\n",
       " 847: 'Florida',\n",
       " 848: 'PIP',\n",
       " 849: 'Central',\n",
       " 850: 'CLINTON',\n",
       " 851: 'IA',\n",
       " 852: 'February',\n",
       " 853: 'INSURED',\n",
       " 854: 'LOSS',\n",
       " 855: 'FAX',\n",
       " 856: 'OFFICE',\n",
       " 857: 'HOURS',\n",
       " 858: 'VYPHAPHONE',\n",
       " 859: 'INTHALANGSY',\n",
       " 860: 'EXT',\n",
       " 861: \"'re\",\n",
       " 862: 'Jacksonville',\n",
       " 863: 'DALLAS',\n",
       " 864: 'TX',\n",
       " 865: 'Some',\n",
       " 866: 'specifics',\n",
       " 867: 'request',\n",
       " 868: 'sincerely',\n",
       " 869: 'CAITLEN',\n",
       " 870: 'CARROLL',\n",
       " 871: 'Ext',\n",
       " 872: 'date',\n",
       " 873: 'name',\n",
       " 874: 'Cellular',\n",
       " 875: 'MedSupport',\n",
       " 876: 'policy',\n",
       " 877: 'policies',\n",
       " 878: 'Female',\n",
       " 879: 'Domestic',\n",
       " 880: 'Partner',\n",
       " 881: 'information',\n",
       " 882: 'Condition',\n",
       " 883: 'am',\n",
       " 884: 'pm',\n",
       " 885: 'Confinement',\n",
       " 886: 'Dates',\n",
       " 887: 'Teri',\n",
       " 888: 'Willochell',\n",
       " 889: 'phycian',\n",
       " 890: 'considerations',\n",
       " 891: 'each',\n",
       " 892: 'such',\n",
       " 893: 'violation',\n",
       " 894: 'number',\n",
       " 895: 'indicated',\n",
       " 896: 'above',\n",
       " 897: 'My',\n",
       " 898: 'Member',\n",
       " 899: 'Relationship',\n",
       " 900: 'person',\n",
       " 901: 'granting',\n",
       " 902: 'authority',\n",
       " 903: 'birth',\n",
       " 904: 'Including',\n",
       " 905: 'L',\n",
       " 906: 'lesser',\n",
       " 907: 'If',\n",
       " 908: 'yes',\n",
       " 909: 'please',\n",
       " 910: 'provide',\n",
       " 911: 'tha',\n",
       " 912: 'diagnosis',\n",
       " 913: 'Treatment',\n",
       " 914: 'advise',\n",
       " 915: 'stop',\n",
       " 916: 'working',\n",
       " 917: 'what',\n",
       " 918: 'Willochel',\n",
       " 919: 'Specially',\n",
       " 920: 'internal',\n",
       " 921: 'medicine',\n",
       " 922: 'MD',\n",
       " 923: 'MedExpress',\n",
       " 924: 'Urgent',\n",
       " 925: 'Care',\n",
       " 926: 'Route',\n",
       " 927: 'HIPPA',\n",
       " 928: 'Socia',\n",
       " 929: 'Location',\n",
       " 930: 'Norwin',\n",
       " 931: 'Huntingdon',\n",
       " 932: 'Holder',\n",
       " 933: 'Sex',\n",
       " 934: 'COMP',\n",
       " 935: 'Clinical',\n",
       " 936: 'Report',\n",
       " 937: 'BP',\n",
       " 938: 'mmHg',\n",
       " 939: 'PULSE',\n",
       " 940: 'bpm',\n",
       " 941: 'RESP',\n",
       " 942: 'TEMP',\n",
       " 943: 'WEIGTH',\n",
       " 944: 'ft',\n",
       " 945: 'BMI',\n",
       " 946: 'LMP',\n",
       " 947: 'PMP',\n",
       " 948: 'SAT',\n",
       " 949: 'CONTUSION',\n",
       " 950: 'Meds',\n",
       " 951: 'ACTIVE',\n",
       " 952: 'acetaminophen',\n",
       " 953: 'albuterol',\n",
       " 954: 'bulk',\n",
       " 955: 'Dilantin',\n",
       " 956: 'gabapentin',\n",
       " 957: 'Humalog',\n",
       " 958: 'Lamictal',\n",
       " 959: 'Lyrica',\n",
       " 960: 'Neurontin',\n",
       " 961: 'valacyclovir',\n",
       " 962: 'Procedures',\n",
       " 963: 'FOOT',\n",
       " 964: 'MIN',\n",
       " 965: 'THREE',\n",
       " 966: 'ESTAB',\n",
       " 967: 'URGENT',\n",
       " 968: 'CARE',\n",
       " 969: 'Pocket',\n",
       " 970: 'Met',\n",
       " 971: 'Estimated',\n",
       " 972: 'charges',\n",
       " 973: 'MSO',\n",
       " 974: 'LLC',\n",
       " 975: 'Med',\n",
       " 976: 'Express',\n",
       " 977: 'UC',\n",
       " 978: 'HUNINGTON',\n",
       " 979: 'Merchant',\n",
       " 980: 'Transaction',\n",
       " 981: 'PURCHASE',\n",
       " 982: 'Approval',\n",
       " 983: 'code',\n",
       " 984: 'Record',\n",
       " 985: 'Visa',\n",
       " 986: 'Trace',\n",
       " 987: 'reference',\n",
       " 988: 'Cardholder',\n",
       " 989: 'identifier',\n",
       " 990: 'Application',\n",
       " 991: 'label',\n",
       " 992: 'TVR',\n",
       " 993: 'AID',\n",
       " 994: 'Subtotal',\n",
       " 995: 'Sales',\n",
       " 996: 'customer',\n",
       " 997: 'copy',\n",
       " 998: 'send',\n",
       " 999: 'payments',\n",
       " ...}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_word_tokens=len(sorted(list(word2int)))\n",
    "num_char_tokens=len(sorted(list(char2int)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vecotrize hierarichal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_char_input_data, decoder_word_input_data, decoder_word_target_data = vectorize_hier_data(input_texts, \n",
    "                                                                                                target_texts, \n",
    "                                                                                                max_words_seq_len, \n",
    "                                                                                                max_chars_seq_len, \n",
    "                                                                                                num_char_tokens, \n",
    "                                                                                                num_word_tokens, \n",
    "                                                                                                word2int, \n",
    "                                                                                                char2int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load char encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/keras/engine/saving.py:269: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "encoder_char_model = load_model(encoder_char_model_file.format(max_sent_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Hierarichal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, None)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, None, 91)          8281      \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection [(None, None, 512), (None 712704    \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 512)               0         \n",
      "=================================================================\n",
      "Total params: 720,985\n",
      "Trainable params: 712,704\n",
      "Non-trainable params: 8,281\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_word_embedding_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder-decoder  model:\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 15, 20)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 15, 512)      720985      input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            (None, 15)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) [(None, 15, 512), (N 1574912     time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 15, 1024)     1031168     input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 512)          0           bidirectional_2[0][1]            \n",
      "                                                                 bidirectional_2[0][3]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 512)          0           bidirectional_2[0][2]            \n",
      "                                                                 bidirectional_2[0][4]            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   [(None, 15, 512), (N 3147776     embedding_3[0][0]                \n",
      "                                                                 concatenate_4[0][0]              \n",
      "                                                                 concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dot_3 (Dot)                     (None, 15, 15)       0           lstm_4[0][0]                     \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 15, 15)       0           dot_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dot_4 (Dot)                     (None, 15, 512)      0           activation_1[0][0]               \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 15, 1024)     0           dot_4[0][0]                      \n",
      "                                                                 lstm_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 15, 1007)     1032175     concatenate_6[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 7,507,016\n",
      "Trainable params: 7,498,735\n",
      "Non-trainable params: 8,281\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:73: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=[<tf.Tenso...)`\n"
     ]
    }
   ],
   "source": [
    "model, encoder_model, decoder_model = build_hier_model(encoder_word_embedding_model=encoder_word_embedding_model, \n",
    "                              max_words_seq_len=max_words_seq_len,\n",
    "                              max_char_seq_len=max_chars_seq_len,\n",
    "                              num_word_tokens=num_word_tokens,\n",
    "                              num_char_tokens=num_char_tokens, \n",
    "                              latent_dim=latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 15, 20)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 15, 512)      720985      input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) [(None, 15, 512), (N 1574912     time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 512)          0           bidirectional_2[0][1]            \n",
      "                                                                 bidirectional_2[0][3]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 512)          0           bidirectional_2[0][2]            \n",
      "                                                                 bidirectional_2[0][4]            \n",
      "==================================================================================================\n",
      "Total params: 2,295,897\n",
      "Trainable params: 2,287,616\n",
      "Non-trainable params: 8,281\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 15)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 15, 1024)     1031168     input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_9 (InputLayer)            (None, 512)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_10 (InputLayer)           (None, 512)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   [(None, 15, 512), (N 3147776     embedding_3[0][0]                \n",
      "                                                                 input_9[0][0]                    \n",
      "                                                                 input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            (None, 15, 512)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dot_3 (Dot)                     (None, 15, 15)       0           lstm_4[1][0]                     \n",
      "                                                                 input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 15, 15)       0           dot_3[1][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dot_4 (Dot)                     (None, 15, 512)      0           activation_1[1][0]               \n",
      "                                                                 input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 15, 1024)     0           dot_4[1][0]                      \n",
      "                                                                 lstm_4[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 15, 1007)     1032175     concatenate_6[1][0]              \n",
      "==================================================================================================\n",
      "Total params: 5,211,119\n",
      "Trainable params: 5,211,119\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Hierarichal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3200 samples, validate on 800 samples\n",
      "Epoch 1/50\n",
      "3200/3200 [==============================] - 65s 20ms/step - loss: 2.4829 - categorical_accuracy: 0.4285 - val_loss: 2.1754 - val_categorical_accuracy: 0.4739\n",
      "\n",
      "Epoch 00001: val_categorical_accuracy improved from -inf to 0.47392, saving model to best_hier_model-15-20.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/keras/engine/network.py:872: UserWarning: Layer lstm_4 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_4/concat:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'concatenate_5/concat:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50\n",
      "3200/3200 [==============================] - 57s 18ms/step - loss: 0.5391 - categorical_accuracy: 0.7279 - val_loss: 1.0848 - val_categorical_accuracy: 0.6024\n",
      "\n",
      "Epoch 00002: val_categorical_accuracy improved from 0.47392 to 0.60238, saving model to best_hier_model-15-20.hdf5\n",
      "Epoch 3/50\n",
      "3200/3200 [==============================] - 46s 14ms/step - loss: 0.2201 - categorical_accuracy: 0.7645 - val_loss: 0.8191 - val_categorical_accuracy: 0.6635\n",
      "\n",
      "Epoch 00003: val_categorical_accuracy improved from 0.60238 to 0.66352, saving model to best_hier_model-15-20.hdf5\n",
      "Epoch 4/50\n",
      "3200/3200 [==============================] - 37s 12ms/step - loss: 0.1464 - categorical_accuracy: 0.7870 - val_loss: 0.7778 - val_categorical_accuracy: 0.6847\n",
      "\n",
      "Epoch 00004: val_categorical_accuracy improved from 0.66352 to 0.68473, saving model to best_hier_model-15-20.hdf5\n",
      "Epoch 5/50\n",
      "3200/3200 [==============================] - 62s 19ms/step - loss: 0.1035 - categorical_accuracy: 0.7934 - val_loss: 0.7521 - val_categorical_accuracy: 0.6953\n",
      "\n",
      "Epoch 00005: val_categorical_accuracy improved from 0.68473 to 0.69528, saving model to best_hier_model-15-20.hdf5\n",
      "Epoch 6/50\n",
      "3200/3200 [==============================] - 87s 27ms/step - loss: 0.0911 - categorical_accuracy: 0.8219 - val_loss: 0.6623 - val_categorical_accuracy: 0.7315\n",
      "\n",
      "Epoch 00006: val_categorical_accuracy improved from 0.69528 to 0.73152, saving model to best_hier_model-15-20.hdf5\n",
      "Epoch 7/50\n",
      "3200/3200 [==============================] - 92s 29ms/step - loss: 0.0777 - categorical_accuracy: 0.8361 - val_loss: 0.6626 - val_categorical_accuracy: 0.7237\n",
      "\n",
      "Epoch 00007: val_categorical_accuracy did not improve from 0.73152\n",
      "Epoch 8/50\n",
      "3200/3200 [==============================] - 95s 30ms/step - loss: 0.0592 - categorical_accuracy: 0.8083 - val_loss: 0.6457 - val_categorical_accuracy: 0.7244\n",
      "\n",
      "Epoch 00008: val_categorical_accuracy did not improve from 0.73152\n",
      "Epoch 9/50\n",
      "3200/3200 [==============================] - 93s 29ms/step - loss: 0.0481 - categorical_accuracy: 0.8203 - val_loss: 0.6405 - val_categorical_accuracy: 0.7394\n",
      "\n",
      "Epoch 00009: val_categorical_accuracy improved from 0.73152 to 0.73942, saving model to best_hier_model-15-20.hdf5\n",
      "Epoch 10/50\n",
      "3200/3200 [==============================] - 64s 20ms/step - loss: 0.0459 - categorical_accuracy: 0.8223 - val_loss: 0.6258 - val_categorical_accuracy: 0.7331\n",
      "\n",
      "Epoch 00010: val_categorical_accuracy did not improve from 0.73942\n",
      "Epoch 11/50\n",
      "3200/3200 [==============================] - 39s 12ms/step - loss: 0.0552 - categorical_accuracy: 0.8156 - val_loss: 0.6335 - val_categorical_accuracy: 0.7426\n",
      "\n",
      "Epoch 00011: val_categorical_accuracy improved from 0.73942 to 0.74262, saving model to best_hier_model-15-20.hdf5\n",
      "Epoch 12/50\n",
      "3200/3200 [==============================] - 36s 11ms/step - loss: 0.0602 - categorical_accuracy: 0.8197 - val_loss: 0.7084 - val_categorical_accuracy: 0.7611\n",
      "\n",
      "Epoch 00012: val_categorical_accuracy improved from 0.74262 to 0.76113, saving model to best_hier_model-15-20.hdf5\n",
      "Epoch 13/50\n",
      "3200/3200 [==============================] - 36s 11ms/step - loss: 0.0620 - categorical_accuracy: 0.8230 - val_loss: 0.7075 - val_categorical_accuracy: 0.7125\n",
      "\n",
      "Epoch 00013: val_categorical_accuracy did not improve from 0.76113\n",
      "Epoch 14/50\n",
      "3200/3200 [==============================] - 36s 11ms/step - loss: 0.0834 - categorical_accuracy: 0.8076 - val_loss: 0.7390 - val_categorical_accuracy: 0.7031\n",
      "\n",
      "Epoch 00014: val_categorical_accuracy did not improve from 0.76113\n",
      "Epoch 15/50\n",
      "3200/3200 [==============================] - 36s 11ms/step - loss: 0.0739 - categorical_accuracy: 0.8119 - val_loss: 0.8469 - val_categorical_accuracy: 0.6952\n",
      "\n",
      "Epoch 00015: val_categorical_accuracy did not improve from 0.76113\n",
      "Epoch 16/50\n",
      "3200/3200 [==============================] - 36s 11ms/step - loss: 0.0795 - categorical_accuracy: 0.8022 - val_loss: 0.8020 - val_categorical_accuracy: 0.7013\n",
      "\n",
      "Epoch 00016: val_categorical_accuracy did not improve from 0.76113\n",
      "Epoch 17/50\n",
      "3200/3200 [==============================] - 36s 11ms/step - loss: 0.0997 - categorical_accuracy: 0.7986 - val_loss: 0.8352 - val_categorical_accuracy: 0.6970\n",
      "\n",
      "Epoch 00017: val_categorical_accuracy did not improve from 0.76113\n",
      "Epoch 18/50\n",
      "3200/3200 [==============================] - 36s 11ms/step - loss: 0.1068 - categorical_accuracy: 0.7870 - val_loss: 0.8667 - val_categorical_accuracy: 0.6737\n",
      "\n",
      "Epoch 00018: val_categorical_accuracy did not improve from 0.76113\n",
      "Epoch 19/50\n",
      "3200/3200 [==============================] - 36s 11ms/step - loss: 0.1025 - categorical_accuracy: 0.7887 - val_loss: 0.8253 - val_categorical_accuracy: 0.6769\n",
      "\n",
      "Epoch 00019: val_categorical_accuracy did not improve from 0.76113\n",
      "Epoch 20/50\n",
      "3200/3200 [==============================] - 36s 11ms/step - loss: 0.1062 - categorical_accuracy: 0.7877 - val_loss: 0.8374 - val_categorical_accuracy: 0.6950\n",
      "\n",
      "Epoch 00020: val_categorical_accuracy did not improve from 0.76113\n",
      "Epoch 21/50\n",
      "3200/3200 [==============================] - 36s 11ms/step - loss: 0.1070 - categorical_accuracy: 0.7906 - val_loss: 0.8022 - val_categorical_accuracy: 0.6945\n",
      "\n",
      "Epoch 00021: val_categorical_accuracy did not improve from 0.76113\n",
      "Epoch 22/50\n",
      "3200/3200 [==============================] - 36s 11ms/step - loss: 0.1146 - categorical_accuracy: 0.7836 - val_loss: 0.8405 - val_categorical_accuracy: 0.6835\n",
      "\n",
      "Epoch 00022: val_categorical_accuracy did not improve from 0.76113\n",
      "Epoch 23/50\n",
      "3200/3200 [==============================] - 36s 11ms/step - loss: 0.0994 - categorical_accuracy: 0.7882 - val_loss: 0.8229 - val_categorical_accuracy: 0.6843\n",
      "\n",
      "Epoch 00023: val_categorical_accuracy did not improve from 0.76113\n",
      "Epoch 24/50\n",
      "3200/3200 [==============================] - 36s 11ms/step - loss: 0.1057 - categorical_accuracy: 0.7831 - val_loss: 0.7858 - val_categorical_accuracy: 0.6919\n",
      "\n",
      "Epoch 00024: val_categorical_accuracy did not improve from 0.76113\n",
      "Epoch 25/50\n",
      "3200/3200 [==============================] - 36s 11ms/step - loss: 0.0776 - categorical_accuracy: 0.7901 - val_loss: 0.7937 - val_categorical_accuracy: 0.7008\n",
      "\n",
      "Epoch 00025: val_categorical_accuracy did not improve from 0.76113\n",
      "Epoch 26/50\n",
      "3200/3200 [==============================] - 37s 11ms/step - loss: 0.0734 - categorical_accuracy: 0.7973 - val_loss: 0.8299 - val_categorical_accuracy: 0.6940\n",
      "\n",
      "Epoch 00026: val_categorical_accuracy did not improve from 0.76113\n",
      "Epoch 27/50\n",
      "3200/3200 [==============================] - 36s 11ms/step - loss: 0.0747 - categorical_accuracy: 0.7975 - val_loss: 0.8607 - val_categorical_accuracy: 0.6972\n",
      "\n",
      "Epoch 00027: val_categorical_accuracy did not improve from 0.76113\n",
      "Epoch 28/50\n",
      "3200/3200 [==============================] - 36s 11ms/step - loss: 0.1274 - categorical_accuracy: 0.7855 - val_loss: 0.9582 - val_categorical_accuracy: 0.6916\n",
      "\n",
      "Epoch 00028: val_categorical_accuracy did not improve from 0.76113\n",
      "Epoch 29/50\n",
      "3200/3200 [==============================] - 36s 11ms/step - loss: 0.1541 - categorical_accuracy: 0.7783 - val_loss: 0.9967 - val_categorical_accuracy: 0.6695\n",
      "\n",
      "Epoch 00029: val_categorical_accuracy did not improve from 0.76113\n",
      "Epoch 30/50\n",
      "3200/3200 [==============================] - 36s 11ms/step - loss: 0.1492 - categorical_accuracy: 0.7809 - val_loss: 0.9499 - val_categorical_accuracy: 0.6751\n",
      "\n",
      "Epoch 00030: val_categorical_accuracy did not improve from 0.76113\n",
      "Epoch 31/50\n",
      "3200/3200 [==============================] - 36s 11ms/step - loss: 0.1835 - categorical_accuracy: 0.7770 - val_loss: 1.0079 - val_categorical_accuracy: 0.6870\n",
      "\n",
      "Epoch 00031: val_categorical_accuracy did not improve from 0.76113\n",
      "Epoch 32/50\n",
      "3200/3200 [==============================] - 36s 11ms/step - loss: 0.1837 - categorical_accuracy: 0.7838 - val_loss: 0.9988 - val_categorical_accuracy: 0.6661\n",
      "\n",
      "Epoch 00032: val_categorical_accuracy did not improve from 0.76113\n",
      "Epoch 33/50\n",
      "3200/3200 [==============================] - 36s 11ms/step - loss: 0.1720 - categorical_accuracy: 0.7768 - val_loss: 1.0300 - val_categorical_accuracy: 0.6644\n",
      "\n",
      "Epoch 00033: val_categorical_accuracy did not improve from 0.76113\n",
      "Epoch 34/50\n",
      "3200/3200 [==============================] - 36s 11ms/step - loss: 0.1626 - categorical_accuracy: 0.7772 - val_loss: 1.0554 - val_categorical_accuracy: 0.6656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00034: val_categorical_accuracy did not improve from 0.76113\n",
      "Epoch 35/50\n",
      "3200/3200 [==============================] - 36s 11ms/step - loss: 0.1470 - categorical_accuracy: 0.7833 - val_loss: 0.9734 - val_categorical_accuracy: 0.6797\n",
      "\n",
      "Epoch 00035: val_categorical_accuracy did not improve from 0.76113\n",
      "Epoch 36/50\n",
      "3200/3200 [==============================] - 36s 11ms/step - loss: 0.1247 - categorical_accuracy: 0.7876 - val_loss: 1.0631 - val_categorical_accuracy: 0.6567\n",
      "\n",
      "Epoch 00036: val_categorical_accuracy did not improve from 0.76113\n",
      "Epoch 37/50\n",
      "3200/3200 [==============================] - 36s 11ms/step - loss: 0.1604 - categorical_accuracy: 0.7810 - val_loss: 1.0725 - val_categorical_accuracy: 0.6442\n",
      "\n",
      "Epoch 00037: val_categorical_accuracy did not improve from 0.76113\n",
      "Epoch 38/50\n",
      " 512/3200 [===>..........................] - ETA: 28s - loss: 0.1307 - categorical_accuracy: 0.7775"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-86cc20843ace>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m           \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m           \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m           shuffle=True)\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2664\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m                                 session)\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 50\n",
    "lr = 0.01\n",
    "\n",
    "model.compile(optimizer=optimizers.Adam(lr=lr), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "#filepath=\"weights-improvement-{epoch:02d}-{val_categorical_accuracy:.2f}.hdf5\"\n",
    "filepath=\"best_hier_model-{}-{}.hdf5\".format(max_words_seq_len,max_chars_seq_len) # Save only the best model for inference step, as saving the epoch and metric might confuse the inference function which model to use\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_categorical_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "tbCallBack = TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "callbacks_list = [checkpoint, tbCallBack]\n",
    "#callbacks_list = [checkpoint, tbCallBack, lrate]\n",
    "model.fit([encoder_char_input_data, decoder_word_input_data], decoder_word_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=callbacks_list,\n",
    "          validation_split=0.2,\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\t Medical Provider Roles UNK Treating \\n UNK UNK UNK UNK UNK UNK UNK UNK '"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_gt_sequence(decoder_word_input_data[idx:idx+1], int2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenize(input_texts[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_texts[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_lst =  word_tokenize(target_texts[idx])\n",
    "words_lst.insert(0, '\\t')\n",
    "words_lst.append('\\n')\n",
    "words_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_gt_sequence(np.argmax(decoder_word_target_data[idx:idx+1], axis=-1), int2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = model.predict([encoder_char_input_data[idx:idx+1], decoder_word_input_data[idx-1:idx]])\n",
    "y.shape\n",
    "#sampled_token_index = np.argmax(y[0, -1, :])\n",
    "d = np.argmax(y, axis=-1)\n",
    "d.shape\n",
    "decode_gt_sequence(d, int2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq = encoder_char_input_data[idx:idx+1]\n",
    "decoded_sentence, _ = decode_sequence(input_seq, encoder_model, decoder_model, max_words_seq_len, num_word_tokens, int2word)\n",
    "print(decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: Claim Type: VB Accident - ccidental Injury\n",
      "GT sentence: Claim Type: VB Accident - Accidental Injury\n",
      "Decoded sentence: Claim Type UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Plicyholzde/rOwner Infxormation\n",
      "GT sentence: Policyholder/Owner Information\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Frst Name:\n",
      "GT sentence: First Name:\n",
      "Decoded sentence: First Name UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: iMddle Name/Initial:m\n",
      "GT sentence: Middle Name/Initial:\n",
      "Decoded sentence: Middle UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Last Name:\n",
      "GT sentence: Last Name:\n",
      "Decoded sentence: Last Name UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Social Security Nmuber:\n",
      "GT sentence: Social Security Number:\n",
      "Decoded sentence: Social Security Number UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Birtuh Date:\n",
      "GT sentence: Birth Date:\n",
      "Decoded sentence: Birth Date UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Gender:\n",
      "GT sentence: Gender:\n",
      "Decoded sentence: Gender UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Langucage Prheerence:\n",
      "GT sentence: Language Preference:\n",
      "Decoded sentence: Language Preference UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Address Line 1:\n",
      "GT sentence: Address Line 1:\n",
      "Decoded sentence: Address Line UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: City:\n",
      "GT sentence: City:\n",
      "Decoded sentence: City UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Statye/lProvice:\n",
      "GT sentence: State/Province:\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Postal Code:\n",
      "GT sentence: Postal Code:\n",
      "Decoded sentence: Postal Code UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Country:\n",
      "GT sentence: Country:\n",
      "Decoded sentence: Country UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Bes tPhone Numcber t oeb xReached During btleDa:y\n",
      "GT sentence: Best Phone Number to be Reached During the Day:\n",
      "Decoded sentence: Best Phone Number to be Reached During the Day UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Email Address:\n",
      "GT sentence: Email Address:\n",
      "Decoded sentence: Email Address UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Page 1 o f1\n",
      "GT sentence: Page 1 of 1\n",
      "Decoded sentence: Page UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: RADILOGY\n",
      "GT sentence: RADIOLOGY\n",
      "Decoded sentence: COLLIER  \n",
      "-\n",
      "Input sentence: REPRT\n",
      "GT sentence: REPORT\n",
      "Decoded sentence: REPORT  \n",
      "-\n",
      "Input sentence: www.rays.net\n",
      "GT sentence: www.rays.net\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Patien MRhN Accession No. Ref. Phsycgian\n",
      "GT sentence: Patient MRN Accession No. Ref. Physician\n",
      "Decoded sentence: Patient MRN Accession No UNK  \n",
      "-\n",
      "Input sentence: UNKNhOWiN\n",
      "GT sentence: UNKNOWN\n",
      "Decoded sentence: HOWEVER  \n",
      "-\n",
      "Input sentence: Study\n",
      "GT sentence: Study\n",
      "Decoded sentence: Study  \n",
      "-\n",
      "Input sentence: Study Date:\n",
      "GT sentence: Study Date:\n",
      "Decoded sentence: Study Date UNK  \n",
      "-\n",
      "Input sentence: Hospqital Code: 2v026 \n",
      "GT sentence: Hospital Code: 2026\n",
      "Decoded sentence: Hospital Code UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: DbOB:\n",
      "GT sentence: DOB:\n",
      "Decoded sentence: DOB UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: 43F.\n",
      "GT sentence: 43F.\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: echnique: 3 views left wrist\n",
      "GT sentence: Technique: 3 views left wrist\n",
      "Decoded sentence: Technique UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Cormarisn: Noneavahilabie\n",
      "GT sentence: Cormarison: None availabie\n",
      "Decoded sentence: Cormarison UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Comparion: Non avaiable\n",
      "GT sentence: Comparison: None available\n",
      "Decoded sentence: Claimant Name UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: FINDINGS\n",
      "GT sentence: FINDINGS:\n",
      "Decoded sentence: BILLING ON BEHALF  \n",
      "-\n",
      "Input sentence: IMRPESSION:\n",
      "GT sentence: IMPRESSION:\n",
      "Decoded sentence: IMPRESSION UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: 2. No acute osesous abnormalityi dentfied.\n",
      "GT sentence: 2. No acute osseous abnormality identified.\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Daytime Phone:\n",
      "GT sentence: Daytime Phone:\n",
      "Decoded sentence: Daytime Phone UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Cliam Evernpt Infowmaion\n",
      "GT sentence: Claim Event Information\n",
      "Decoded sentence: Claim Event Information  \n",
      "-\n",
      "Input sentence: Stopped Working:? Yes\n",
      "GT sentence: Stopped Working?: Yes\n",
      "Decoded sentence: Stopped Working UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: aDte Lst Physically atWork:\n",
      "GT sentence: Date Last Physically at Work:\n",
      "Decoded sentence: Date Last Physically at Work UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Hours Wokd on Last Day:8\n",
      "GT sentence: Hours Worked on Last Day: 8\n",
      "Decoded sentence: Hours Worked on Last Day UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Horus Scheduled to Wokr on Las tDay:\n",
      "GT sentence: Hours Scheduled to Work on Last Day:\n",
      "Decoded sentence: Hours Scheduled to Work on Last Day UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Date First Missed eWork:\n",
      "GT sentence: Date First Missed Work:\n",
      "Decoded sentence: Date First Missed Work UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Returnde to Work?: N\n",
      "GT sentence: Returned to Work?: No\n",
      "Decoded sentence: Returned to Work UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: dAccdent rok Relate:d eYs\n",
      "GT sentence: Accident Work Related: Yes\n",
      "Decoded sentence: Accident Work Related UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Tim of Accident:\n",
      "GT sentence: Time of Accident:\n",
      "Decoded sentence: Time of Accident UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: AccientD ae:\n",
      "GT sentence: Accident Date:\n",
      "Decoded sentence: Accident Date UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Diagnosis Code: Arthiscopicn urey left wirst.\n",
      "GT sentence: Diagnosis Code: Arthiscopic surgery left wrist.\n",
      "Decoded sentence: Diagnosis Code UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Surjgery Inforamtion\n",
      "GT sentence: Surgery Information\n",
      "Decoded sentence: Surgery Information  \n",
      "-\n",
      "Input sentence: Is Suyrgery eRquired: Yes\n",
      "GT sentence: Is Surgery Required: Yes\n",
      "Decoded sentence: Is Surgery Required UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Surgery Dqate:\n",
      "GT sentence: Surgery Date:\n",
      "Decoded sentence: Surgery Date UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: InpatisentOutpatient Indictor:d Outpatjient\n",
      "GT sentence: Inpatient/Outpatient Indicator: Outpatient\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Medical Provider Informtion- Physician\n",
      "GT sentence: Medical Provider Information - Physician\n",
      "Decoded sentence: Medical Provider Information UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Medical Provider Roles: Treating\n",
      "GT sentence: Medical Provider Roles: Treating\n",
      "Decoded sentence: Medical Provider Roles UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: ProvideriFrst Name: Patrick\n",
      "GT sentence: Provider First Name: Patrick\n",
      "Decoded sentence: Provider First Name UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Provider Last Name: Emerson\n",
      "GT sentence: Provider Last Name: Emerson\n",
      "Decoded sentence: Provider Last Name UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Address Lkine 1 \n",
      "GT sentence: Address Line 1 :\n",
      "Decoded sentence: Address Line UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: City:\n",
      "GT sentence: City:\n",
      "Decoded sentence: City UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: State/Proince:\n",
      "GT sentence: State/Province:\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Postal nCode\n",
      "GT sentence: Postal Code:\n",
      "Decoded sentence: Postal Code UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Coufntry\n",
      "\n",
      "GT sentence: Country:\n",
      "Decoded sentence: Country UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Busines pTelephone:\n",
      "GT sentence: Business Telephone:\n",
      "Decoded sentence: Business Telephone UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: BmusinessF ax\n",
      "GT sentence: Business Fax\n",
      "Decoded sentence: Business Fax  \n",
      "-\n",
      "Input sentence: Datoe f FirstV isit\n",
      "GT sentence: Date of First Visit:\n",
      "Decoded sentence: Date of First Visit UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Dvate of Next iVsit:\n",
      "GT sentence: Date of Next Visit:\n",
      "Decoded sentence: Date of Next Visit UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: MeydiaclfProviedr Information  Hospitalization\n",
      "GT sentence: Medical Provider Information - Hospitalization\n",
      "Decoded sentence: Medical Provider Information UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Hospita Name:\n",
      "GT sentence: Hospital Name:\n",
      "Decoded sentence: Hospital Name UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: ddress Line 1:\n",
      "GT sentence: Address Line 1:\n",
      "Decoded sentence: Address Line UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: City:\n",
      "GT sentence: City:\n",
      "Decoded sentence: City UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: State/Provinyce:\n",
      "GT sentence: State/Province:\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: zPostla Code:\n",
      "GT sentence: Postal Code:\n",
      "Decoded sentence: Postal Code UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Counry:\n",
      "GT sentence: Country:\n",
      "Decoded sentence: Country UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Date of Visit/Admission:\n",
      "GT sentence: Date of Visit/Admission:\n",
      "Decoded sentence: Date of UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: aDtde of Discharge:n\n",
      "GT sentence: Date of Discharge:\n",
      "Decoded sentence: Date of Discharge UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Procedure: Left wrist aerhiscopi surgery\n",
      "GT sentence: Procedure: Left wrist arthiscopic surgery\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Employmsetn Information\n",
      "GT sentence: Employment Information\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: Employer Name:\n",
      "GT sentence: Employer Name:\n",
      "Decoded sentence: Employer Name UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Policy mube:r\n",
      "GT sentence: Policy Number:\n",
      "Decoded sentence: Policy Number UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Eletcrconic Submission\n",
      "GT sentence: Electronic Submission\n",
      "Decoded sentence: Electronic Submission  \n",
      "-\n",
      "Input sentence: Claim Event Ienifier:\n",
      "GT sentence: Claim Event Identifier:\n",
      "Decoded sentence: Claim Event Identifier UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Submission Dte:\n",
      "GT sentence: Submission Date:\n",
      "Decoded sentence: Submission Date UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Electronixcally Signed Indicator: Yes\n",
      "GT sentence: Electronically Signed Indicator: Yes\n",
      "Decoded sentence: Electronically Signed Indicator UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Fraud Statements Reviweed and Electroncally\n",
      "GT sentence: Fraud Statements Reviewed and Electronically\n",
      "Decoded sentence: Fraud Statements Reviewed and Electronically  \n",
      "-\n",
      "Input sentence: Signed Date:\n",
      "GT sentence: Signed Date:\n",
      "Decoded sentence: Signed Date UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: unum\n",
      "GT sentence: unum\n",
      "Decoded sentence: unum  \n",
      "-\n",
      "Input sentence: rThe Berneafits Center\n",
      "GT sentence: The Benefits Center\n",
      "Decoded sentence: The Benefits Center  \n",
      "-\n",
      "Input sentence: (Not for uFMLA Requsets)\n",
      "GT sentence: (Not for FMLA Requests)\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: ElecrtonciallySignedf 02/28/2018\n",
      "GT sentence: Electronically Signed 02/28/2018\n",
      "Decoded sentence: CINCINNATI UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Insured’s Sgnatumre Date Signed\n",
      "GT sentence: Insured’s Signature Date Signed\n",
      "Decoded sentence: Insured Coverage Type Coverage Effective Date  \n",
      "-\n",
      "Input sentence: Printed NameSocial SeucrityN mber\n",
      "GT sentence: Printed Name Social Security Number\n",
      "Decoded sentence: Printed Name Social Security Number  \n",
      "-\n",
      "Input sentence: CL-1116 (11/1)4\n",
      "GT sentence: CL-1116 (11/14)\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Uum\n",
      "GT sentence: Unum\n",
      "Decoded sentence: Unum  \n",
      "-\n",
      "Input sentence: Confirmation of Coveraeg\n",
      "GT sentence: Confirmation of Coverage\n",
      "Decoded sentence: Confirmation of Coverage  \n",
      "-\n",
      "Input sentence: Employer\n",
      "\n",
      "GT sentence: Employer:\n",
      "Decoded sentence: Employer UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Giropu Policy #:\n",
      "GT sentence: Group Policy #:\n",
      "Decoded sentence: Group Policy UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Customer  Poicy #:\n",
      "GT sentence: Customer  Policy #:\n",
      "Decoded sentence: Customer Policy UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: EE Name:\n",
      "GT sentence: EE Name:\n",
      "Decoded sentence: EE Name UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Insufre dCoverage Typie cCovreage EffectiveDate\n",
      "GT sentence: Insured Coverage Type Coverage Effective Date\n",
      "Decoded sentence: Insured Coverage Type Coverage Effective Date  \n",
      "-\n",
      "Input sentence: Emplyoee OfifJ-ob Aczc Januray 1, 2017\n",
      "GT sentence: Employee Off-Job Acc January 1, 2017\n",
      "Decoded sentence: Employee UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: jEploeev Welnless Beneit Jnuary 1, 2017\n",
      "GT sentence: Employee Wellness Benefit January 1, 2017\n",
      "Decoded sentence: Employee Wellness Benefit January UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: TotalMonthly Preimum:p\n",
      "GT sentence: Total Monthly Premium:\n",
      "Decoded sentence: Total Monthly Premium UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Total Emaplaoyee Montly Pyaroll Ddedpucion:\n",
      "GT sentence: Total Employee Montly Payroll Deduction:\n",
      "Decoded sentence: Total Employee UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Name:\n",
      "GT sentence: Name:\n",
      "Decoded sentence: Name UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n"
     ]
    }
   ],
   "source": [
    "# Sample output from train data\n",
    "decoded_sentences = []\n",
    "target_texts_ =  []\n",
    "for seq_index in range(100):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "\n",
    "    input_seq = encoder_char_input_data[seq_index: seq_index + 1]\n",
    "    target_seq = np.argmax(decoder_word_target_data[seq_index: seq_index + 1], axis=-1)\n",
    "    #print(target_seq)\n",
    "    \n",
    "    decoded_sentence, _ = decode_sequence(input_seq, encoder_model, decoder_model, max_words_seq_len, num_word_tokens, int2word)\n",
    "    target_text = target_texts[seq_index][1:-1]\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('GT sentence:', target_text)\n",
    "    print('Decoded sentence:', decoded_sentence)   \n",
    "    decoded_sentences.append(decoded_sentence)\n",
    "    target_texts_.append(target_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: text\n",
      "GT sentence: Claim Type: VB Accident - Accidental Injury\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Policyholder/Owner Information\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: First Name:\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Middle Name/Initial:\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Last Name:\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence:  \n",
      "GT sentence: Social Security Number:\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Birth Date:\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Gender:\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Language Preference:\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Fai\n",
      "GT sentence: Address Line 1:\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: 10\n",
      "GT sentence: City:\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: 7521509\n",
      "GT sentence: State/Province:\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: (FISTDEOO)\n",
      "GT sentence: Postal Code:\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: at\n",
      "GT sentence: Country:\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: 11/3/2017\n",
      "GT sentence: Best Phone Number to be Reached During the Day:\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: 5:23:19\n",
      "GT sentence: Email Address:\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: from\n",
      "GT sentence: Page 1 of 1\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: -9373834004\n",
      "GT sentence: RADIOLOGY\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Req\n",
      "GT sentence: REPORT\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: IC\n",
      "GT sentence: www.rays.net\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: 2017:1030525109:292E.\n",
      "GT sentence: Patient MRN Accession No. Ref. Physician\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Page\n",
      "GT sentence: UNKNOWN\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: 4\n",
      "GT sentence: Study\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: of\n",
      "GT sentence: Study Date:\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: 5\n",
      "GT sentence: Hospital Code: 2026\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: (C)\n",
      "GT sentence: DOB:\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: 43F.\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Technique: 3 views left wrist\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Cormarison: None availabie\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence:  \n",
      "GT sentence: Comparison: None available\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: FINDINGS:\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: IMPRESSION:\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: 2. No acute osseous abnormality identified.\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence:  \n",
      "GT sentence: Daytime Phone:\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Claim Event Information\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Stopped Working?: Yes\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Date Last Physically at Work:\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: 11/3/2017\n",
      "GT sentence: Hours Worked on Last Day: 8\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: FRI\n",
      "GT sentence: Hours Scheduled to Work on Last Day:\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: 8:26\n",
      "GT sentence: Date First Missed Work:\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: FAX\n",
      "GT sentence: Returned to Work?: No\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: 2373834004\n",
      "GT sentence: Accident Work Related: Yes\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Kjooas00s\n",
      "GT sentence: Time of Accident:\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Accident Date:\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Diagnosis Code: Arthiscopic surgery left wrist.\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Surgery Information\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: as3-ursasy3\n",
      "GT sentence: Is Surgery Required: Yes\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: 11:30:11\n",
      "GT sentence: Surgery Date:\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: 11/2/2017\n",
      "GT sentence: Inpatient/Outpatient Indicator: Outpatient\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: vis\n",
      "GT sentence: Medical Provider Information - Physician\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Medical Provider Roles: Treating\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Provider First Name: Patrick\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Provider Last Name: Emerson\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: ®\n",
      "GT sentence: Address Line 1 :\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: ®\n",
      "GT sentence: City:\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: &\n",
      "GT sentence: State/Province:\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: ACCIDENT\n",
      "GT sentence: Postal Code:\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: CLAIM\n",
      "GT sentence: Country:\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: FORM\n",
      "GT sentence: Business Telephone:\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Business Fax\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: uu\n",
      "GT sentence: Date of First Visit:\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: num’\n",
      "GT sentence: Date of Next Visit:\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Tha\n",
      "GT sentence: Medical Provider Information - Hospitalization\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Benelits\n",
      "GT sentence: Hospital Name:\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Canter\n",
      "GT sentence: Address Line 1:\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: City:\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: P.O.\n",
      "GT sentence: State/Province:\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Bax\n",
      "GT sentence: Postal Code:\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: 100158,\n",
      "GT sentence: Country:\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Calumbin,\n",
      "GT sentence: Date of Visit/Admission:\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: EC\n",
      "GT sentence: Date of Discharge:\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: 20202-3150\n",
      "GT sentence: Procedure: Left wrist arthiscopic surgery\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Employment Information\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Tol-frea:\n",
      "GT sentence: Employer Name:\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: 1-800-635-5587\n",
      "GT sentence: Policy Number:\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Fax:\n",
      "GT sentence: Electronic Submission\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: 1-800-447-2488\n",
      "GT sentence: Claim Event Identifier:\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Submission Date:\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Gall\n",
      "GT sentence: Electronically Signed Indicator: Yes\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: toll-free\n",
      "GT sentence: Fraud Statements Reviewed and Electronically\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Monday\n",
      "GT sentence: Signed Date:\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: through\n",
      "GT sentence: unum\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Friday,\n",
      "GT sentence: The Benefits Center\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: 8\n",
      "GT sentence: (Not for FMLA Requests)\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: a.m.\n",
      "GT sentence: Electronically Signed 02/28/2018\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: lo\n",
      "GT sentence: Insured’s Signature Date Signed\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: 8\n",
      "GT sentence: Printed Name Social Security Number\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: p.m,\n",
      "GT sentence: CL-1116 (11/14)\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Eagtarn\n",
      "GT sentence: Unum\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Time.\n",
      "GT sentence: Confirmation of Coverage\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Employer:\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Group Policy #:\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Customer  Policy #:\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence:  \n",
      "GT sentence: EE Name:\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Insured Coverage Type Coverage Effective Date\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Employee Off-Job Acc January 1, 2017\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Employee Wellness Benefit January 1, 2017\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence:  \n",
      "GT sentence: Total Monthly Premium:\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Total Employee Montly Payroll Deduction:\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Name:\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n"
     ]
    }
   ],
   "source": [
    "input_texts = ['text',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'Fai',\n",
    "'10',\n",
    "'7521509',\n",
    "'(FISTDEOO)',\n",
    "'at',\n",
    "'11/3/2017',\n",
    "'5:23:19',\n",
    "'from',\n",
    "'-9373834004',\n",
    "'Req',\n",
    "'IC',\n",
    "'2017:1030525109:292E.',\n",
    "'Page',\n",
    "'4',\n",
    "'of',\n",
    "'5',\n",
    "'(C)',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'11/3/2017',\n",
    "'FRI',\n",
    "'8:26',\n",
    "'FAX',\n",
    "'2373834004',\n",
    "'Kjooas00s',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'as3-ursasy3',\n",
    "'11:30:11',\n",
    "'11/2/2017',\n",
    "'vis',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'®',\n",
    "'®',\n",
    "'&',\n",
    "'ACCIDENT',\n",
    "'CLAIM',\n",
    "'FORM',\n",
    "'',\n",
    "'uu',\n",
    "'num’',\n",
    "'Tha',\n",
    "'Benelits',\n",
    "'Canter',\n",
    "'',\n",
    "'P.O.',\n",
    "'Bax',\n",
    "'100158,',\n",
    "'Calumbin,',\n",
    "'EC',\n",
    "'20202-3150',\n",
    "'',\n",
    "'Tol-frea:',\n",
    "'1-800-635-5587',\n",
    "'Fax:',\n",
    "'1-800-447-2488',\n",
    "'',\n",
    "'Gall',\n",
    "'toll-free',\n",
    "'Monday',\n",
    "'through',\n",
    "'Friday,',\n",
    "'8',\n",
    "'a.m.',\n",
    "'lo',\n",
    "'8',\n",
    "'p.m,',\n",
    "'Eagtarn',\n",
    "'Time.',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'[',\n",
    "'ATTENDING',\n",
    "'PHYSICIAN',\n",
    "'STATEMENT',\n",
    "']',\n",
    "'',\n",
    "'',\n",
    "'IneurexiPolicyt',\n",
    "'alcar',\n",
    "'Hama',\n",
    "'(Lael',\n",
    "'Name,',\n",
    "'Flis!',\n",
    "'Nama,',\n",
    "'MI,',\n",
    "'Suffix)',\n",
    "'Data',\n",
    "'of',\n",
    "'Risth',\n",
    "'{msmidrfyy)',\n",
    "'-',\n",
    "'',\n",
    "'',\n",
    "'Faupi',\n",
    "'Nana',\n",
    "'{Laut',\n",
    "'Hume,',\n",
    "'Flial',\n",
    "'Numa,',\n",
    "'1',\n",
    "'Sut)',\n",
    "'Dats',\n",
    "'al',\n",
    "'Bln',\n",
    "'rAvad)',\n",
    "'Ul',\n",
    "'_',\n",
    "'',\n",
    "'-[ECIpENT',\n",
    "'DETAILS',\n",
    "']',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'a',\n",
    "'thls',\n",
    "'Gundilan',\n",
    "'the',\n",
    "'result',\n",
    "'of',\n",
    "'a',\n",
    "'acddental',\n",
    "'inury?',\n",
    "'ves',\n",
    "'O',\n",
    "'No',\n",
    "'if',\n",
    "'yas,',\n",
    "'dale',\n",
    "'of',\n",
    "'accident',\n",
    "'qre/ddlyy)',\n",
    "'[1',\n",
    "'0]',\n",
    "'[z]e',\n",
    "'[=]',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'Is',\n",
    "'Mig',\n",
    "'condition',\n",
    "'Lhe',\n",
    "'result',\n",
    "'of',\n",
    "'hefer',\n",
    "'employment',\n",
    "'£1',\n",
    "'Yes',\n",
    "'pNo',\n",
    "'[1',\n",
    "'Unknown',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'Plaaze',\n",
    "'verily',\n",
    "'treatment',\n",
    "'for',\n",
    "'the',\n",
    "'accident',\n",
    "'lalad',\n",
    "'above.',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'Dalaw',\n",
    "'of',\n",
    "'Diagnosis',\n",
    "'Diagncsis',\n",
    "'Description',\n",
    "'Prosadure',\n",
    "'Procedure',\n",
    "'Dascription',\n",
    "'',\n",
    "'Branden',\n",
    "'(Including',\n",
    "'|',\n",
    "'Cudo',\n",
    "'(GD)',\n",
    "'ous',\n",
    "'',\n",
    "'Confinement)',\n",
    "'eR',\n",
    "'ap',\n",
    "'HAS',\n",
    "'TTT',\n",
    "'',\n",
    "'BEEF',\n",
    "'eR',\n",
    "'',\n",
    "'wiz]',\n",
    "'.',\n",
    "'S33,5XxA',\n",
    "'Hh',\n",
    "'rioes',\n",
    "'ey',\n",
    "'race',\n",
    "'Word',\n",
    "'',\n",
    "'awqd]',\n",
    "'',\n",
    "'weak',\n",
    "'3',\n",
    "'n',\n",
    "'[aveny',\n",
    "'[d',\n",
    "'',\n",
    "'wifi',\n",
    "'Wl',\n",
    "'',\n",
    "'oa',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'Has',\n",
    "'lhe',\n",
    "'pallet',\n",
    "'bean',\n",
    "'trastad',\n",
    "'for',\n",
    "'tha',\n",
    "'same',\n",
    "'ar',\n",
    "'&',\n",
    "'S(tilar',\n",
    "'candillan',\n",
    "'by',\n",
    "'anolher',\n",
    "'phyalelan',\n",
    "'In',\n",
    "'tha',\n",
    "'past?',\n",
    "'[1',\n",
    "'Yen',\n",
    "'Bho',\n",
    "'',\n",
    "'M',\n",
    "'yor,',\n",
    "'pioona',\n",
    "'provid',\n",
    "'tha',\n",
    "'fares:',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'Diageosis:',\n",
    "'Tramiment',\n",
    "'Daten:',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'id',\n",
    "'ya.1',\n",
    "'#dving',\n",
    "'Lhe',\n",
    "'patient',\n",
    "'to',\n",
    "'clap',\n",
    "'working?',\n",
    "'RECEIVED',\n",
    "'',\n",
    "'It',\n",
    "'yes,',\n",
    "'B8',\n",
    "'of',\n",
    "'what',\n",
    "'cate?',\n",
    "'(mmidkyy)',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'[23]',\n",
    "'[117]',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'[Ih',\n",
    "'cielih',\n",
    "'fa',\n",
    "'rotated',\n",
    "'to',\n",
    "'normal',\n",
    "'prepnency,',\n",
    "'please',\n",
    "'grovida',\n",
    "'tha',\n",
    "'idliawing:',\n",
    "'NOV',\n",
    "'',\n",
    "'Expecigd',\n",
    "'Delivery',\n",
    "'Dale',\n",
    "'(mimicd/yy)',\n",
    "'Aclual',\n",
    "'Delivery',\n",
    "'Dale',\n",
    "'{mmiddlyy',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'Phyeiclan',\n",
    "'informaiton',\n",
    "'HUMAN',\n",
    "'REGOURCITE',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'FRAUD',\n",
    "'NOTICE:',\n",
    "'Any',\n",
    "'person',\n",
    "'wha',\n",
    "'knowingly',\n",
    "'files',\n",
    "'&',\n",
    "'statement',\n",
    "'of',\n",
    "'clalm',\n",
    "'containing',\n",
    "'FALSE',\n",
    "'or',\n",
    "'misleading',\n",
    "'information',\n",
    "'8',\n",
    "'',\n",
    "'subject',\n",
    "'to',\n",
    "'criminal',\n",
    "'and',\n",
    "'elvil',\n",
    "'penallies.',\n",
    "'This',\n",
    "'includes',\n",
    "'Attending',\n",
    "'Physician',\n",
    "'portions',\n",
    "'of',\n",
    "'the',\n",
    "'claim',\n",
    "'farm.',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'CS',\n",
    "'yma',\n",
    "'SEAS',\n",
    "'Ta',\n",
    "'hve',\n",
    "'glan',\n",
    "'=',\n",
    "'',\n",
    "'The',\n",
    "'above',\n",
    "'statements',\n",
    "'ara',\n",
    "'trun',\n",
    "'And',\n",
    "'rompints',\n",
    "'to',\n",
    "'tho',\n",
    "'bot',\n",
    "'of',\n",
    "'my',\n",
    "'knowledge',\n",
    "'and',\n",
    "'bolluf.',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'Physician',\n",
    "'Name',\n",
    "'(Lea!',\n",
    "'Name,',\n",
    "'Firat',\n",
    "'Name,',\n",
    "'MI,',\n",
    "'Suita)',\n",
    "'Plases',\n",
    "'Print',\n",
    "'Co',\n",
    "'FHman',\n",
    "'log',\n",
    "'Mm',\n",
    "'',\n",
    "'/',\n",
    "'‘',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'Medical',\n",
    "'Speclaty',\n",
    "'[Tr',\n",
    "'eactal-',\n",
    "']',\n",
    "'|',\n",
    "'D',\n",
    "'of',\n",
    "'r',\n",
    "'of',\n",
    "'Ch',\n",
    "'2',\n",
    "'',\n",
    "'2',\n",
    "'Le',\n",
    "'',\n",
    "'',\n",
    "'==',\n",
    "'Zoi!',\n",
    "'M',\n",
    "'o',\n",
    "'“Fanart',\n",
    "'',\n",
    "'',\n",
    "'=',\n",
    "'Balfrone',\n",
    "'ie',\n",
    "'2',\n",
    "'Sle',\n",
    "'iu',\n",
    "'',\n",
    "'il',\n",
    "'HY',\n",
    "'BY',\n",
    "'1942',\n",
    "'Fax',\n",
    "'Number',\n",
    "'yz—',\n",
    "'43',\n",
    "'-8',\n",
    "'7775',\n",
    "'Fhyalafans',\n",
    "'Tax',\n",
    "'ID',\n",
    "'Number.',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'Aro',\n",
    "'you',\n",
    "'refateq',\n",
    "'to',\n",
    "'hiv',\n",
    "'pollen?',\n",
    "'0',\n",
    "'Yoe',\n",
    "'LlMo',\n",
    "'|',\n",
    "'yes,',\n",
    "'wal',\n",
    "'iv',\n",
    "'the',\n",
    "'relelianshipT',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'Physlclan',\n",
    "'Slgnature',\n",
    "'Date',\n",
    "'',\n",
    "'CL-1023',\n",
    "'-2717',\n",
    "'=',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'—',]\n",
    "\n",
    "encoder_char_input_data, decoder_word_input_data, decoder_word_target_data = vectorize_hier_data(input_texts, \n",
    "                                                                                                [], \n",
    "                                                                                                max_words_seq_len, \n",
    "                                                                                                max_chars_seq_len, \n",
    "                                                                                                num_char_tokens, \n",
    "                                                                                                num_word_tokens, \n",
    "                                                                                                word2int, \n",
    "                                                                                                char2int)\n",
    "\n",
    "# Sample output from train data\n",
    "decoded_sentences = []\n",
    "target_texts_ =  []\n",
    "for seq_index in range(100):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "\n",
    "    input_seq = encoder_char_input_data[seq_index: seq_index + 1]\n",
    "    target_seq = np.argmax(decoder_word_target_data[seq_index: seq_index + 1], axis=-1)\n",
    "    #print(target_seq)\n",
    "    \n",
    "    decoded_sentence, _ = decode_sequence(input_seq, encoder_model, decoder_model, max_words_seq_len, num_word_tokens, int2word)\n",
    "    target_text = target_texts[seq_index][1:-1]\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('GT sentence:', target_text)\n",
    "    print('Decoded sentence:', decoded_sentence)   \n",
    "    decoded_sentences.append(decoded_sentence)\n",
    "    target_texts_.append(target_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
