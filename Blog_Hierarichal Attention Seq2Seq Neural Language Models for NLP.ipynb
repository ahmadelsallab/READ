{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <Center>Hierarichal Attention Seq2Seq Neural Language Models for NLP</Center>\n",
    "# Hierarichy in NLP\n",
    "Language is structured by nature. Our brains use this structure to understand the semantics behind the language. Hence, Natural Language Understanding (NLU) is not possible without making use of such hierarichy.\n",
    "\n",
    "\n",
    "# Pre-trained NLM in NLP\n",
    "In his fast.ai course, Jeremy Howard and Sebastian Ruder, introduced [ULMFit paper] based on pre-training a language model before going to sentiment classification, using transfer learning, which gives them around 4% improvement. \n",
    "\n",
    "# Pre-trained NLM with seq2seq attention models for hierarichal models\n",
    "In this post, I'm trying to use the same idea of using pre-trained NLM in NLP tasks, but this time I want to use seq2seq with attention mechanisms as the NLM method.\n",
    "The pre-trained NLMs shall be at different hierarichal levels; words and characters. After that, the pre-trained models shall be stacked in a hierarichal model to perform some NLP task.\n",
    "\n",
    "I choose two tasks; sentiment classification and spelling correction.\n",
    "\n",
    "\n",
    "# Sentiment Classification \n",
    "## Symantic structure for Sentiment Classification\n",
    "For the past five years, I've been working on sentiment classification models. Throughout this period, I have touched the importance of context parsing for such hard NLP problem. After all, sentiment classification is challenging even sometimes for humans, especially when sarcasm, and long culture background comes into play.\n",
    "\n",
    "## Recusrive AE and Sentiment Tree Bank\n",
    "In my 2017 paper, [AROMA ref], we used the Recursive Auto Encoder (RAE) [socher ref 2011], to create a sentence representation (or Embedding in the NLP and ML language), based on some reading hierarichy (or parsing order in the NLP terminoloy). Such hierarichy of reading should reflect specific key words, and inflection words, upon which the sentiment decision is based. For example, \"The movie is [terrible]\" has negative sentiment due to the keyword \"terrible\", however, \"The is [not] that [bad]\" has positive sentiment due to negation of the keyword \"bad\" with \"not\".\n",
    "\n",
    "[Parse order from RNTN or RAE with sentiment inflection example]\n",
    "\n",
    "Such hierarichy of sentence reading (or parsing), can be obtained using syntatic parsers [Stanford ref]. However, such parsers will only reflect the grammatical building structure of the sentence, but not necessarily the semantic parsing order that builds a sentiment understanding about the sentence. For that, in [Socher RNTN], a Sentiment Tree Bank was built to train such tree based on sentiment inflection tree parsing. However, building such a data bank is very costly in terms of human effort.\n",
    "\n",
    "[Standford parse tree vs. Sentiment Tree Bank]\n",
    "\n",
    "## Seq2seq and Attention mechanisms\n",
    "Recently, attention models are grabbing more attention in the NLP world, especially in the field of NMT [Bahdanau] and [Lyoung] and [Transformer]. Such attention mechanisms can have a strong selective representation power to discover such parsing order, focusing on the important parts of the sentence that produces the ground truth sentiment. In [Hinton Grammar of Foreign language] the attention mechanism is used to build a parser. \n",
    "\n",
    "## Hierarichal Attention (HATT) Classifier\n",
    "Also, language hierarichy is critical to build a representation of a document or a moview review. While reading, we see characters that form words, which form sentences, which form paragraphs, which finally form a document. In [HATT] such hierarichy is used, toghether with attention mechanisms, to create a sentiment classifier. An interesting idea I had when reading this paper is, what if we start from the character level representation of the word? This would avoid the Out-of-Vocabulary (OOV) phenomenon that arise with Embeddings (when we have words the model is never trained on before). Although such extension is not particularly critical to sentiment classification task, since we care about certain keywords that are most likely exist in our vocabulary, however, character level NLP is interesting to many other domains, like language modelling, NMT, spelling correction,...etc.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "IMDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word level seq2seq LM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HATT Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning to HATT model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention visualization\n",
    "__Do we really have a parse tree__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Hierarichal Seq2seq Attention model for spelling correction\n",
    "- Char level input: OOV. Good for spelling correction\n",
    "- Word level output: Good for language knowledge\n",
    "- Hierarichy is good to capture language structure and semantics. Also, it's good to keep the LSTM sequence at reasonable length. Example: char2char is good up to 50 chars then it becomes worse.\n",
    "- Importance of Embedding for char embedding, with mask_zero=0 to suppress padding. PAD symbole must take 0 entry in the car2int vocab to be masked\n",
    "- Importance of attention: idea, two ways (Bahdanau and Lyoung) before and after for LM and spelling. Visualization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "In this post, we explored the potential of pre-trained seq2seq attention NLM for two NLP tasks: 1) sentiment classificatio and 2) spelling correction. Throughout the two tasks, we touched on the importance of hierarichy for NLP.\n",
    "\n",
    "Full attention mechanisms [Transformer], not based on LSTM, is an interesting efficient idea that is not yet fully explored. In the coming posts I will explore the Transformer model with all the above ideas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
