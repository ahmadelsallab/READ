{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# author - Richard Liao \n",
    "# Dec 26 2016\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "os.environ['KERAS_BACKEND']='tensorflow'\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "#from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten, Lambda, Concatenate, Reshape\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout, LSTM, GRU, Bidirectional, TimeDistributed\n",
    "from keras.models import Model\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpu_alloc(device_id):\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]=device_id\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_alloc(\"1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENT_LENGTH = 100\n",
    "MAX_SENTS = 15\n",
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2\n",
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for dataset\n",
    "    Every dataset is lower cased except\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"\\\\\", \"\", string)    \n",
    "    string = re.sub(r\"\\'\", \"\", string)    \n",
    "    string = re.sub(r\"\\\"\", \"\", string)    \n",
    "    return string.strip().lower()\n",
    "\n",
    "data_train = pd.read_csv('./dat/imdb/labeledTrainData.tsv', sep='\\t')\n",
    "print(data_train.shape)\n",
    "\n",
    "from nltk import tokenize\n",
    "\n",
    "reviews = []\n",
    "labels = []\n",
    "texts = []\n",
    "\n",
    "for idx in range(data_train.review.shape[0]):\n",
    "    print('Parsing review ' + str(idx))\n",
    "    text = BeautifulSoup(data_train.review[idx]).get_text()\n",
    "    text = clean_str(text)#.get_text().encode('ascii','ignore'))\n",
    "    #print('Text:\\n' + text)\n",
    "    texts.append(text)\n",
    "    sentences = tokenize.sent_tokenize(text)\n",
    "    reviews.append(sentences)\n",
    "    '''\n",
    "    for sent in sentences:\n",
    "          print('Sentence:\\n' + sent)\n",
    "    '''\n",
    "    labels.append(data_train.sentiment[idx])\n",
    "\n",
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "\n",
    "data_lst = []\n",
    "labels_lst = []\n",
    "for i, sentences in enumerate(reviews):\n",
    "    data = np.zeros((MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')\n",
    "    for j, sent in enumerate(sentences):\n",
    "        if j< MAX_SENTS:\n",
    "            wordTokens = text_to_word_sequence(sent)\n",
    "            k=0\n",
    "            for _, word in enumerate(wordTokens):\n",
    "                if k<MAX_SENT_LENGTH and tokenizer.word_index[word]<MAX_NB_WORDS:\n",
    "                    data[j,k] = tokenizer.word_index[word]\n",
    "                    k=k+1\n",
    "    data_lst.append(data)\n",
    "    labels_lst.append(labels[i])\n",
    "data = np.array(data_lst)\n",
    "labels = np.array(labels_lst)\n",
    "\n",
    "\n",
    "                    \n",
    "word_index = tokenizer.word_index\n",
    "print('Total %s unique tokens.' % len(word_index))\n",
    "\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]\n",
    "\n",
    "print('Number of positive and negative reviews in traing and validation set')\n",
    "print(y_train.sum(axis=0))\n",
    "print(y_val.sum(axis=0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "GLOVE_DIR = \"./dat/glove\"\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "\n",
    "f.close()\n",
    "\n",
    "print('Total %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "w_emb_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            #input_length=1,\n",
    "                            trainable=True)\n",
    "sent_enc_layer = Bidirectional(LSTM(100))\n",
    "'''\n",
    "w_input = Input(shape=(), dtype='int32')\n",
    "w_emb = w_emb_layer(w_input)\n",
    "w_emb = Reshape((1,w_emb.shape[1]))(w_emb)\n",
    "w_emb_model = Model(w_input, w_emb)\n",
    "'''\n",
    "\n",
    "rev_enc_layer = Bidirectional(LSTM(100))\n",
    "\n",
    "review_input = Input(shape=(MAX_SENTS,MAX_SENT_LENGTH), dtype='int32')\n",
    "\n",
    "s_embs = []\n",
    "for i in range(MAX_SENTS):\n",
    "    sent = Lambda(lambda x: x[:,i,:])(review_input)#sentence_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\n",
    "    '''\n",
    "    w_embs = []\n",
    "    for j in range(MAX_SENT_LENGTH):\n",
    "        word = Lambda(lambda x: x[:,j])(sent)\n",
    "        \n",
    "        w_emb = w_emb_layer(word)\n",
    "        #print(w_emb.shape)\n",
    "        #print(w_emb._keras_shape)\n",
    "        w_emb = Reshape((1,int(w_emb.shape[1])))(w_emb)\n",
    "        #w_emb = K.expand_dims(w_emb, -2)\n",
    "        #print(w_emb.shape)\n",
    "        #rint(w_emb.shape[1])\n",
    "        #w_emb = Reshape((1,100))(w_emb)\n",
    "        #rint(w_emb._keras_shape)\n",
    "        #w_emb = w_emb_model(word)\n",
    "        w_embs.append(w_emb)\n",
    "    \n",
    "    #print(w_embs[0]._keras_shape)\n",
    "    w_embs = Concatenate(axis=-2)(w_embs)\n",
    "    #print(w_embs.shape)\n",
    "    '''\n",
    "    w_embs = w_emb_layer(sent)\n",
    "    s_emb = sent_enc_layer(w_embs)\n",
    "    #print(s_emb.shape)\n",
    "    s_emb = Reshape((1, int(s_emb.shape[1])))(s_emb)\n",
    "    s_embs.append(s_emb)\n",
    "\n",
    "s_embs = Concatenate(axis=-2)(s_embs)\n",
    "rev_emb = rev_enc_layer(s_embs)\n",
    "\n",
    "preds = Dense(2, activation='softmax')(rev_emb)\n",
    "model = Model(review_input, preds)\n",
    "'''\n",
    "sentence_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sentence_input)\n",
    "l_lstm = Bidirectional(LSTM(100))(embedded_sequences)\n",
    "sentEncoder = Model(sentence_input, l_lstm)\n",
    "print(sentEncoder.summary())\n",
    "review_input = Input(shape=(MAX_SENTS,MAX_SENT_LENGTH), dtype='int32')\n",
    "review_encoder = TimeDistributed(sentEncoder)(review_input)\n",
    "print(review_encoder.shape)\n",
    "l_lstm_sent = Bidirectional(LSTM(100))(review_encoder)\n",
    "preds = Dense(2, activation='softmax')(l_lstm_sent)\n",
    "model = Model(review_input, preds)\n",
    "'''\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"model fitting - Hierachical LSTM\")\n",
    "NUM_EPOCHS=2\n",
    "history = model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "          nb_epoch=NUM_EPOCHS, batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "history_dict = history.history\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values = history_dict['val_loss']\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "plt.plot(epochs, loss_values, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "acc_values = history_dict['acc']\n",
    "val_acc_values = history_dict['val_acc']\n",
    "plt.plot(epochs, acc_values, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc_values, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_test = pd.read_csv('./dat/imdb/labeledTrainData.tsv', sep='\\t')\n",
    "print(data_test.shape)\n",
    "\n",
    "\n",
    "reviews = []\n",
    "texts = []\n",
    "\n",
    "for idx in range(data_test.review.shape[0]):\n",
    "    print('Parsing review ' + str(idx))\n",
    "    text = BeautifulSoup(data_test.review[idx]).get_text()\n",
    "    text = clean_str(text)#.get_text().encode('ascii','ignore'))\n",
    "\n",
    "    texts.append(text)\n",
    "    sentences = tokenize.sent_tokenize(text)\n",
    "    reviews.append(sentences)\n",
    "\n",
    "\n",
    "data_lst = []\n",
    "labels_lst = []\n",
    "for i, sentences in enumerate(reviews):\n",
    "    data = np.zeros((MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')\n",
    "    for j, sent in enumerate(sentences):\n",
    "        if j< MAX_SENTS:\n",
    "            wordTokens = text_to_word_sequence(sent)\n",
    "            k=0\n",
    "            for _, word in enumerate(wordTokens):\n",
    "                if k<MAX_SENT_LENGTH and tokenizer.word_index[word]<MAX_NB_WORDS:\n",
    "                    data[j,k] = tokenizer.word_index[word]\n",
    "                    k=k+1\n",
    "    data_lst.append(data)\n",
    "    labels_lst.append(labels[i])\n",
    "data = np.array(data_lst)\n",
    "#labels = np.array(labels_lst)                 \n",
    "                    \n",
    "test_input_data = data\n",
    "test_texts = list(data_test.review.apply(BeautifulSoup).apply(BeautifulSoup.get_text).apply(clean_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, rev in enumerate(data_test.review):\n",
    "    print(rev)\n",
    "    test_input = test_input_data[i].copy()\n",
    "    test_input = np.reshape(test_input, (1,test_input.shape[0], test_input.shape[1]))\n",
    "    prediction = model.predict(test_input)\n",
    "    print('Prediction: ', prediction)\n",
    "    sentiment = np.argmax(prediction)\n",
    "    print('Sentiment: ' + str(sentiment))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
