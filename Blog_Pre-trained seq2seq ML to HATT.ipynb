{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "For the past five years, I've been working on sentiment classification models. Throughout this period, I have touched the importance of context parsing for such hard NLP problem. After all, sentiment classification is challenging even sometimes for humans, especially when sarcasm, and long culture background comes into play.\n",
    "\n",
    "## Recusrive AE and Sentiment Tree Bank\n",
    "In my 2017 paper, [AROMA ref], we used the Recursive Auto Encoder (RAE) [socher ref 2011], to create a sentence representation (or Embedding in the NLP and ML language), based on some reading hierarichy (or parsing order in the NLP terminoloy). Such hierarichy of reading should reflect specific key words, and inflection words, upon which the sentiment decision is based. For example, \"The movie is [terrible]\" has negative sentiment due to the keyword \"terrible\", however, \"The is [not] that [bad]\" has positive sentiment due to negation of the keyword \"bad\" with \"not\".\n",
    "\n",
    "[Parse order from RNTN or RAE with sentiment inflection example]\n",
    "\n",
    "Such hierarichy of sentence reading (or parsing), can be obtained using syntatic parsers [Stanford ref]. However, such parsers will only reflect the grammatical building structure of the sentence, but not necessarily the semantic parsing order that builds a sentiment understanding about the sentence. For that, in [Socher RNTN], a Sentiment Tree Bank was built to train such tree based on sentiment inflection tree parsing. However, building such a data bank is very costly in terms of human effort.\n",
    "\n",
    "[Standford parse tree vs. Sentiment Tree Bank]\n",
    "\n",
    "## Seq2seq and Attention mechanisms\n",
    "Recently, attention models are grabbing more attention in the NLP world, especially in the field of NMT [Bahdanau] and [Lyoung] and [Transformer]. Such attention mechanisms can have a strong selective representation power to discover such parsing order, focusing on the important parts of the sentence that produces the ground truth sentiment. In [Hinton Grammar of Foreign language] the attention mechanism is used to build a parser. \n",
    "\n",
    "## Hierarichal Attention (HATT) Classifier\n",
    "Also, language hierarichy is critical to build a representation of a document or a moview review. While reading, we see characters that form words, which form sentences, which form paragraphs, which finally form a document. In [HATT] such hierarichy is used, toghether with attention mechanisms, to create a sentiment classifier. An interesting idea I had when reading this paper is, what if we start from the character level representation of the word? This would avoid the Out-of-Vocabulary (OOV) phenomenon that arise with Embeddings (when we have words the model is never trained on before). Although such extension is not particularly critical to sentiment classification task, since we care about certain keywords that are most likely exist in our vocabulary, however, character level NLP is interesting to many other domains, like language modelling, NMT, spelling correction,...etc.\n",
    "\n",
    "## Transfer learning of pre-trained language models\n",
    "Finally, in his fast.ai course, Jeremy Howard and Sebastian Ruder, introduced [ULMFit paper] based on pre-training a language model before going to sentiment classification, using transfer learning, which gives them around 4% improvement. \n",
    "\n",
    "In this post, I am trying to merge the idea of HATT, with the pre-trained LM idea, however, pre-training in our case will follow seq2seq approach with attention to build a language model, both at the character and word levels. In addition, I wanted to see the effect of full hierarichy, starting from the character level, to the document level, on the final learnt representation for sentiment classification task.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "IMDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Char level seq2seq LM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word level seq2seq LM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HATT Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning to HATT model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention visualization\n",
    "__Do we really have a parse tree__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "In this post we explored a mixture of recent advances in sentiment classification, and the potential of attention mechanisms, and pre-training seq2seq+attention language models, all in full hierarichy starting from the character to document level.\n",
    "\n",
    "Full attention mechanisms [Transformer], not based on LSTM, is an interesting efficient idea that is not yet fully explored. In the coming posts I will explore the Transformer model with all the above ideas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
