{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Bidirectional, Concatenate, GRU, Dot, TimeDistributed, Activation, Embedding, Lambda, Concatenate, Reshape\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, LearningRateScheduler\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from keras.models import load_model\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import pandas as pd\n",
    "import re\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpu_alloc(device_id):\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]=device_id\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_alloc(\"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_chars_vocab(all_texts):\n",
    "    '''Build vocab dictionary to victorize chars into ints'''\n",
    "    vocab_to_int = {}\n",
    "    count = 0 # Start index for any char will be 1, as 0 is masked by the Embedding/Masking layer\n",
    "    codes = ['UNK', ' ', '\\t','\\n']# Start 'UNK' at the first entry, to keep its index=0 to be masked\n",
    "    for code in codes:\n",
    "        if code not in vocab_to_int:\n",
    "            vocab_to_int[code] = count\n",
    "            count += 1    \n",
    "    \n",
    "    for sentence in all_texts:\n",
    "        for char in sentence:\n",
    "            if char not in vocab_to_int:\n",
    "                vocab_to_int[char] = count\n",
    "                count += 1\n",
    "\n",
    "\n",
    "    '''''Build inverse translation from int to char'''\n",
    "    int_to_vocab = {}\n",
    "    for character, value in vocab_to_int.items():\n",
    "        int_to_vocab[value] = character\n",
    "        \n",
    "    return vocab_to_int, int_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sentences_data(input_texts, target_labels, max_sents_per_doc, max_words_per_sent, max_chars_per_word, \n",
    "                             num_classes, char2int):\n",
    "\n",
    "    \n",
    "    \n",
    "    hier_input_data = np.zeros((len(input_texts), \n",
    "                                max_sents_per_doc, \n",
    "                                max_words_per_sent, \n",
    "                                max_chars_per_word), dtype='float32')\n",
    "    \n",
    "        \n",
    "    hier_target_data = np.zeros((len(input_texts), num_classes), dtype='float32')\n",
    "    if(target_labels == None):\n",
    "        target_labels = np.zeros(len(input_texts), dtype='int32')\n",
    "    \n",
    "    for i, (input_text, target_label) in enumerate(zip(input_texts, target_labels)):\n",
    "        #sents_lst = sent_tokenize(clean_str(BeautifulSoup(input_text).get_text())) # TODO: Move to clean str\n",
    "        sents_lst = sent_tokenize(input_text)\n",
    "        \n",
    "        \n",
    "        if len(sents_lst) > max_sents_per_doc:\n",
    "            continue\n",
    "        \n",
    "        for j, sent in enumerate(sents_lst):\n",
    "                \n",
    "            words_lst = word_tokenize(input_text)\n",
    "            \n",
    "            if(len(words_lst) > max_words_per_sent):\n",
    "                continue\n",
    "            \n",
    "            \n",
    "            for k, word in enumerate(words_lst):\n",
    "                \n",
    "                \n",
    "                if(len(word) > max_chars_per_word):\n",
    "                    continue\n",
    "                \n",
    "                for l, char in enumerate(word):\n",
    "                    # c0..cn\n",
    "                    if(char in char2int):\n",
    "                        hier_input_data[i, j, k, l] = char2int[char]\n",
    "                        try:\n",
    "                            hier_target_data[i, target_label] = 1\n",
    "                        except:\n",
    "                            print(target_label)\n",
    "\n",
    "                \n",
    "    return hier_input_data, hier_target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_chars2word_model_simple_BiLSTM(num_encoder_tokens, latent_dim):\n",
    "    # Define an input sequence and process it.\n",
    "    encoder_inputs = Input(shape=(None,), dtype='float32')\n",
    "    #print('encoder_inputs' + str(encoder_inputs.shape))\n",
    "    encoder_inputs_ = Embedding(num_encoder_tokens, num_encoder_tokens,                           \n",
    "                            weights=[np.eye(num_encoder_tokens)],\n",
    "                            mask_zero=True, trainable=False)(encoder_inputs)    \n",
    "    #encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "    #encoder = Bidirectional(LSTM(latent_dim, return_state=True, return_sequences=True)) # Bi LSTM\n",
    "    encoder = Bidirectional(LSTM(latent_dim)) # Bi LSTM\n",
    "    #print('encoder_inputs_' + str(encoder_inputs_.shape))\n",
    "    #encoder_outputs, state_f_h, state_f_c, state_b_h, state_b_c = encoder(encoder_inputs_)# Bi LSTM\n",
    "    encoder_outputs = encoder(encoder_inputs_)# Bi LSTM\n",
    "    #print('encoder_outputs' + str(encoder_outputs.shape))\n",
    "    #encoder_embedding_output = Lambda(lambda x: x[:,-1,:])(encoder_outputs)\n",
    "    encoder_embedding_output = encoder_outputs\n",
    "    #print('encoder_embedding_output' + str(encoder_embedding_output.shape))\n",
    "    encoder_word_embedding_model = Model(input=encoder_inputs, output=encoder_embedding_output)\n",
    "\n",
    "    return encoder_word_embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_words2sent_model_simple_BiLSTM(encoder_word_embedding_model, \n",
    "                           max_words_seq_len, \n",
    "                           max_char_seq_len, \n",
    "                           latent_dim):\n",
    "    # Define an input sequence and process it.\n",
    "\n",
    "    inputs = Input(shape=(max_words_seq_len, max_char_seq_len,), dtype='float32')\n",
    "    #print(inputs.shape)\n",
    "    input_words = TimeDistributed(encoder_word_embedding_model)(inputs)\n",
    "\n",
    "    encoder_inputs_ = input_words   \n",
    "    #encoder_inputs = Input(shape=(None, char_vocab_size))\n",
    "    encoder = Bidirectional(LSTM(latent_dim, return_state=True, return_sequences=True)) # Bi LSTM\n",
    "    encoder_outputs, state_f_h, state_f_c, state_b_h, state_b_c = encoder(encoder_inputs_)# Bi LSTM\n",
    "        \n",
    "    encoder_embedding_output = Lambda(lambda x: x[:,-1,:])(encoder_outputs)\n",
    "    encoder_sentence_embedding_model = Model(input=inputs, output=encoder_embedding_output)\n",
    "\n",
    "    return encoder_sentence_embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "def build_sent2doc_model(encoder_sentence_embedding_model, \n",
    "                         max_sents_seq_len, \n",
    "                         max_words_seq_len, \n",
    "                         max_char_seq_len, \n",
    "                         word2sent_latent_dim,\n",
    "                         sent2doc_latent_dim):\n",
    "    \n",
    "    inputs = Input(shape=(max_sents_seq_len, max_words_seq_len, max_char_seq_len,), dtype='float32')\n",
    "    \n",
    "    sents_states = []\n",
    "    \n",
    "    for s in range(max_sents_seq_len):\n",
    "        \n",
    "        encoder_words_inputs = Lambda(lambda x: x[:,s,:,:])(inputs)\n",
    "        #print(encoder_words_inputs.shape)\n",
    "        encoder_words_outputs = encoder_sentence_embedding_model(encoder_words_inputs)\n",
    "        encoder_words_outputs = Reshape((1,word2sent_latent_dim*2))(encoder_words_outputs)\n",
    "        #_, h, c = encoder_sentence_embedding_model(encoder_words_inputs)\n",
    "        '''\n",
    "        input_words = TimeDistributed(encoder_word_embedding_model)(inputs)\n",
    "\n",
    "        encoder_inputs_ = input_words   \n",
    "        #encoder_inputs = Input(shape=(None, char_vocab_size))\n",
    "        encoder = Bidirectional(LSTM(latent_dim, return_state=True, return_sequences=True)) # Bi LSTM\n",
    "        encoder_outputs, state_f_h, state_f_c, state_b_h, state_b_c = encoder(encoder_inputs_)# Bi LSTM\n",
    "\n",
    "        encoder_embedding_output = Lambda(lambda x: x[:,-1,:])(encoder_outputs)\n",
    "        '''\n",
    "        \n",
    "        \n",
    "        #encoder_words_states = Concatenate()([h,c])\n",
    "        #print(encoder_chars_states)\n",
    "        #encoder_words_states = Reshape((1,word2sent_latent_dim*4))(encoder_words_states)\n",
    "        #print(encoder_words_outputs.shape)\n",
    "        sents_states.append(encoder_words_outputs)\n",
    "    #print(sents_states[0]._keras_shape)\n",
    "    input_sents = Concatenate(axis=-2)(sents_states)\n",
    "    #print(input_sents.shape)\n",
    "    encoder_inputs_ = input_sents   \n",
    "    encoder = Bidirectional(LSTM(sent2doc_latent_dim, return_state=True, return_sequences=True)) # Bi LSTM\n",
    "    encoder_outputs, state_f_h, state_f_c, state_b_h, state_b_c = encoder(encoder_inputs_)# Bi LSTM\n",
    "    state_h = Concatenate()([state_f_h, state_b_h])# Bi LSTM\n",
    "    state_c = Concatenate()([state_f_c, state_b_c])# Bi LSTM\n",
    "    encoder_embedding_output = Lambda(lambda x: x[:,-1,:])(encoder_outputs)\n",
    "    \n",
    "    encoder_document_embedding_model = Model(input=inputs, output=encoder_embedding_output)\n",
    "    '''\n",
    "    preds = Dense(2, activation='softmax')(encoder_embedding_output)\n",
    "    model = Model(inputs, preds)\n",
    "    '''\n",
    "    #return model, encoder_document_embedding_model\n",
    "    return encoder_document_embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_hier_senti_model(encoder_document_embedding_model,\n",
    "                           max_sents_seq_len, \n",
    "                           max_words_seq_len, \n",
    "                           max_char_seq_len):\n",
    "    inputs = Input(shape=(max_sents_seq_len, max_words_seq_len, max_char_seq_len,), dtype='float32')\n",
    "    encoder_embedding_output = encoder_document_embedding_model(inputs)\n",
    "    preds = Dense(2, activation='softmax')(encoder_embedding_output)\n",
    "    model = Model(inputs, preds)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for dataset\n",
    "    Every dataset is lower cased except\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"\\\\\", \"\", string)    \n",
    "    string = re.sub(r\"\\'\", \"\", string)    \n",
    "    string = re.sub(r\"\\\"\", \"\", string)    \n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading and analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../dat/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5814_8</td>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2381_9</td>\n",
       "      <td>1</td>\n",
       "      <td>\\The Classic War of the Worlds\\\" by Timothy Hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7759_3</td>\n",
       "      <td>0</td>\n",
       "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3630_4</td>\n",
       "      <td>0</td>\n",
       "      <td>It must be assumed that those who praised this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9495_8</td>\n",
       "      <td>1</td>\n",
       "      <td>Superbly trashy and wondrously unpretentious 8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  sentiment                                             review\n",
       "0  5814_8          1  With all this stuff going down at the moment w...\n",
       "1  2381_9          1  \\The Classic War of the Worlds\\\" by Timothy Hi...\n",
       "2  7759_3          0  The film starts with a manager (Nicholas Bell)...\n",
       "3  3630_4          0  It must be assumed that those who praised this...\n",
       "4  9495_8          1  Superbly trashy and wondrously unpretentious 8..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file = 'imdb/labeledTrainData.tsv'\n",
    "data_train = pd.read_csv(os.path.join(data_path, data_file), sep='\\t')\n",
    "print(data_train.shape)\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data_train[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12311_10</td>\n",
       "      <td>Naturally in a film who's main themes are of m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8348_2</td>\n",
       "      <td>This movie is a disaster within a disaster fil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5828_4</td>\n",
       "      <td>All in all, this is a movie for kids. We saw i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7186_2</td>\n",
       "      <td>Afraid of the Dark left me with the impression...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12128_7</td>\n",
       "      <td>A very accurate depiction of small time mob li...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                             review\n",
       "0  12311_10  Naturally in a film who's main themes are of m...\n",
       "1    8348_2  This movie is a disaster within a disaster fil...\n",
       "2    5828_4  All in all, this is a movie for kids. We saw i...\n",
       "3    7186_2  Afraid of the Dark left me with the impression...\n",
       "4   12128_7  A very accurate depiction of small time mob li..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file = 'imdb/testData.tsv'\n",
    "data_test = pd.read_csv(os.path.join(data_path, data_file), sep='\\t')\n",
    "print(data_test.shape)\n",
    "data_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = data_test[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      With all this stuff going down at the moment w...\n",
       "1      \\The Classic War of the Worlds\\\" by Timothy Hi...\n",
       "2      The film starts with a manager (Nicholas Bell)...\n",
       "3      It must be assumed that those who praised this...\n",
       "4      Superbly trashy and wondrously unpretentious 8...\n",
       "5      I dont know why people think this is such a ba...\n",
       "6      This movie could have been very good, but come...\n",
       "7      I watched this video at a friend's house. I'm ...\n",
       "8      A friend of mine bought this film for £1, and ...\n",
       "9      <br /><br />This movie is full of references. ...\n",
       "10     What happens when an army of wetbacks, towelhe...\n",
       "11     Although I generally do not like remakes belie...\n",
       "12     \\Mr. Harvey Lights a Candle\\\" is anchored by a...\n",
       "13     I had a feeling that after \\Submerged\\\", this ...\n",
       "14     note to George Litman, and others: the Mystery...\n",
       "15     Stephen King adaptation (scripted by King hims...\n",
       "16     `The Matrix' was an exciting summer blockbuste...\n",
       "17     Ulli Lommel's 1980 film 'The Boogey Man' is no...\n",
       "18     This movie is one among the very few Indian mo...\n",
       "19     Most people, especially young people, may not ...\n",
       "20     \\Soylent Green\\\" is one of the best and most d...\n",
       "21     Michael Stearns plays Mike, a sexually frustra...\n",
       "22     This happy-go-luck 1939 military swashbuckler,...\n",
       "23     I would love to have that two hours of my life...\n",
       "24     The script for this movie was probably found i...\n",
       "25     Looking for Quo Vadis at my local video store,...\n",
       "26     Note to all mad scientists everywhere: if you'...\n",
       "27     What the ........... is this ? This must, with...\n",
       "28     Intrigued by the synopsis (every gay video the...\n",
       "29     Would anyone really watch this RUBBISH if it d...\n",
       "                             ...                        \n",
       "970    This movie was disaster at Box Office, and the...\n",
       "971    8 Simple Rules is a funny show but it also has...\n",
       "972    This movie was strange... I watched it while i...\n",
       "973    *** Contains Spoilers ***<br /><br />I did not...\n",
       "974    ...for this movie defines a new low in Bollywo...\n",
       "975    This dreadful film assembles every Asian stere...\n",
       "976    The original movie, The Odd Couple, has some w...\n",
       "977    Gédéon and Jules Naudet wanted to film a docum...\n",
       "978    \\Hotel du Nord \\\" is the only Carné movie from...\n",
       "979    One of Boris Karloff's real clinkers. Essentia...\n",
       "980    It is not every film's job to stimulate you su...\n",
       "981    LOL! Not a bad way to start it. I thought this...\n",
       "982    Modern viewers know this little film primarily...\n",
       "983    Like most comments I saw this film under the n...\n",
       "984    Diana Guzman is an angry young woman. Survivin...\n",
       "985    Rated PG-13 for violence, brief sexual humor a...\n",
       "986    First of all yes I'm white, so I try to tread ...\n",
       "987    A film that is so much a 30's Warners film in ...\n",
       "988    Though I'm not the biggest fan of wirework bas...\n",
       "989    I have to totally disagree with the other comm...\n",
       "990    I picked this movie up to replace the dismal c...\n",
       "991    Usually, any film with Sylvester Stallone is u...\n",
       "992    This is a VERY entertaining movie. A few of th...\n",
       "993    Think Pierce Brosnan and you think suave, dapp...\n",
       "994    A new way to enjoy Goldsworthy's work, Rivers ...\n",
       "995    The only thing I remember about this movie are...\n",
       "996    This is a kind of movie that will stay with yo...\n",
       "997    I just didn't get this movie...Was it a musica...\n",
       "998    Granting the budget and time constraints of se...\n",
       "999    This move was on TV last night. I guess as a t...\n",
       "Name: review, Length: 1000, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts = data_train.review  + data_test.review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram of lenghts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file /opt/anaconda3/lib/python3.6/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "chars_per_words_lengths = []\n",
    "words_per_sents_lengths = []\n",
    "sents_per_docs_lengths = []\n",
    "\n",
    "# Chars per word should be on all text\n",
    "\n",
    "for text in all_texts:\n",
    "    \n",
    "    sents = sent_tokenize(clean_str(BeautifulSoup(text).get_text()))\n",
    "    sents_per_docs_lengths.append(len(sents))\n",
    "    for sent in sents:       \n",
    "    \n",
    "        words = word_tokenize(text)\n",
    "        words_per_sents_lengths.append(len(words))\n",
    "        for word in words:\n",
    "            chars_per_words_lengths.append(len(word))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdba8176908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "h_s = plt.hist(sents_per_docs_lengths, bins=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.800000000000001"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(sents_per_docs_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(sents_per_docs_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.45303783595946"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(sents_per_docs_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdba009d908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "h_w = plt.hist(words_per_sents_lengths, bins=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "677.36979797979802"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(words_per_sents_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3067"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(words_per_sents_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "361.23638615703925"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(words_per_sents_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdb9839e438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "h_c = plt.hist(chars_per_words_lengths, bins=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.8788835783566293"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(chars_per_words_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(chars_per_words_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5373921840645841"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(chars_per_words_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file /opt/anaconda3/lib/python3.6/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "all_texts = list(all_texts.apply(BeautifulSoup).apply(BeautifulSoup.get_text).apply(clean_str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file /opt/anaconda3/lib/python3.6/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "train_texts = list(data_train.review.apply(BeautifulSoup).apply(BeautifulSoup.get_text).apply(clean_str))\n",
    "test_texts = list(data_test.review.apply(BeautifulSoup).apply(BeautifulSoup.get_text).apply(clean_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build char vocab (all text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vocab_to_int, int_to_vocab = build_chars_vocab(all_texts)\n",
    "#np.savez('vocab_char-{}'.format(max_sent_len), vocab_to_int=vocab_to_int, int_to_vocab=int_to_vocab, max_sent_len=max_sent_len, min_sent_len=min_sent_len )\n",
    "char2int = vocab_to_int\n",
    "int2char = int_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_characters = sorted(list(vocab_to_int))\n",
    "target_characters = sorted(list(vocab_to_int))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in all_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 1000\n",
      "Number of unique input tokens: 108\n",
      "Number of unique output tokens: 108\n",
      "Max sequence length for inputs: 13235\n"
     ]
    }
   ],
   "source": [
    "print('Number of samples:', len(all_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\t': 2,\n",
       " '\\n': 3,\n",
       " ' ': 1,\n",
       " '!': 37,\n",
       " '#': 67,\n",
       " '$': 51,\n",
       " '%': 60,\n",
       " '&': 55,\n",
       " '(': 35,\n",
       " ')': 36,\n",
       " '*': 53,\n",
       " '+': 66,\n",
       " ',': 23,\n",
       " '-': 41,\n",
       " '.': 27,\n",
       " '/': 49,\n",
       " '0': 31,\n",
       " '1': 42,\n",
       " '2': 30,\n",
       " '3': 56,\n",
       " '4': 48,\n",
       " '5': 45,\n",
       " '6': 43,\n",
       " '7': 50,\n",
       " '8': 46,\n",
       " '9': 44,\n",
       " ':': 39,\n",
       " ';': 38,\n",
       " '<': 90,\n",
       " '=': 59,\n",
       " '>': 91,\n",
       " '?': 34,\n",
       " '@': 71,\n",
       " 'UNK': 0,\n",
       " '[': 62,\n",
       " ']': 63,\n",
       " '^': 89,\n",
       " '_': 72,\n",
       " '`': 57,\n",
       " 'a': 8,\n",
       " 'b': 28,\n",
       " 'c': 22,\n",
       " 'd': 16,\n",
       " 'e': 17,\n",
       " 'f': 12,\n",
       " 'g': 13,\n",
       " 'h': 7,\n",
       " 'i': 5,\n",
       " 'j': 19,\n",
       " 'k': 26,\n",
       " 'l': 9,\n",
       " 'm': 18,\n",
       " 'n': 15,\n",
       " 'o': 14,\n",
       " 'p': 29,\n",
       " 'q': 33,\n",
       " 'r': 21,\n",
       " 's': 10,\n",
       " 't': 6,\n",
       " 'u': 11,\n",
       " 'v': 20,\n",
       " 'w': 4,\n",
       " 'x': 32,\n",
       " 'y': 24,\n",
       " 'z': 25,\n",
       " '{': 96,\n",
       " '|': 105,\n",
       " '}': 97,\n",
       " '~': 65,\n",
       " '\\x84': 70,\n",
       " '\\x85': 64,\n",
       " '\\x91': 102,\n",
       " '\\x96': 47,\n",
       " '\\x97': 73,\n",
       " '¡': 93,\n",
       " '£': 52,\n",
       " '¨': 40,\n",
       " '®': 83,\n",
       " '´': 54,\n",
       " '·': 106,\n",
       " '½': 82,\n",
       " 'à': 81,\n",
       " 'á': 75,\n",
       " 'â': 84,\n",
       " 'ã': 98,\n",
       " 'ä': 76,\n",
       " 'æ': 92,\n",
       " 'ç': 85,\n",
       " 'è': 68,\n",
       " 'é': 58,\n",
       " 'ê': 61,\n",
       " 'ï': 80,\n",
       " 'ñ': 101,\n",
       " 'ó': 69,\n",
       " 'ô': 107,\n",
       " 'ö': 86,\n",
       " 'ø': 104,\n",
       " 'ù': 87,\n",
       " 'ü': 88,\n",
       " 'ı': 74,\n",
       " '–': 78,\n",
       " '‘': 79,\n",
       " '’': 77,\n",
       " '“': 99,\n",
       " '”': 100,\n",
       " '…': 103,\n",
       " '、': 95,\n",
       " '，': 94}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char2int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'UNK',\n",
       " 1: ' ',\n",
       " 2: '\\t',\n",
       " 3: '\\n',\n",
       " 4: 'w',\n",
       " 5: 'i',\n",
       " 6: 't',\n",
       " 7: 'h',\n",
       " 8: 'a',\n",
       " 9: 'l',\n",
       " 10: 's',\n",
       " 11: 'u',\n",
       " 12: 'f',\n",
       " 13: 'g',\n",
       " 14: 'o',\n",
       " 15: 'n',\n",
       " 16: 'd',\n",
       " 17: 'e',\n",
       " 18: 'm',\n",
       " 19: 'j',\n",
       " 20: 'v',\n",
       " 21: 'r',\n",
       " 22: 'c',\n",
       " 23: ',',\n",
       " 24: 'y',\n",
       " 25: 'z',\n",
       " 26: 'k',\n",
       " 27: '.',\n",
       " 28: 'b',\n",
       " 29: 'p',\n",
       " 30: '2',\n",
       " 31: '0',\n",
       " 32: 'x',\n",
       " 33: 'q',\n",
       " 34: '?',\n",
       " 35: '(',\n",
       " 36: ')',\n",
       " 37: '!',\n",
       " 38: ';',\n",
       " 39: ':',\n",
       " 40: '¨',\n",
       " 41: '-',\n",
       " 42: '1',\n",
       " 43: '6',\n",
       " 44: '9',\n",
       " 45: '5',\n",
       " 46: '8',\n",
       " 47: '\\x96',\n",
       " 48: '4',\n",
       " 49: '/',\n",
       " 50: '7',\n",
       " 51: '$',\n",
       " 52: '£',\n",
       " 53: '*',\n",
       " 54: '´',\n",
       " 55: '&',\n",
       " 56: '3',\n",
       " 57: '`',\n",
       " 58: 'é',\n",
       " 59: '=',\n",
       " 60: '%',\n",
       " 61: 'ê',\n",
       " 62: '[',\n",
       " 63: ']',\n",
       " 64: '\\x85',\n",
       " 65: '~',\n",
       " 66: '+',\n",
       " 67: '#',\n",
       " 68: 'è',\n",
       " 69: 'ó',\n",
       " 70: '\\x84',\n",
       " 71: '@',\n",
       " 72: '_',\n",
       " 73: '\\x97',\n",
       " 74: 'ı',\n",
       " 75: 'á',\n",
       " 76: 'ä',\n",
       " 77: '’',\n",
       " 78: '–',\n",
       " 79: '‘',\n",
       " 80: 'ï',\n",
       " 81: 'à',\n",
       " 82: '½',\n",
       " 83: '®',\n",
       " 84: 'â',\n",
       " 85: 'ç',\n",
       " 86: 'ö',\n",
       " 87: 'ù',\n",
       " 88: 'ü',\n",
       " 89: '^',\n",
       " 90: '<',\n",
       " 91: '>',\n",
       " 92: 'æ',\n",
       " 93: '¡',\n",
       " 94: '，',\n",
       " 95: '、',\n",
       " 96: '{',\n",
       " 97: '}',\n",
       " 98: 'ã',\n",
       " 99: '“',\n",
       " 100: '”',\n",
       " 101: 'ñ',\n",
       " 102: '\\x91',\n",
       " 103: '…',\n",
       " 104: 'ø',\n",
       " 105: '|',\n",
       " 106: '·',\n",
       " 107: 'ô'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int2char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(int_to_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train review model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load documents data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX_SENTS_PER_DOC = 10\n",
      "\n",
      "MAX_WORDS_PER_SENT = 40\n",
      "\n",
      "MAX_CHARS_PER_WORD = 20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "MAX_SENTS_PER_DOC = int(np.mean(sents_per_docs_lengths)) + 1\n",
    "MAX_WORDS_PER_SENT = int(np.mean(words_per_sents_lengths)) + 1\n",
    "MAX_CHARS_PER_WORD = int(np.mean(chars_per_words_lengths)) + 1\n",
    "'''\n",
    "MAX_SENTS_PER_DOC = 10\n",
    "MAX_WORDS_PER_SENT = 40\n",
    "MAX_CHARS_PER_WORD = 20\n",
    "print('MAX_SENTS_PER_DOC = ' + str(MAX_SENTS_PER_DOC) + '\\n')\n",
    "print('MAX_WORDS_PER_SENT = ' + str(MAX_WORDS_PER_SENT) + '\\n')\n",
    "print('MAX_CHARS_PER_WORD = ' + str(MAX_CHARS_PER_WORD) + '\\n')\n",
    "\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "EMBEDDING_DIM = 50\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize documents data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_data, train_targets = vectorize_sentences_data(input_texts=train_texts, \n",
    "                                                               target_labels=list(data_train.sentiment), \n",
    "                                                               max_sents_per_doc=MAX_SENTS_PER_DOC, \n",
    "                                                               max_words_per_sent=MAX_WORDS_PER_SENT, \n",
    "                                                               max_chars_per_word=MAX_CHARS_PER_WORD, \n",
    "                                                               num_classes=NUM_CLASSES, \n",
    "                                                               char2int=char2int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_data, _ = vectorize_sentences_data(input_texts=test_texts, \n",
    "                                               target_labels=None, \n",
    "                                               max_sents_per_doc=MAX_SENTS_PER_DOC, \n",
    "                                               max_words_per_sent=MAX_WORDS_PER_SENT, \n",
    "                                               max_chars_per_word=MAX_CHARS_PER_WORD, \n",
    "                                               num_classes=NUM_CLASSES, \n",
    "                                               char2int=char2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 10, 40, 20)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 2)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 10, 40, 20)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"bi...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, None)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, None, 108)         11664     \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 100)               63600     \n",
      "=================================================================\n",
      "Total params: 75,264\n",
      "Trainable params: 63,600\n",
      "Non-trainable params: 11,664\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"la...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 40, 20)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 40, 100)           75264     \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection [(None, 40, 256), (None,  234496    \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 256)               0         \n",
      "=================================================================\n",
      "Total params: 309,760\n",
      "Trainable params: 298,096\n",
      "Non-trainable params: 11,664\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:47: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"la...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 10, 40, 20)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 40, 20)       0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 40, 20)       0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 40, 20)       0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 40, 20)       0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 40, 20)       0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, 40, 20)       0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_8 (Lambda)               (None, 40, 20)       0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_9 (Lambda)               (None, 40, 20)       0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_10 (Lambda)              (None, 40, 20)       0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_11 (Lambda)              (None, 40, 20)       0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "model_2 (Model)                 (None, 256)          309760      lambda_2[0][0]                   \n",
      "                                                                 lambda_3[0][0]                   \n",
      "                                                                 lambda_4[0][0]                   \n",
      "                                                                 lambda_5[0][0]                   \n",
      "                                                                 lambda_6[0][0]                   \n",
      "                                                                 lambda_7[0][0]                   \n",
      "                                                                 lambda_8[0][0]                   \n",
      "                                                                 lambda_9[0][0]                   \n",
      "                                                                 lambda_10[0][0]                  \n",
      "                                                                 lambda_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 1, 256)       0           model_2[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 1, 256)       0           model_2[2][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 1, 256)       0           model_2[3][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_4 (Reshape)             (None, 1, 256)       0           model_2[4][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_5 (Reshape)             (None, 1, 256)       0           model_2[5][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_6 (Reshape)             (None, 1, 256)       0           model_2[6][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_7 (Reshape)             (None, 1, 256)       0           model_2[7][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_8 (Reshape)             (None, 1, 256)       0           model_2[8][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_9 (Reshape)             (None, 1, 256)       0           model_2[9][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_10 (Reshape)            (None, 1, 256)       0           model_2[10][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 10, 256)      0           reshape_1[0][0]                  \n",
      "                                                                 reshape_2[0][0]                  \n",
      "                                                                 reshape_3[0][0]                  \n",
      "                                                                 reshape_4[0][0]                  \n",
      "                                                                 reshape_5[0][0]                  \n",
      "                                                                 reshape_6[0][0]                  \n",
      "                                                                 reshape_7[0][0]                  \n",
      "                                                                 reshape_8[0][0]                  \n",
      "                                                                 reshape_9[0][0]                  \n",
      "                                                                 reshape_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) [(None, 10, 256), (N 394240      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_12 (Lambda)              (None, 256)          0           bidirectional_3[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 704,000\n",
      "Trainable params: 692,336\n",
      "Non-trainable params: 11,664\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 10, 40, 20)        0         \n",
      "_________________________________________________________________\n",
      "model_3 (Model)              (None, 256)               704000    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 704,514\n",
      "Trainable params: 692,850\n",
      "Non-trainable params: 11,664\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "char2word_latent_dim = 50\n",
    "word2sent_latent_dim = 128\n",
    "sent2doc_latent_dim = 128\n",
    "char_vocab_size = len(char2int)\n",
    "\n",
    "#MAX_SENTS_PER_DOC = 11\n",
    "#MAX_WORDS_PER_SENT = 24\n",
    "#MAX_CHARS_PER_WORD = 5\n",
    "#_, _, _, encoder_word_embedding_model = build_chars2word_model(num_encoder_tokens=char_vocab_size, latent_dim=chars2word_latent_dim)\n",
    "encoder_word_embedding_model = build_chars2word_model_simple_BiLSTM(num_encoder_tokens=char_vocab_size, latent_dim=char2word_latent_dim)\n",
    "print(encoder_word_embedding_model.summary())\n",
    "'''\n",
    "_, _, _, encoder_sentence_embedding_model = build_words2sent_model(encoder_word_embedding_model, \n",
    "                                                                   max_words_seq_len=MAX_WORDS_PER_SENT, \n",
    "                                                                   max_char_seq_len=MAX_CHARS_PER_WORD,\n",
    "                                                                   latent_dim=words2sent_latent_dim)\n",
    "'''\n",
    "encoder_sentence_embedding_model = build_words2sent_model_simple_BiLSTM(encoder_word_embedding_model, \n",
    "                                                                   max_words_seq_len=MAX_WORDS_PER_SENT, \n",
    "                                                                   max_char_seq_len=MAX_CHARS_PER_WORD, \n",
    "                                                                   latent_dim=word2sent_latent_dim)\n",
    "print(encoder_sentence_embedding_model.summary())\n",
    "\n",
    "encoder_document_embedding_model = build_sent2doc_model(encoder_sentence_embedding_model, \n",
    "                                                 max_sents_seq_len=MAX_SENTS_PER_DOC, \n",
    "                                                 max_words_seq_len=MAX_WORDS_PER_SENT, \n",
    "                                                 max_char_seq_len=MAX_CHARS_PER_WORD, \n",
    "                                                 word2sent_latent_dim=word2sent_latent_dim,\n",
    "                                                 sent2doc_latent_dim=sent2doc_latent_dim)\n",
    "print(encoder_document_embedding_model.summary())\n",
    "model = build_hier_senti_model(encoder_document_embedding_model=encoder_document_embedding_model,\n",
    "                                max_sents_seq_len=MAX_SENTS_PER_DOC, \n",
    "                                max_words_seq_len=MAX_WORDS_PER_SENT, \n",
    "                                max_char_seq_len=MAX_CHARS_PER_WORD)\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "800/800 [==============================] - 47s 59ms/step - loss: 0.0056 - categorical_accuracy: 0.5962 - val_loss: 0.0000e+00 - val_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00001: val_categorical_accuracy improved from -inf to 1.00000, saving model to best_hier_senti_model-10-40.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fda61016860>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 1\n",
    "lr = 0.01\n",
    "\n",
    "model.compile(optimizer=optimizers.Adam(lr=lr), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "#filepath=\"weights-improvement-{epoch:02d}-{val_categorical_accuracy:.2f}.hdf5\"\n",
    "filepath=\"best_hier_senti_model-{}-{}.hdf5\".format(MAX_SENTS_PER_DOC,MAX_WORDS_PER_SENT,MAX_CHARS_PER_WORD) # Save only the best model for inference step, as saving the epoch and metric might confuse the inference function which model to use\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_categorical_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "tbCallBack = TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "callbacks_list = [checkpoint, tbCallBack]\n",
    "#callbacks_list = [checkpoint, tbCallBack, lrate]\n",
    "model.fit(train_input_data, train_targets,\n",
    "          #validation_data=(test_input_data, test_targets)\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=callbacks_list,\n",
    "          validation_split=0.2,\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naturally in a film whos main themes are of mortality, nostalgia, and loss of innocence it is perhaps not surprising that it is rated more highly by older viewers than younger ones. however there is a craftsmanship and completeness to the film which anyone can enjoy. the pace is steady and constant, the characters full and engaging, the relationships and interactions natural showing that you do not need floods of tears to show emotion, screams to show fear, shouting to show dispute or violence to show anger. naturally joyces short story lends the film a ready made structure as perfect as a polished diamond, but the small changes huston makes such as the inclusion of the poem fit in neatly. it is truly a masterpiece of tact, subtlety and overwhelming beauty.\n",
      "Sentiment: [[ 0.50165802  0.49834198]]\n",
      "this movie is a disaster within a disaster film. it is full of great action scenes, which are only meaningful if you throw away all sense of reality. lets see, word to the wise, lava burns you; steam burns you. you cant stand next to lava. diverting a minor lava flow is difficult, let alone a significant one. scares me to think that some might actually believe what they saw in this movie.even worse is the significant amount of talent that went into making this film. i mean the acting is actually very good. the effects are above average. hard to believe somebody read the scripts for this and allowed all this talent to be wasted. i guess my suggestion would be that if this movie is about to start on tv ... look away! it is like a train wreck: it is so awful that once you know what is coming, you just have to watch. look away and spend your time on more meaningful content.\n",
      "Sentiment: [[ 0.50165802  0.49834198]]\n",
      "all in all, this is a movie for kids. we saw it tonight and my child loved it. at one point my kids excitement was so great that sitting was impossible. however, i am a great fan of a.a. milnes books which are very subtle and hide a wry intelligence behind the childlike quality of its leading characters. this film was not subtle. it seems a shame that disney cannot see the benefit of making movies from more of the stories contained in those pages, although perhaps, it doesnt have the permission to use them. i found myself wishing the theater was replaying winnie-the-pooh and tigger too, instead. the characters voices were very good. i was only really bothered by kanga. the music, however, was twice as loud in parts than the dialog, and incongruous to the film.as for the story, it was a bit preachy and militant in tone. overall, i was disappointed, but i would go again just to see the same excitement on my childs face.i liked lumpys laugh....\n",
      "Sentiment: [[ 0.50165802  0.49834198]]\n",
      "afraid of the dark left me with the impression that several different screenplays were written, all too short for a feature length film, then spliced together clumsily into this frankensteins monster.at his best, the protagonist, lucas, is creepy. as hard as it is to draw a bead on the secondary characters, theyre far more sympathetic.afraid of the dark could have achieved mediocrity had it taken just one approach and seen it through -- and had it made lucas simply psychotic and confused instead of ghoulish and off-putting. i wanted to see him packed off into an asylum so the rest of the characters could have a normal life.\n",
      "Sentiment: [[ 0.50165802  0.49834198]]\n",
      "a very accurate depiction of small time mob life filmed in new jersey. the story, characters and script are believable but the acting drops the ball. still, its worth watching, especially for the strong images, some still with me even though i first viewed this 25 years ago.a young hood steps up and starts doing bigger things (tries to) but these things keep going wrong, leading the local boss to suspect that his end is being skimmed off, not a good place to be if you enjoy your health, or life.this is the film that introduced joe pesce to martin scorsese. also present is that perennial screen wise guy, frank vincent. strong on characterizations and visuals. sound muddled and much of the acting is amateurish, but a great story.\n",
      "Sentiment: [[ 0.50165802  0.49834198]]\n",
      "...as valuable as king tuts tomb! (ok, maybe not that valuable, but worth hunting down if you can). i notice no one has commented on this movie for some years, and i hope a fresh post will spark some new comments. this is a film that i remembered only snippets of from childhood, and only saw recently when i tired of waiting for fox to honour its own past, and hunted down the korean dvd (in english, but with unremovable korean subtitles). i wont go through another long plot description - suffice to say that seeing it for the first time in its proper widescreen format left me agape at the vistas and the scope of the film. the matte paintings still hold up, and the palace sets are truly breathtaking. but it is the smaller scale details that lend this film its depth and richness, offering a glimpse into the lifestyles of egypts poor as well as its elite. the bazaars, hovels, docks, embalming houses, and taverns are as fascinating as pharaohs throne room. while errors abound on the large scale (most notably the dynastic succession), the details are more meticulously researched than the vast majority of hollywoods films. visually, its not without its flaws - the interiors are often too overly lit and colourful to blend seamlessly with the exteriors. nevertheless, this is a movie that should be credited for being as audacious in the small as it is in the large. tedious? in parts, absolutely. overacted? underacted? yes, both - though understated might be a more apt description. too long? absolutely not. i wished they had spent more time with sinuhes experiences in the house of death, and among the hittites, and less with his romance with nefer, though. historically inaccurate? yes, that too, but so was shakespeare. nobody chastises him for it. i appreciate historical accuracy as much as the next guy, but ultimately it has to be remembered that cinema is theater, not a history lesson.\n",
      "Sentiment: [[ 0.50165802  0.49834198]]\n",
      "this has to be one of the biggest misfires ever...the script was nice and could have ended a lot better.the actors should have played better and maybe then i would have given this movie a slightly better grade. maybe hollywood should remake this movie with some little better actors and better director.sorry guys for disappointment but the movie is bad.if i had to re-watch it it would be like torture. i dont want to spoil everyones opinion with mine so..my advice is watch the movie first..see if u like it and after vote(do not vote before you watch it ! ) and by the way... have fun watching it ! dont just peek...watch it till the end :))))))))) !!\n",
      "Sentiment: [[ 0.50165802  0.49834198]]\n",
      "this is one of those movies i watched, and wondered, why did i watch it? what did i find so interesting about it? being a truck driver myself, i didnt find it very realistic. no, ive never used a lot lizard, nor have i ever seen, nor heard about one traveling around the country in a brand new seventy thousand dollar rv, either.same thing about a pimp whom has never sampled the lady in question (until the end of the movie, and well, he still really didnt...), and only getting 50 bucks a cut, when the prostitute gets $200.00 (well, $150.00 after his cut, yeah...).i still laugh at the lot lizard comment ivey made (thems lot lizards, theyll screw anything with 20 bucks, and some are men dressed as woman... or something equally as weird), meaning, were better then them, as we may still be prostitutes, but we get paid better.other then that, its just a story of a young woman whom wanted something more from life then a dead end job while living at home (shes 18, remember?) and embarrassed by her mother basically doing the same thing (dead end job). at least she had a roof over her head and a job. she turned five tricks on the road... i wonder if the $750.00 she made was worth it? id guess not.\n",
      "Sentiment: [[ 0.50165802  0.49834198]]\n",
      "the worst movie ive seen in years (and ive seen a lot of movies). acting is terrible, there is no plot whatsoever, there is no point whatsoever, i felt robbed after i rented this movie. they recommended it to me mind you! a disgrace for terrible movies! stay away from this terrible piece of c**p. save your money !\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: [[ 0.50165802  0.49834198]]\n",
      "five medical students (kevin bacon, david labraccio; william baldwin, dr. joe hurley; oliver platt, randy steckle; julia roberts, dr. rachel mannus; kiefer sutherland, nelson) experiment with clandestine near death & afterlife experiences, (re)searching for medical & personal enlightenment. one by one, each medical students heart is stopped, then revived.under temporary death spells each experiences bizarre visions, including forgotten childhood memories. their flashbacks are like childrens nightmares. the revived students are disturbed by remembering regretful acts they had committed or had done against them. as they experience afterlife, they bring real life experiences back into the present. as they continue to experiment, their remembrances dramatically intensify; so much so, some are physically overcome. thus, they probe & transcend deeper into the death-afterlife experiences attempting to find a cure.even though the dvd was released in 2007, this motion picture was released in 1990. therefore, kevin bacon, william baldwin, julia roberts & kiefer sutherland were in the early stages of their adult acting careers. besides the plot being extremely intriguing, the suspense building to a dramatic climax & the script being tight & convincing, all of the young actors make flatliners, what is now an all-star cult semi-sci-fi suspense. who knew 17 years ago that the film careers of this young group of actors would skyrocket? i suspect that director joel schumacher did.\n",
      "Sentiment: [[ 0.50165802  0.49834198]]\n",
      "the mill on the floss was one of the lesser novels by mary ann evans, who wrote under the male pseudonym george eliot. i tried to read this dull and very turgid novel years ago, but was unable to finish it. ill review this film version solely on its own merits, as i dont know how faithfully it follows the original novel.the films opening credits are printed in an old english typeface that suggests the mediaeval period, and so its a very poor choice for a film with a 19th-century setting. (on the other hand, about halfway into the film, we see a close-up shot of a handbill advertising an estate auction. this handbill is set in authentic victorian type fonts, and looks *very* convincing.) most of this film is extremely convincing in its depiction of the architecture and clothing of early 19th-century england. the precise location of this films story is never disclosed, but - judging by the actors accents - id place it as somewhere in the cotswolds, perhaps warwickshire.the plot, what there is of it, involves a mill that changes hands a couple of times (over a couple of decades) between two rival families, one wealthy and one working-class. i disagree with another imdb reviewer who claims that james mason has only a small role in this film. mason has the largest and most central role in this drama, as the scion of the wealthier family. as the spoilt and petulant tom tulliver, mason is darkly brooding and impetuous. his performance here belongs in a better film: it made me want to see wuthering heights recast with mason as heathcliff.as this is a multi-generational saga (something which george eliot did much better in middlemarch), several of the main roles in this film are split among two actors apiece: child actors in the prologue, adults in the main narrative. the prologue of this film features a very well-written scene, establishing tom tulliver as wilful and bully-ragging from an early age, and young philip wakeham as decent and thoughtful. through hard labour, philip has earned a halfpenny: tom tries to bully it away from him, but is unwilling to take the coin by brute force: he wants philip to *give* it to him. all the child actors in this movie, male and female, are talented and attractive. unfortunately, all of the children speak their dialogue in posh plummy-voiced accents that are utterly unlike the accents of the actors and actresses who play those same roles as adults. this discrepancy calls attention to the staginess of the material. regrettably, none of the later scenes are as good as this prologue.the climax features a crowd of labourers in a rainstorm, much better paced and photographed than the earlier scenes. but modern viewers (in britain, at least) can no longer take this sort of material seriously. by now, practically every british comedian has done a trouble at t mill, squire comedy routine, parodying precisely this subject matter, so i had difficulty watching this movie with a straight face.the character actress martita hunt is good in a small role, but the opening credits (in that old english typeface) misspell her forename as marita. ill rate this dull movie 3 points out of 10: one point apiece for james masons performance, the early scene with the children, and the authentic victorian typesetting in that auctioneers handbill.\n",
      "Sentiment: [[ 0.50165802  0.49834198]]\n",
      "i just saw this film at the phoenix film festival today and loved it. the synopsis was listed in our program as an old shakespearean actor invites his three children to his suicide party. i wasnt sure if i was going to see it because when i read about it i liked the idea of a suicide party it sounded very interesting to me, but old shakespearean actor had me worried that the film would be kind of dry and boring. but i decided to give it a try. i am glad that i did. it was not dry and boring in the least, that dialogue was great, funny in a clever way, but not pretentious and difficult to understand. peter falk was terrific in this role, he stole the show. i also was pleasantly surprised by laura san giacomos performance, usually she bugs me, but i enjoyed watching her in this film very much. i think judge reinholds part could have been done better by another actor, at times he seemed kind of cheesy and it looked like acting, not like you were just watching this character. but the movie was so good i was able to forgive one actors awkwardness. i would recommend this film to anyone and have already told a few people to see it as soon as it is available to the general public. who knew suicide could be so hilarious?\n",
      "Sentiment: [[ 0.50165802  0.49834198]]\n",
      "the love letter is one of those movies that could have been really clever, but they wasted it. focusing on a letter wreaking havoc in a small town, the movie has an all-star cast with nothing to do. tom selleck and alice drummond had so recently co-starred in the super-hilarious in & out (also about an upset in a small town), in which they were both great, but here they look as though theyre getting drug all over the place. i cant tell what the people behind the camera are trying to do here (if anything), but they sure didnt accomplish anything. how tragic, that a potential laugh riot got so sorrowfully wasted.\n",
      "Sentiment: [[ 0.50165802  0.49834198]]\n",
      "another fantastic offering from the monkey island team and though it was a long time coming and had to survive the departure of ron gilbert its another worthy installment. my only gripe is that it was a little short seeming in comparison to the previous two, though that might be because of a glorious lack of disk-swapping. roll on mi4.\n",
      "Sentiment: [[ 0.50165802  0.49834198]]\n",
      "this was included on the disk shorts: volume 2--a rather dull collection of short films. shorts are among my favorite style of films but somehow the people assembling this second collection had a hard time finding quality content--and it wasnt nearly as good as the first volume or other shorts collections. this short film feels like its woefully incomplete. there is a story, but so much in unanswered that the viewer, like me, feels a bit left out and unfulfilled.the film begins with a woman, her boyfriend and her westie (thats a dog, by the way) going to a lonely beach. the lady speaks with an accent that, at times, is a bit difficult to follow. given that i am hard of hearing, i sure would have loved if it had been closed captioned. anyway, the boyfriend goes for a swim while she naps. when she awakens, her dog is gone. she panics and makes the guy follow her all about looking for the dog. they spend most of the time arguing and being disagreeable. then, out of the blue, they stop to have sex. later, they find the dog--end of story.as far as the characters go, both seemed rather dysfunctional and unlikable. she was a fussy and demanding lady and he seemed to have contempt for her. when you wondered why they were together, their little sex break showed what bond kept them together.some might like the characterizations--i kept finding the people irritating and unreal--more like caricatures than people you might meet or know. also, the payoff for all this just isnt worth the wait (unless you want to see the guy naked).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: [[ 0.50165802  0.49834198]]\n",
      "im not really much of an abbott & costello fan (although i do enjoy whos on first) and, to be honest, there wasnt much in this movie that would inspire me to watch any more of their work. it wasnt really bad. it had some mildly amusing scenes, and actually a very convincing giant played by buddy baer, but somehow, given the fame of the duo and the esteem in which theyre generally held, i have to say i was expecting more. as the story goes, the pair stumble into a babysitting job, and during the reading of jack & the beanstalk as a bedtime story (with the kid reading it to costello), costellos jack falls asleep and dreams himself into the story. theres a wizard of oz kind of feel to the story, in that the characters in the dream are all the equivalents of real-life acquaintances of jack, and the movie opens in black & white and shifts to colour during the dream sequence. the fight scenes between jack and the giant and the dance scene between jack and polly (dorothy ford) are among the amusing parts of the movie. polly, of course, also leads to one of the questions of the movie - what happened to her? jack and gang apparently left her behind in the giants castle! i know - it was just a dream, so who cares. still - i wondered. there were also a couple of cute song and dance routines. my 4 year old giggled a bit during this, so she was able to appreciate some of the humour. i found it to be an acceptable timewaster, but certainly not anything that would convince you of abbott and costello as comic geniuses. 4/10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-12ac0f5943d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrev\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_texts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0msentiment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_input_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Sentiment: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentiment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1165\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1166\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1167\u001b[0;31m                                             steps=steps)\n\u001b[0m\u001b[1;32m   1168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1169\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2664\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m                                 session)\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i, rev in enumerate(test_texts):\n",
    "    print(rev)\n",
    "    sentiment = model.predict(np.expand_dims(test_input_data[i], 0))\n",
    "    print('Sentiment: ' + str(sentiment))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
