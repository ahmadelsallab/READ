{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ULMFiT.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"x0bGro57Vzvp","colab_type":"text"},"source":["# Get data"]},{"cell_type":"code","metadata":{"id":"5zBkgEdsV1Zv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":122},"outputId":"324c421c-62f9-40a2-adc6-aba1ca78ef1f","executionInfo":{"status":"ok","timestamp":1557310970252,"user_tz":-120,"elapsed":2516,"user":{"displayName":"Ahmad El Sallab","photoUrl":"","userId":"06251939449358079201"}}},"source":["!git clone https://github.com/khumbuai/ulmfit_keras.git"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Cloning into 'ulmfit_keras'...\n","remote: Enumerating objects: 1, done.\u001b[K\n","remote: Counting objects: 100% (1/1)   \u001b[K\rremote: Counting objects: 100% (1/1), done.\u001b[K\n","Receiving objects:   0% (1/536)   \rReceiving objects:   1% (6/536)   \rReceiving objects:   2% (11/536)   \rReceiving objects:   3% (17/536)   \rReceiving objects:   4% (22/536)   \rReceiving objects:   5% (27/536)   \rReceiving objects:   6% (33/536)   \rReceiving objects:   7% (38/536)   \rReceiving objects:   8% (43/536)   \rReceiving objects:   9% (49/536)   \rReceiving objects:  10% (54/536)   \rReceiving objects:  11% (59/536)   \rReceiving objects:  12% (65/536)   \rReceiving objects:  13% (70/536)   \rReceiving objects:  14% (76/536)   \rReceiving objects:  15% (81/536)   \rReceiving objects:  16% (86/536)   \rReceiving objects:  17% (92/536)   \rReceiving objects:  18% (97/536)   \rReceiving objects:  19% (102/536)   \rReceiving objects:  20% (108/536)   \rReceiving objects:  21% (113/536)   \rReceiving objects:  22% (118/536)   \rReceiving objects:  23% (124/536)   \rReceiving objects:  24% (129/536)   \rReceiving objects:  25% (134/536)   \rReceiving objects:  26% (140/536)   \rReceiving objects:  27% (145/536)   \rReceiving objects:  28% (151/536)   \rReceiving objects:  29% (156/536)   \rReceiving objects:  30% (161/536)   \rReceiving objects:  31% (167/536)   \rReceiving objects:  32% (172/536)   \rReceiving objects:  33% (177/536)   \rReceiving objects:  34% (183/536)   \rremote: Total 536 (delta 0), reused 0 (delta 0), pack-reused 535\u001b[K\n","Receiving objects:  35% (188/536)   \rReceiving objects:  36% (193/536)   \rReceiving objects:  37% (199/536)   \rReceiving objects:  38% (204/536)   \rReceiving objects:  39% (210/536)   \rReceiving objects:  40% (215/536)   \rReceiving objects:  41% (220/536)   \rReceiving objects:  42% (226/536)   \rReceiving objects:  43% (231/536)   \rReceiving objects:  44% (236/536)   \rReceiving objects:  45% (242/536)   \rReceiving objects:  46% (247/536)   \rReceiving objects:  47% (252/536)   \rReceiving objects:  48% (258/536)   \rReceiving objects:  49% (263/536)   \rReceiving objects:  50% (268/536)   \rReceiving objects:  51% (274/536)   \rReceiving objects:  52% (279/536)   \rReceiving objects:  53% (285/536)   \rReceiving objects:  54% (290/536)   \rReceiving objects:  55% (295/536)   \rReceiving objects:  56% (301/536)   \rReceiving objects:  57% (306/536)   \rReceiving objects:  58% (311/536)   \rReceiving objects:  59% (317/536)   \rReceiving objects:  60% (322/536)   \rReceiving objects:  61% (327/536)   \rReceiving objects:  62% (333/536)   \rReceiving objects:  63% (338/536)   \rReceiving objects:  64% (344/536)   \rReceiving objects:  65% (349/536)   \rReceiving objects:  66% (354/536)   \rReceiving objects:  67% (360/536)   \rReceiving objects:  68% (365/536)   \rReceiving objects:  69% (370/536)   \rReceiving objects:  70% (376/536)   \rReceiving objects:  71% (381/536)   \rReceiving objects:  72% (386/536)   \rReceiving objects:  73% (392/536)   \rReceiving objects:  74% (397/536)   \rReceiving objects:  75% (402/536)   \rReceiving objects:  76% (408/536)   \rReceiving objects:  77% (413/536)   \rReceiving objects:  78% (419/536)   \rReceiving objects:  79% (424/536)   \rReceiving objects:  80% (429/536)   \rReceiving objects:  81% (435/536)   \rReceiving objects:  82% (440/536)   \rReceiving objects:  83% (445/536)   \rReceiving objects:  84% (451/536)   \rReceiving objects:  85% (456/536)   \rReceiving objects:  86% (461/536)   \rReceiving objects:  87% (467/536)   \rReceiving objects:  88% (472/536)   \rReceiving objects:  89% (478/536)   \rReceiving objects:  90% (483/536)   \rReceiving objects:  91% (488/536)   \rReceiving objects:  92% (494/536)   \rReceiving objects:  93% (499/536)   \rReceiving objects:  94% (504/536)   \rReceiving objects:  95% (510/536)   \rReceiving objects:  96% (515/536)   \rReceiving objects:  97% (520/536)   \rReceiving objects:  98% (526/536)   \rReceiving objects:  99% (531/536)   \rReceiving objects: 100% (536/536)   \rReceiving objects: 100% (536/536), 108.75 KiB | 2.22 MiB/s, done.\n","Resolving deltas:   0% (0/343)   \rResolving deltas:  11% (40/343)   \rResolving deltas:  12% (42/343)   \rResolving deltas:  14% (49/343)   \rResolving deltas:  15% (53/343)   \rResolving deltas:  20% (71/343)   \rResolving deltas:  21% (75/343)   \rResolving deltas:  24% (84/343)   \rResolving deltas:  32% (112/343)   \rResolving deltas:  38% (131/343)   \rResolving deltas:  40% (139/343)   \rResolving deltas:  42% (146/343)   \rResolving deltas:  43% (149/343)   \rResolving deltas:  45% (157/343)   \rResolving deltas:  46% (161/343)   \rResolving deltas:  47% (164/343)   \rResolving deltas:  48% (165/343)   \rResolving deltas:  53% (184/343)   \rResolving deltas:  55% (190/343)   \rResolving deltas:  58% (200/343)   \rResolving deltas:  59% (203/343)   \rResolving deltas:  60% (206/343)   \rResolving deltas:  62% (213/343)   \rResolving deltas:  63% (217/343)   \rResolving deltas:  65% (223/343)   \rResolving deltas:  68% (235/343)   \rResolving deltas:  69% (240/343)   \rResolving deltas:  72% (249/343)   \rResolving deltas:  77% (265/343)   \rResolving deltas:  81% (280/343)   \rResolving deltas:  82% (283/343)   \rResolving deltas:  83% (287/343)   \rResolving deltas:  88% (304/343)   \rResolving deltas:  89% (306/343)   \rResolving deltas:  90% (310/343)   \rResolving deltas:  91% (313/343)   \rResolving deltas:  93% (321/343)   \rResolving deltas:  94% (323/343)   \rResolving deltas:  95% (326/343)   \rResolving deltas:  97% (334/343)   \rResolving deltas:  98% (337/343)   \rResolving deltas:  99% (340/343)   \rResolving deltas: 100% (343/343)   \rResolving deltas: 100% (343/343), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KV9CrHRY3W0F","colab_type":"code","colab":{}},"source":["import os\n","os.chdir('./ulmfit_keras/')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4wh17OMs3trs","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":580},"outputId":"64279026-9889-4767-b9e9-2b81902c8b1c","executionInfo":{"status":"ok","timestamp":1557311012590,"user_tz":-120,"elapsed":44839,"user":{"displayName":"Ahmad El Sallab","photoUrl":"","userId":"06251939449358079201"}}},"source":["!chmod -R 777 *\n","!./get_data.sh"],"execution_count":3,"outputs":[{"output_type":"stream","text":["=== Acquiring datasets ===\n","---\n","- Downloading WikiText-2 (WT2)\n","- Downloading WikiText-103 (WT2)\n","--2019-05-08 10:22:52--  https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-v1.zip\n","Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.132.229\n","Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.132.229|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 190229076 (181M) [application/zip]\n","Saving to: ‘wikitext-103-v1.zip’\n","\n","wikitext-103-v1.zip 100%[===================>] 181.42M  78.5MB/s    in 2.3s    \n","\n","2019-05-08 10:22:55 (78.5 MB/s) - ‘wikitext-103-v1.zip’ saved [190229076/190229076]\n","\n","- Downloading enwik8 (Character)\n","--2019-05-08 10:23:02--  http://mattmahoney.net/dc/enwik8.zip\n","Resolving mattmahoney.net (mattmahoney.net)... 67.195.197.75\n","Connecting to mattmahoney.net (mattmahoney.net)|67.195.197.75|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 36445475 (35M) [application/zip]\n","Saving to: ‘enwik8.zip’\n","\n","enwik8.zip          100%[===================>]  34.76M  2.04MB/s    in 17s     \n","\n","2019-05-08 10:23:19 (2.02 MB/s) - ‘enwik8.zip’ saved [36445475/36445475]\n","\n","python3: can't open file 'prep_enwik8.py': [Errno 2] No such file or directory\n","- Downloading Penn Treebank (PTB)\n","- Downloading Penn Treebank (Character)\n","---\n","Happy language modeling :)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qI4JaOlz34BX","colab_type":"code","colab":{}},"source":["!mkdir keras_lm/assets"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"R11fdlSu4nVS","colab_type":"code","colab":{}},"source":["!cp -rf data/* keras_lm/assets"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1ft8xrje47e4","colab_type":"code","colab":{}},"source":["!rm -rf data"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZTTO5XR7WPK2","colab_type":"code","colab":{}},"source":["import os\n","os.chdir('./keras_lm')\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y0yJTBIsVzvw","colab_type":"text"},"source":["## Datasets"]},{"cell_type":"markdown","metadata":{"id":"IjbiL9ovVzwG","colab_type":"text"},"source":["## Pretrained models"]},{"cell_type":"code","metadata":{"id":"WAfOyHhxVzwJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":210},"outputId":"e07d1d0a-88fe-4c35-ae6e-956f4bc0c334","executionInfo":{"status":"ok","timestamp":1557311024072,"user_tz":-120,"elapsed":56309,"user":{"displayName":"Ahmad El Sallab","photoUrl":"","userId":"06251939449358079201"}}},"source":["\n","!wget -P assets/wikitext-103 http://files.fast.ai/models/wt103/itos_wt103.pkl"],"execution_count":8,"outputs":[{"output_type":"stream","text":["--2019-05-08 10:23:43--  http://files.fast.ai/models/wt103/itos_wt103.pkl\n","Resolving files.fast.ai (files.fast.ai)... 67.205.15.147\n","Connecting to files.fast.ai (files.fast.ai)|67.205.15.147|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 4161252 (4.0M) [text/plain]\n","Saving to: ‘assets/wikitext-103/itos_wt103.pkl’\n","\n","\ritos_wt103.pkl        0%[                    ]       0  --.-KB/s               \ritos_wt103.pkl       82%[===============>    ]   3.29M  15.7MB/s               \ritos_wt103.pkl      100%[===================>]   3.97M  18.4MB/s    in 0.2s    \n","\n","2019-05-08 10:23:43 (18.4 MB/s) - ‘assets/wikitext-103/itos_wt103.pkl’ saved [4161252/4161252]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RiyLJLifVzwV","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":210},"outputId":"3146962a-5462-4e03-904f-30686c2dacdb","executionInfo":{"status":"ok","timestamp":1557311029786,"user_tz":-120,"elapsed":62022,"user":{"displayName":"Ahmad El Sallab","photoUrl":"","userId":"06251939449358079201"}}},"source":["!wget -P assets/weights/ http://files.fast.ai/models/wt103/fwd_wt103.h5"],"execution_count":9,"outputs":[{"output_type":"stream","text":["--2019-05-08 10:23:44--  http://files.fast.ai/models/wt103/fwd_wt103.h5\n","Resolving files.fast.ai (files.fast.ai)... 67.205.15.147\n","Connecting to files.fast.ai (files.fast.ai)|67.205.15.147|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 462387687 (441M) [text/plain]\n","Saving to: ‘assets/weights/fwd_wt103.h5’\n","\n","fwd_wt103.h5        100%[===================>] 440.97M  99.6MB/s    in 4.6s    \n","\n","2019-05-08 10:23:49 (95.5 MB/s) - ‘assets/weights/fwd_wt103.h5’ saved [462387687/462387687]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mR26vgSlVzwe","colab_type":"code","colab":{}},"source":["#!wget -xP assets/weights/  http://files.fast.ai/models/wt103/fwd_wt103_enc.h5\n","#!wget -xP assets/weights/  http://files.fast.ai/models/wt103/bwd_wt103_enc.h5\n","#!wget -xP assets/weights/  http://files.fast.ai/models/wt103/bwd_wt103.h5"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_kjyZQH1Vzwk","colab_type":"text"},"source":["## Install packages"]},{"cell_type":"code","metadata":{"id":"sGWaev_UVzwl","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":914},"outputId":"11ac24b7-653e-4aaa-b359-8391fe42ec8f","executionInfo":{"status":"ok","timestamp":1557311100151,"user_tz":-120,"elapsed":132383,"user":{"displayName":"Ahmad El Sallab","photoUrl":"","userId":"06251939449358079201"}}},"source":["!pip install torch_nightly -f https://download.pytorch.org/whl/nightly/cu92/torch_nightly.html\n","!pip install fastai"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Looking in links: https://download.pytorch.org/whl/nightly/cu92/torch_nightly.html\n","Collecting torch_nightly\n","\u001b[?25l  Downloading https://download.pytorch.org/whl/nightly/cu92/torch_nightly-1.0.0.dev20181206-cp36-cp36m-linux_x86_64.whl (576.2MB)\n","\u001b[K     |████████████████████████████████| 576.2MB 22kB/s \n","\u001b[?25hInstalling collected packages: torch-nightly\n","Successfully installed torch-nightly-1.0.0.dev20181206\n","Requirement already satisfied: fastai in /usr/local/lib/python3.6/dist-packages (1.0.52)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from fastai) (0.2.2.post3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from fastai) (2.21.0)\n","Requirement already satisfied: bottleneck in /usr/local/lib/python3.6/dist-packages (from fastai) (1.2.1)\n","Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.6/dist-packages (from fastai) (1.16.3)\n","Requirement already satisfied: typing in /usr/local/lib/python3.6/dist-packages (from fastai) (3.6.6)\n","Requirement already satisfied: fastprogress>=0.1.19 in /usr/local/lib/python3.6/dist-packages (from fastai) (0.1.21)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from fastai) (19.0)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from fastai) (4.3.0)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from fastai) (0.6)\n","Requirement already satisfied: nvidia-ml-py3 in /usr/local/lib/python3.6/dist-packages (from fastai) (7.352.0)\n","Requirement already satisfied: spacy>=2.0.18 in /usr/local/lib/python3.6/dist-packages (from fastai) (2.0.18)\n","Requirement already satisfied: numexpr in /usr/local/lib/python3.6/dist-packages (from fastai) (2.6.9)\n","Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from fastai) (1.1.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from fastai) (3.13)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from fastai) (0.24.2)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from fastai) (4.6.3)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from fastai) (1.2.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from fastai) (3.0.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision->fastai) (1.12.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->fastai) (2019.3.9)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->fastai) (2.8)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->fastai) (1.24.2)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->fastai) (3.0.4)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->fastai) (2.4.0)\n","Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow->fastai) (0.46)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (2.0.2)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (1.0.2)\n","Requirement already satisfied: regex==2018.01.10 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (2018.1.10)\n","Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (2.0.1)\n","Requirement already satisfied: thinc<6.13.0,>=6.12.1 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (6.12.1)\n","Requirement already satisfied: ujson>=1.35 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (1.35)\n","Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (0.9.6)\n","Requirement already satisfied: dill<0.3,>=0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (0.2.9)\n","Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas->fastai) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas->fastai) (2.5.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fastai) (0.10.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fastai) (1.1.0)\n","Requirement already satisfied: wrapt<1.11.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy>=2.0.18->fastai) (1.10.11)\n","Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy>=2.0.18->fastai) (4.28.1)\n","Requirement already satisfied: msgpack-numpy<0.4.4 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy>=2.0.18->fastai) (0.4.3.2)\n","Requirement already satisfied: msgpack<0.6.0,>=0.5.6 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy>=2.0.18->fastai) (0.5.6)\n","Requirement already satisfied: cytoolz<0.10,>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy>=2.0.18->fastai) (0.9.0.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->fastai) (41.0.1)\n","Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cytoolz<0.10,>=0.9.0->thinc<6.13.0,>=6.12.1->spacy>=2.0.18->fastai) (0.9.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4vEfdDA1YhJ4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":105},"outputId":"d520e32a-4620-42a8-f511-abcad9fb7894","executionInfo":{"status":"ok","timestamp":1557311104250,"user_tz":-120,"elapsed":136480,"user":{"displayName":"Ahmad El Sallab","photoUrl":"","userId":"06251939449358079201"}}},"source":["!pip install attrdict"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Collecting attrdict\n","  Downloading https://files.pythonhosted.org/packages/ef/97/28fe7e68bc7adfce67d4339756e85e9fcf3c6fd7f0c0781695352b70472c/attrdict-2.0.1-py2.py3-none-any.whl\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from attrdict) (1.12.0)\n","Installing collected packages: attrdict\n","Successfully installed attrdict-2.0.1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yp9eTZuJVzwu","colab_type":"text"},"source":["# Prepare Wikitxt"]},{"cell_type":"code","metadata":{"id":"UOZWY7v0Y8z3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"885845b6-b1e7-40f2-c66e-cae5fc84ef14","executionInfo":{"status":"ok","timestamp":1557311106357,"user_tz":-120,"elapsed":138585,"user":{"displayName":"Ahmad El Sallab","photoUrl":"","userId":"06251939449358079201"}}},"source":["!ls assets/wikitext-103/"],"execution_count":13,"outputs":[{"output_type":"stream","text":["itos_wt103.pkl\ttest.txt  train.txt  valid.txt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"raFXD4NXVzwy","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"a5951093-6781-4026-d38b-9f08720e61c2"},"source":["#from preprocessing.create_corpus import *\n","from utils.utils import LoadParameters\n","\n","params = LoadParameters()\n","WIKI103_FOLDER = params.params['wiki103_text_folder']\n","PYTORCH_IDX2WORD_FILEPATH = params.params['pytorch_idx2word_filepath']\n","from utils.utils import LoadParameters\n","\n","create_corpus(PYTORCH_IDX2WORD_FILEPATH, WIKI103_FOLDER)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["1499551it [1:04:34, 737.55it/s]"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"o1_ZhCrQVzw6","colab_type":"text"},"source":["# Train generic LM"]},{"cell_type":"code","metadata":{"id":"-nNBl6rKVzw8","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":519},"outputId":"af8a53fd-0381-441a-ee21-307568b92796","executionInfo":{"status":"error","timestamp":1557318881294,"user_tz":-120,"elapsed":3855,"user":{"displayName":"Ahmad El Sallab","photoUrl":"","userId":"06251939449358079201"}}},"source":["from language_model.train import *\n","\n","\n","import pickle\n","\n","import keras.backend as K\n","import numpy as np\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\n","from keras.optimizers import Adam\n","\n","from language_model.batch_generators import BatchGenerator\n","from language_model.model import build_language_model\n","\n","\n","from utils.utils import LoadParameters\n","\n","# 1. Load parameters from config.yaml file\n","params = LoadParameters()\n","WIKIPEDIA_CORPUS_FILE = params.params['wikipedia_corpus_file']\n","LANGUAGE_MODEL_WEIGHT = params.params['language_model_weight']\n","LANGUAGE_MODEL_PARAMS = params.params['lm_params']\n","\n","epochs = params.params['lm_epochs']\n","batch_size = params.params['lm_batch_size']\n","valid_batch_size = params.params['lm_valid_batch_size']\n","seq_len = params.params['lm_seq_len']\n","\n","# 2. Load Corpus\n","[train, valid, test, word2idx, idx2word] = pickle.load(open(WIKIPEDIA_CORPUS_FILE, 'rb'))\n","\n","train_gen = BatchGenerator(train, batch_size, seq_len, modify_seq_len=True)\n","valid_gen = BatchGenerator(valid, valid_batch_size, seq_len, modify_seq_len=True)\n","\n","K.clear_session()\n","num_words = len(word2idx) + 1\n","\n","model = build_language_model(num_words, **LANGUAGE_MODEL_PARAMS)\n","model.compile(loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy'],\n","              optimizer=Adam(lr=3e-4, beta_1=0.8, beta_2=0.99))\n","\n","model.summary()\n","\n","callbacks = [EarlyStopping(patience=5),\n","             ModelCheckpoint(LANGUAGE_MODEL_WEIGHT, save_weights_only=True)]\n","\n","history = model.fit_generator(train_gen,\n","                              steps_per_epoch=len(train) // (seq_len * batch_size),\n","                              epochs=epochs,\n","                              validation_data=valid_gen,\n","                              validation_steps=len(valid) // (seq_len * batch_size),\n","                              callbacks=callbacks,\n","                              )\n","\n","evaluate_model(model, word2idx, 'i feel sick and go to the ')\n","\n"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-c6bbf2fa1dee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlanguage_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/ulmfit_keras/keras_lm/language_model/train.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras_lm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguage_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_generators\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBatchGenerator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras_lm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguage_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbuild_language_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras_lm'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]},{"cell_type":"code","metadata":{"id":"Nat9yjBUcTjA","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":502},"outputId":"f19919d6-2b78-42b9-a1b5-d491fc64ac30","executionInfo":{"status":"error","timestamp":1557318976023,"user_tz":-120,"elapsed":540,"user":{"displayName":"Ahmad El Sallab","photoUrl":"","userId":"06251939449358079201"}}},"source":["import pickle\n","\n","import keras.backend as K\n","import numpy as np\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\n","from keras.optimizers import Adam\n","\n","from language_model.batch_generators import BatchGenerator\n","from language_model.model import build_language_model\n","\n","\n","def evaluate_model(model, word2idx, test_sentence, num_predictions=5):\n","    \"\"\"\n","    Visual preidictions of the language model. The test_sentence is appended with num_predictions words,\n","    which are predicted as the next words from the model.\n","    :param str test_sentence:\n","    :param int num_predictions:\n","    :return: None\n","    \"\"\"\n","\n","    idx2word = {i: w for w, i in word2idx.items()}\n","    test_sentence = test_sentence.split()\n","    encoded_sentence = [word2idx[w] for w in test_sentence]\n","\n","    for i in range(num_predictions):\n","        X = np.reshape(encoded_sentence, (1, len(encoded_sentence)))\n","\n","        pred = model.predict(X)\n","        answer = np.argmax(pred, axis=2)\n","\n","        predicted_idx = answer[0][-2]\n","        encoded_sentence.append(predicted_idx)\n","\n","    print(' '.join([idx2word[i] for i in encoded_sentence]))\n","\n","\n","from utils import LoadParameters\n","\n","# 1. Load parameters from config.yaml file\n","params = LoadParameters()\n","WIKIPEDIA_CORPUS_FILE = params.params['wikipedia_corpus_file']\n","LANGUAGE_MODEL_WEIGHT = params.params['language_model_weight']\n","LANGUAGE_MODEL_PARAMS = params.params['lm_params']\n","\n","epochs = params.params['lm_epochs']\n","batch_size = params.params['lm_batch_size']\n","valid_batch_size = params.params['lm_valid_batch_size']\n","seq_len = params.params['lm_seq_len']\n","\n","# 2. Load Corpus\n","[train, valid, test, word2idx, idx2word] = pickle.load(open(WIKIPEDIA_CORPUS_FILE, 'rb'))\n","\n","train_gen = BatchGenerator(train, batch_size, seq_len, modify_seq_len=True)\n","valid_gen = BatchGenerator(valid, valid_batch_size, seq_len, modify_seq_len=True)\n","\n","K.clear_session()\n","num_words = len(word2idx) + 1\n","\n","model = build_language_model(num_words, **LANGUAGE_MODEL_PARAMS)\n","model.compile(loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy'],\n","              optimizer=Adam(lr=3e-4, beta_1=0.8, beta_2=0.99))\n","\n","model.summary()\n","\n","callbacks = [EarlyStopping(patience=5),\n","             ModelCheckpoint(LANGUAGE_MODEL_WEIGHT, save_weights_only=True)]\n","\n","history = model.fit_generator(train_gen,\n","                              steps_per_epoch=len(train) // (seq_len * batch_size),\n","                              epochs=epochs,\n","                              validation_data=valid_gen,\n","                              validation_steps=len(valid) // (seq_len * batch_size),\n","                              callbacks=callbacks,\n","                              )\n","\n","evaluate_model(model, word2idx, 'i feel sick and go to the ')\n"],"execution_count":9,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-14735430f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlanguage_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_generators\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBatchGenerator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlanguage_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbuild_language_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/ulmfit_keras/keras_lm/language_model/model.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras_lm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguage_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcustom_layers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTiedEmbeddingsTransposed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras_lm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguage_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcustom_layers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mQRNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras_lm'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]},{"cell_type":"markdown","metadata":{"id":"AsmrbsbPVzxG","colab_type":"text"},"source":["# Fine tune language model"]},{"cell_type":"code","metadata":{"id":"nxHE66PTVzxI","colab_type":"code","colab":{}},"source":["from language_model.finetune_lm import *\n","\n","import pandas as pd\n","from collections import Counter, defaultdict\n","\n","from utils.utils import LoadParameters\n","\n","# 1. Load parameters from config.yaml\n","params = LoadParameters()\n","\n","PYTORCH_ITOS_FILEPATH = params.params['pytorch_idx2word_filepath']\n","WEIGTHS_FILEPATH = params.params['language_model_weight']\n","FINETUNED_WEIGTHS_FILEPATH = params.params['finetuned_language_model_weight']\n","FINETUNED_WORD2IDX_FILEPATH = params.params['finetuned_word2idx_filepath']\n","LANGUAGE_MODEL_PARAMS = params.params['lm_params']\n","FINETUNED_CORPUS_FILEPATH = params.params['finetuned_corpus_filepath']\n","\n","batch_size = params.params['lm_batch_size']\n","seq_length = params.params['lm_seq_len']\n","\n","# 2. Initialize pretrained language model.\n","K.clear_session()\n","with open(PYTORCH_ITOS_FILEPATH, 'rb') as f:\n","    words = pickle.load(f)\n","\n","word2idx = {word: idx for idx, word in enumerate(words)}\n","\n","word2idx = defaultdict(lambda: word2idx['_unk_'], word2idx)\n","\n","num_words = len(word2idx)\n","\n","language_model = build_language_model(num_words, **LANGUAGE_MODEL_PARAMS)\n","language_model.summary()\n","language_model.load_weights(WEIGTHS_FILEPATH)\n","\n","# 3. Open target training dataset. We assume that the dataframes contains the already tokenized sentences.\n","train_df = pd.read_csv(os.path.join(FINETUNED_CORPUS_FILEPATH, 'train.csv'), names=['mood', 'text'])\n","valid_df = pd.read_csv(os.path.join(FINETUNED_CORPUS_FILEPATH, 'test.csv'), names=['mood', 'text'])\n","\n","train_text = read_df(train_df)\n","valid_text = read_df(valid_df)\n","\n","text = train_text + valid_text\n","\n","unique_words = [o for o, c in Counter(text).most_common(100000) if c > 10]\n","\n","# 4. Add new words to the word2idx dictionary and update the language model.\n","num_words_not_in_corpus = len(set(unique_words) - set(word2idx.keys()))\n","word2idx = update_word2idx(unique_words, word2idx)\n","language_model = update_language_model(language_model, num_words_not_in_corpus, **LANGUAGE_MODEL_PARAMS)\n","\n","language_model.summary()\n","\n","# 5. Prepare training and validation data\n","train = [word2idx[word] for word in train_text]\n","valid = [word2idx[word] for word in valid_text]\n","\n","# 6. Finetune model\n","train_gen = BatchGenerator(train, batch_size, seq_length, modify_seq_len=True)\n","valid_gen = BatchGenerator(valid, batch_size, seq_length, modify_seq_len=True)\n","\n","callbacks = [EarlyStopping(patience=5),\n","             ModelCheckpoint(FINETUNED_WEIGTHS_FILEPATH, save_weights_only=True)]\n","\n","language_model.compile(loss=sparse_categorical_crossentropy,\n","                       metrics=['sparse_categorical_accuracy'],\n","                       optimizer=LRMultiplierSGD(lr=0.2 * 3e-4, momentum=0., decay=0., nesterov=False)\n","                       )\n","\n","language_model.fit_generator(train_gen,\n","                             steps_per_epoch=len(train) // (seq_length * batch_size),\n","                             epochs=20,\n","                             validation_data=valid_gen,\n","                             validation_steps=len(valid) // (seq_length * batch_size),\n","                             callbacks=callbacks,\n","                             )\n","\n","evaluate_model(language_model, word2idx, 'i feel sick and go to the', num_predictions=5)\n","\n","# 7. Save word2idx dictionary\n","with open(FINETUNED_WORD2IDX_FILEPATH, 'wb') as f:\n","    pickle.dump(word2idx, f)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CjkFO7FSVzxN","colab_type":"text"},"source":["# ULMFiT integration (LM+Cls)"]},{"cell_type":"code","metadata":{"id":"blfooTRKVzxO","colab_type":"code","colab":{}},"source":["from trasnfer_model.train import *\n","\n","from utils.utils import LoadParameters\n","\n","# 1. Load parameters from config.yaml\n","params = LoadParameters()\n","\n","CORPUS_FILEPATH = params.params['wikipedia_corpus_file']\n","TRAINING_DATA_FILEPATH = params.params['classification_csv']\n","CLASSIFICATON_MODEL_FILE = params.params['classifiaction_language_model_weight']\n","\n","FINETUNED_WEIGTHS_FILEPATH = params.params['finetuned_language_model_weight']\n","NUMBER_OF_LABELS = params.params['number_of_labels']\n","\n","# 2. Initialize pretrained language model.\n","K.clear_session()\n","wikitext_corpus = pickle.load(open(CORPUS_FILEPATH,'rb'))\n","num_words = len(wikitext_corpus.word2idx) +1\n","\n","language_model = build_language_model(num_words, embedding_size=300, use_gpu=True)\n","language_model.summary()\n","language_model.load_weights(FINETUNED_WEIGTHS_FILEPATH)\n","\n","# 3. Initialize classifiaction_model\n","classification_model = build_classification_model(language_model, NUMBER_OF_LABELS, **params.params['cm_params'])\n","\n","# 4. Load X_train from pickle file\n","with open(TRAINING_DATA_FILEPATH, 'rb') as f:\n","    X_train, y_train = pickle.load(f)\n","\n","# 5. Train classification model\n","epochs_list = [1 for layer in classification_model.layers]\n","learning_rates_list = [0.01 for layer in classification_model.layers]\n","train_classifiaction_model(classification_model, X_train, y_train, epochs_list, learning_rates_list)\n","\n","# 6. Save classification model\n","classification_model.save(CLASSIFICATON_MODEL_FILE, overwrite=True)"],"execution_count":0,"outputs":[]}]}