{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ULMFiT_Flat code.ipynb","version":"0.3.2","provenance":[{"file_id":"1jGfIMZiPPJ1DFbKW0l78saV74xMbFz6w","timestamp":1557321182728}],"collapsed_sections":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"x0bGro57Vzvp","colab_type":"text"},"source":["# Get data"]},{"cell_type":"code","metadata":{"id":"5zBkgEdsV1Zv","colab_type":"code","outputId":"f3056462-2ebb-4880-9bdc-388192b9bb9f","executionInfo":{"status":"ok","timestamp":1557493623932,"user_tz":-120,"elapsed":4305,"user":{"displayName":"Ahmad El Sallab","photoUrl":"","userId":"06251939449358079201"}},"colab":{"base_uri":"https://localhost:8080/","height":121}},"source":["!git clone https://github.com/khumbuai/ulmfit_keras.git"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Cloning into 'ulmfit_keras'...\n","remote: Enumerating objects: 1, done.\u001b[K\n","remote: Counting objects: 100% (1/1)   \u001b[K\rremote: Counting objects: 100% (1/1), done.\u001b[K\n","Receiving objects:   0% (1/536)   \rReceiving objects:   1% (6/536)   \rReceiving objects:   2% (11/536)   \rReceiving objects:   3% (17/536)   \rReceiving objects:   4% (22/536)   \rReceiving objects:   5% (27/536)   \rReceiving objects:   6% (33/536)   \rReceiving objects:   7% (38/536)   \rReceiving objects:   8% (43/536)   \rReceiving objects:   9% (49/536)   \rReceiving objects:  10% (54/536)   \rReceiving objects:  11% (59/536)   \rReceiving objects:  12% (65/536)   \rReceiving objects:  13% (70/536)   \rReceiving objects:  14% (76/536)   \rReceiving objects:  15% (81/536)   \rReceiving objects:  16% (86/536)   \rReceiving objects:  17% (92/536)   \rReceiving objects:  18% (97/536)   \rReceiving objects:  19% (102/536)   \rReceiving objects:  20% (108/536)   \rReceiving objects:  21% (113/536)   \rReceiving objects:  22% (118/536)   \rReceiving objects:  23% (124/536)   \rReceiving objects:  24% (129/536)   \rReceiving objects:  25% (134/536)   \rReceiving objects:  26% (140/536)   \rremote: Total 536 (delta 0), reused 0 (delta 0), pack-reused 535\u001b[K\n","Receiving objects:  27% (145/536)   \rReceiving objects:  28% (151/536)   \rReceiving objects:  29% (156/536)   \rReceiving objects:  30% (161/536)   \rReceiving objects:  31% (167/536)   \rReceiving objects:  32% (172/536)   \rReceiving objects:  33% (177/536)   \rReceiving objects:  34% (183/536)   \rReceiving objects:  35% (188/536)   \rReceiving objects:  36% (193/536)   \rReceiving objects:  37% (199/536)   \rReceiving objects:  38% (204/536)   \rReceiving objects:  39% (210/536)   \rReceiving objects:  40% (215/536)   \rReceiving objects:  41% (220/536)   \rReceiving objects:  42% (226/536)   \rReceiving objects:  43% (231/536)   \rReceiving objects:  44% (236/536)   \rReceiving objects:  45% (242/536)   \rReceiving objects:  46% (247/536)   \rReceiving objects:  47% (252/536)   \rReceiving objects:  48% (258/536)   \rReceiving objects:  49% (263/536)   \rReceiving objects:  50% (268/536)   \rReceiving objects:  51% (274/536)   \rReceiving objects:  52% (279/536)   \rReceiving objects:  53% (285/536)   \rReceiving objects:  54% (290/536)   \rReceiving objects:  55% (295/536)   \rReceiving objects:  56% (301/536)   \rReceiving objects:  57% (306/536)   \rReceiving objects:  58% (311/536)   \rReceiving objects:  59% (317/536)   \rReceiving objects:  60% (322/536)   \rReceiving objects:  61% (327/536)   \rReceiving objects:  62% (333/536)   \rReceiving objects:  63% (338/536)   \rReceiving objects:  64% (344/536)   \rReceiving objects:  65% (349/536)   \rReceiving objects:  66% (354/536)   \rReceiving objects:  67% (360/536)   \rReceiving objects:  68% (365/536)   \rReceiving objects:  69% (370/536)   \rReceiving objects:  70% (376/536)   \rReceiving objects:  71% (381/536)   \rReceiving objects:  72% (386/536)   \rReceiving objects:  73% (392/536)   \rReceiving objects:  74% (397/536)   \rReceiving objects:  75% (402/536)   \rReceiving objects:  76% (408/536)   \rReceiving objects:  77% (413/536)   \rReceiving objects:  78% (419/536)   \rReceiving objects:  79% (424/536)   \rReceiving objects:  80% (429/536)   \rReceiving objects:  81% (435/536)   \rReceiving objects:  82% (440/536)   \rReceiving objects:  83% (445/536)   \rReceiving objects:  84% (451/536)   \rReceiving objects:  85% (456/536)   \rReceiving objects:  86% (461/536)   \rReceiving objects:  87% (467/536)   \rReceiving objects:  88% (472/536)   \rReceiving objects:  89% (478/536)   \rReceiving objects:  90% (483/536)   \rReceiving objects:  91% (488/536)   \rReceiving objects:  92% (494/536)   \rReceiving objects:  93% (499/536)   \rReceiving objects:  94% (504/536)   \rReceiving objects:  95% (510/536)   \rReceiving objects:  96% (515/536)   \rReceiving objects:  97% (520/536)   \rReceiving objects:  98% (526/536)   \rReceiving objects:  99% (531/536)   \rReceiving objects: 100% (536/536)   \rReceiving objects: 100% (536/536), 108.75 KiB | 6.80 MiB/s, done.\n","Resolving deltas:   0% (0/343)   \rResolving deltas:   1% (5/343)   \rResolving deltas:   3% (11/343)   \rResolving deltas:   7% (26/343)   \rResolving deltas:  10% (35/343)   \rResolving deltas:  16% (55/343)   \rResolving deltas:  20% (71/343)   \rResolving deltas:  22% (76/343)   \rResolving deltas:  27% (95/343)   \rResolving deltas:  36% (126/343)   \rResolving deltas:  37% (130/343)   \rResolving deltas:  38% (132/343)   \rResolving deltas:  40% (139/343)   \rResolving deltas:  42% (147/343)   \rResolving deltas:  45% (156/343)   \rResolving deltas:  46% (160/343)   \rResolving deltas:  47% (163/343)   \rResolving deltas:  48% (165/343)   \rResolving deltas:  53% (183/343)   \rResolving deltas:  55% (189/343)   \rResolving deltas:  58% (199/343)   \rResolving deltas:  59% (203/343)   \rResolving deltas:  62% (213/343)   \rResolving deltas:  63% (217/343)   \rResolving deltas:  65% (223/343)   \rResolving deltas:  66% (229/343)   \rResolving deltas:  67% (230/343)   \rResolving deltas:  76% (261/343)   \rResolving deltas:  77% (265/343)   \rResolving deltas:  81% (278/343)   \rResolving deltas:  82% (284/343)   \rResolving deltas:  87% (299/343)   \rResolving deltas:  88% (304/343)   \rResolving deltas:  89% (306/343)   \rResolving deltas:  91% (313/343)   \rResolving deltas:  92% (317/343)   \rResolving deltas:  95% (326/343)   \rResolving deltas:  97% (333/343)   \rResolving deltas:  98% (339/343)   \rResolving deltas:  99% (341/343)   \rResolving deltas: 100% (343/343)   \rResolving deltas: 100% (343/343), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KV9CrHRY3W0F","colab_type":"code","colab":{}},"source":["import os\n","os.chdir('./ulmfit_keras/')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4wh17OMs3trs","colab_type":"code","outputId":"42553ec0-788f-4038-ce41-e12b2c43f014","executionInfo":{"status":"ok","timestamp":1557493694887,"user_tz":-120,"elapsed":75155,"user":{"displayName":"Ahmad El Sallab","photoUrl":"","userId":"06251939449358079201"}},"colab":{"base_uri":"https://localhost:8080/","height":572}},"source":["!chmod -R 777 *\n","!./get_data.sh"],"execution_count":4,"outputs":[{"output_type":"stream","text":["=== Acquiring datasets ===\n","---\n","- Downloading WikiText-2 (WT2)\n","- Downloading WikiText-103 (WT2)\n","--2019-05-10 13:07:35--  https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-v1.zip\n","Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.184.253\n","Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.184.253|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 190229076 (181M) [application/zip]\n","Saving to: ‘wikitext-103-v1.zip’\n","\n","wikitext-103-v1.zip 100%[===================>] 181.42M  45.8MB/s    in 4.4s    \n","\n","2019-05-10 13:07:40 (41.5 MB/s) - ‘wikitext-103-v1.zip’ saved [190229076/190229076]\n","\n","- Downloading enwik8 (Character)\n","--2019-05-10 13:07:45--  http://mattmahoney.net/dc/enwik8.zip\n","Resolving mattmahoney.net (mattmahoney.net)... 67.195.197.75\n","Connecting to mattmahoney.net (mattmahoney.net)|67.195.197.75|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 36445475 (35M) [application/zip]\n","Saving to: ‘enwik8.zip’\n","\n","enwik8.zip          100%[===================>]  34.76M   877KB/s    in 41s     \n","\n","2019-05-10 13:08:27 (863 KB/s) - ‘enwik8.zip’ saved [36445475/36445475]\n","\n","python3: can't open file 'prep_enwik8.py': [Errno 2] No such file or directory\n","- Downloading Penn Treebank (PTB)\n","- Downloading Penn Treebank (Character)\n","---\n","Happy language modeling :)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qI4JaOlz34BX","colab_type":"code","colab":{}},"source":["!mkdir keras_lm/assets"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"R11fdlSu4nVS","colab_type":"code","colab":{}},"source":["!cp -rf data/* keras_lm/assets"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1ft8xrje47e4","colab_type":"code","colab":{}},"source":["!rm -rf data"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZTTO5XR7WPK2","colab_type":"code","colab":{}},"source":["import os\n","os.chdir('./keras_lm')\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y0yJTBIsVzvw","colab_type":"text"},"source":["## Datasets"]},{"cell_type":"markdown","metadata":{"id":"IjbiL9ovVzwG","colab_type":"text"},"source":["## Pretrained models"]},{"cell_type":"code","metadata":{"id":"WAfOyHhxVzwJ","colab_type":"code","outputId":"e9cf4578-e784-4fb7-9368-02533bb5b0c9","executionInfo":{"status":"ok","timestamp":1557493708131,"user_tz":-120,"elapsed":88187,"user":{"displayName":"Ahmad El Sallab","photoUrl":"","userId":"06251939449358079201"}},"colab":{"base_uri":"https://localhost:8080/","height":208}},"source":["\n","!wget -P assets/wikitext-103 http://files.fast.ai/models/wt103/itos_wt103.pkl"],"execution_count":9,"outputs":[{"output_type":"stream","text":["--2019-05-10 13:08:54--  http://files.fast.ai/models/wt103/itos_wt103.pkl\n","Resolving files.fast.ai (files.fast.ai)... 67.205.15.147\n","Connecting to files.fast.ai (files.fast.ai)|67.205.15.147|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 4161252 (4.0M) [text/plain]\n","Saving to: ‘assets/wikitext-103/itos_wt103.pkl’\n","\n","itos_wt103.pkl      100%[===================>]   3.97M  7.05MB/s    in 0.6s    \n","\n","2019-05-10 13:08:55 (7.05 MB/s) - ‘assets/wikitext-103/itos_wt103.pkl’ saved [4161252/4161252]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RiyLJLifVzwV","colab_type":"code","outputId":"9d2db058-3bd6-4a9e-c4c9-1df3e02bbc44","executionInfo":{"status":"ok","timestamp":1557493720914,"user_tz":-120,"elapsed":100939,"user":{"displayName":"Ahmad El Sallab","photoUrl":"","userId":"06251939449358079201"}},"colab":{"base_uri":"https://localhost:8080/","height":208}},"source":["!wget -P assets/weights/ http://files.fast.ai/models/wt103/fwd_wt103.h5"],"execution_count":10,"outputs":[{"output_type":"stream","text":["--2019-05-10 13:08:57--  http://files.fast.ai/models/wt103/fwd_wt103.h5\n","Resolving files.fast.ai (files.fast.ai)... 67.205.15.147\n","Connecting to files.fast.ai (files.fast.ai)|67.205.15.147|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 462387687 (441M) [text/plain]\n","Saving to: ‘assets/weights/fwd_wt103.h5’\n","\n","fwd_wt103.h5        100%[===================>] 440.97M  41.0MB/s    in 11s     \n","\n","2019-05-10 13:09:08 (39.2 MB/s) - ‘assets/weights/fwd_wt103.h5’ saved [462387687/462387687]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mR26vgSlVzwe","colab_type":"code","colab":{}},"source":["#!wget -xP assets/weights/  http://files.fast.ai/models/wt103/fwd_wt103_enc.h5\n","#!wget -xP assets/weights/  http://files.fast.ai/models/wt103/bwd_wt103_enc.h5\n","#!wget -xP assets/weights/  http://files.fast.ai/models/wt103/bwd_wt103.h5"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_kjyZQH1Vzwk","colab_type":"text"},"source":["## Install packages"]},{"cell_type":"code","metadata":{"id":"sGWaev_UVzwl","colab_type":"code","outputId":"6608362a-152a-4bfb-deb5-19668f600155","executionInfo":{"status":"ok","timestamp":1557493771991,"user_tz":-120,"elapsed":151954,"user":{"displayName":"Ahmad El Sallab","photoUrl":"","userId":"06251939449358079201"}},"colab":{"base_uri":"https://localhost:8080/","height":921}},"source":["!pip install torch_nightly -f https://download.pytorch.org/whl/nightly/cu92/torch_nightly.html\n","!pip install fastai"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Looking in links: https://download.pytorch.org/whl/nightly/cu92/torch_nightly.html\n","Collecting torch_nightly\n","\u001b[?25l  Downloading https://download.pytorch.org/whl/nightly/cu92/torch_nightly-1.0.0.dev20181206-cp36-cp36m-linux_x86_64.whl (576.2MB)\n","\u001b[K     |████████████████████████████████| 576.2MB 24kB/s \n","\u001b[?25hInstalling collected packages: torch-nightly\n","Successfully installed torch-nightly-1.0.0.dev20181206\n","Requirement already satisfied: fastai in /usr/local/lib/python3.6/dist-packages (1.0.52)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from fastai) (0.2.2.post3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from fastai) (2.21.0)\n","Requirement already satisfied: fastprogress>=0.1.19 in /usr/local/lib/python3.6/dist-packages (from fastai) (0.1.21)\n","Requirement already satisfied: nvidia-ml-py3 in /usr/local/lib/python3.6/dist-packages (from fastai) (7.352.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from fastai) (0.24.2)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from fastai) (3.0.3)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from fastai) (1.2.1)\n","Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from fastai) (1.1.0)\n","Requirement already satisfied: spacy>=2.0.18 in /usr/local/lib/python3.6/dist-packages (from fastai) (2.0.18)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from fastai) (19.0)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from fastai) (4.3.0)\n","Requirement already satisfied: numexpr in /usr/local/lib/python3.6/dist-packages (from fastai) (2.6.9)\n","Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.6/dist-packages (from fastai) (1.16.3)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from fastai) (0.6)\n","Requirement already satisfied: bottleneck in /usr/local/lib/python3.6/dist-packages (from fastai) (1.2.1)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from fastai) (4.6.3)\n","Requirement already satisfied: typing in /usr/local/lib/python3.6/dist-packages (from fastai) (3.6.6)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from fastai) (3.13)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision->fastai) (1.12.0)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->fastai) (2.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->fastai) (2019.3.9)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->fastai) (3.0.4)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->fastai) (1.24.2)\n","Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas->fastai) (2.5.3)\n","Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas->fastai) (2018.9)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fastai) (0.10.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fastai) (2.4.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fastai) (1.1.0)\n","Requirement already satisfied: ujson>=1.35 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (1.35)\n","Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (0.9.6)\n","Requirement already satisfied: dill<0.3,>=0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (0.2.9)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (1.0.2)\n","Requirement already satisfied: thinc<6.13.0,>=6.12.1 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (6.12.1)\n","Requirement already satisfied: regex==2018.01.10 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (2018.1.10)\n","Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (2.0.1)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (2.0.2)\n","Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow->fastai) (0.46)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->fastai) (41.0.1)\n","Requirement already satisfied: msgpack-numpy<0.4.4 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy>=2.0.18->fastai) (0.4.3.2)\n","Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy>=2.0.18->fastai) (4.28.1)\n","Requirement already satisfied: cytoolz<0.10,>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy>=2.0.18->fastai) (0.9.0.1)\n","Requirement already satisfied: msgpack<0.6.0,>=0.5.6 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy>=2.0.18->fastai) (0.5.6)\n","Requirement already satisfied: wrapt<1.11.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy>=2.0.18->fastai) (1.10.11)\n","Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cytoolz<0.10,>=0.9.0->thinc<6.13.0,>=6.12.1->spacy>=2.0.18->fastai) (0.9.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4vEfdDA1YhJ4","colab_type":"code","outputId":"6819086b-85c8-40c5-b7af-e38779dd887b","executionInfo":{"status":"ok","timestamp":1557493776192,"user_tz":-120,"elapsed":156126,"user":{"displayName":"Ahmad El Sallab","photoUrl":"","userId":"06251939449358079201"}},"colab":{"base_uri":"https://localhost:8080/","height":124}},"source":["!pip install attrdict"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Collecting attrdict\n","  Downloading https://files.pythonhosted.org/packages/ef/97/28fe7e68bc7adfce67d4339756e85e9fcf3c6fd7f0c0781695352b70472c/attrdict-2.0.1-py2.py3-none-any.whl\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from attrdict) (1.12.0)\n","Installing collected packages: attrdict\n","Successfully installed attrdict-2.0.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"K_ZM3KSDlunH","colab_type":"code","colab":{}},"source":["import numpy as np\n","import csv\n","import urllib3\n","import os\n","import pathlib\n","import yaml\n","from attrdict import AttrDict\n","import pandas as pd\n","\n","from fastai.text import Tokenizer\n","from fastai.text import partition_by_cores\n","\n","CONFIG_PATH = str(pathlib.Path(os.getcwd()) / 'configs' / 'config.yaml')\n","\n","\n","# Alex Martelli's 'Borg'\n","# http://python-3-patterns-idioms-test.readthedocs.io/en/latest/Singleton.html\n","class _Borg:\n","    _shared_state = {}\n","\n","    def __init__(self):\n","        self.__dict__ = self._shared_state\n","\n","\n","class LoadParameters(_Borg):\n","    def __init__(self, fallback_file=CONFIG_PATH):\n","        _Borg.__init__(self)\n","\n","        self.fallback_file = fallback_file\n","        self.params = self._read_yaml()\n","\n","    def _read_yaml(self):\n","        with open(self.fallback_file) as f:\n","            config = yaml.load(f)\n","        return AttrDict(config)\n","\n","\n","\n","def preprocess_imdb_sentiments(root_directory):\n","    \"\"\"\n","    Transforms the IMDB sentiment dataset into a dataframe.\n","\n","    IMBD dataset can be found on http://ai.stanford.edu/~amaas/data/sentiment/\n","    :param root_directory: Root directory containing the train and test folders\n","    :return:\n","    \"\"\"\n","\n","    classes = ['neg', 'pos']\n","\n","    def get_texts(path):\n","        texts,labels = [],[]\n","        for idx, label in enumerate(classes):\n","            for fname in os.listdir(os.path.join(path, label)):\n","                if fname.endswith('.txt'):\n","                    with open(os.path.join(os.path.join(path, label), fname), 'r') as text:\n","                        texts.append(text.read())\n","                        labels.append(idx)\n","\n","        texts = Tokenizer.proc_all_mp(partition_by_cores(texts))\n","        texts = [' '.join(text) + '<eos>' for text in texts]\n","\n","        return np.array(texts), np.array(labels)\n","\n","    trn_texts, trn_labels = get_texts(os.path.join(root_directory, 'train'))\n","    val_texts, val_labels = get_texts(os.path.join(root_directory, 'test'))\n","\n","    col_names = ['labels', 'text']\n","\n","    np.random.seed(42)\n","    trn_idx = np.random.permutation(len(trn_texts))\n","    val_idx = np.random.permutation(len(val_texts))\n","\n","    trn_texts = trn_texts[trn_idx]\n","    val_texts = val_texts[val_idx]\n","\n","    trn_labels = trn_labels[trn_idx]\n","    val_labels = val_labels[val_idx]\n","\n","    df_trn = pd.DataFrame({'text':trn_texts, 'labels':trn_labels}, columns=col_names)\n","    df_val = pd.DataFrame({'text':val_texts, 'labels':val_labels}, columns=col_names)\n","\n","    df_trn[df_trn['labels'] != 2].to_csv(os.path.join(root_directory, 'train.csv'), header=False, index=False)\n","    df_val.to_csv(os.path.join(root_directory, 'test.csv'), header=False, index=False)\n","\n","\n","def maybe_download(filename, source_url, work_directory):\n","    \"\"\"Download the filename from the source url into the working directory if it does not exist.\"\"\"\n","    if not os.path.exists(work_directory):\n","        os.makedirs(work_directory)\n","    filepath = os.path.join(work_directory, filename)\n","    if not os.path.exists(filepath):\n","        urllib3.disable_warnings()\n","        with urllib3.PoolManager() as http:\n","            r = http.request('GET', source_url)\n","            with open(filepath, 'wb') as fout:\n","                fout.write(r.read())\n","\n","        print('{} succesfully downloaded'.format(filename))\n","    return filepath\n","\n","\n","def write_file(file_path, text_path, num_tokens):\n","    total_num_tokens = 0\n","    print(f'Writing to {file_path}...')\n","    with open(file_path, 'w', encoding='utf-8') as f_out:\n","        writer = csv.writer(f_out)\n","        with open(text_path, 'r') as f:\n","            for i, text in enumerate(f):\n","\n","                writer.writerow([text])\n","                # f_out.write(text)\n","\n","                # calculate approximate length based on tokens\n","                total_num_tokens += len(text.split())\n","                if total_num_tokens > num_tokens:\n","                    break\n","                if i % 10000 == 0:\n","                    print('Processed {:,} documents. Total # tokens: {:,}.'.format(i, total_num_tokens))\n","    print('{}. # documents: {:,}. # tokens: {:,}.'.format(file_path, i, total_num_tokens))\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yp9eTZuJVzwu","colab_type":"text"},"source":["# Prepare Wikitxt"]},{"cell_type":"code","metadata":{"id":"UOZWY7v0Y8z3","colab_type":"code","outputId":"ee02fc59-1c46-4506-f73a-e8a875b8365e","executionInfo":{"status":"ok","timestamp":1557493780521,"user_tz":-120,"elapsed":160403,"user":{"displayName":"Ahmad El Sallab","photoUrl":"","userId":"06251939449358079201"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["!ls assets/wikitext-103/"],"execution_count":15,"outputs":[{"output_type":"stream","text":["itos_wt103.pkl\ttest.txt  train.txt  valid.txt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"m-wId0sFbhfr","colab_type":"code","colab":{}},"source":["import os\n","import pickle\n","from collections import Counter\n","\n","from tqdm import tqdm\n","import re\n","import html\n","\n","from fastai.text import partition_by_cores, Tokenizer\n","import spacy\n","import sys\n","\n","\n","class Missingdict(dict):\n","    \"\"\"\n","    Own implementation of defaultdict, which does not add a new key: value pair for keys which were not in the dictionary\n","    before lookup.\n","    \"\"\"\n","    def __init__(self, *args,  default_value=None, **kwargs,):\n","        super(Missingdict, self).__init__(*args, **kwargs)\n","        self.default_value = default_value\n","\n","    def __missing__(self, key):\n","        return self.default_value\n","\n","\n","class Corpus(object):\n","    \"\"\"\n","    Loads the train.txt, valid.txt and test.txt files and tokenizes them.\n","    The class then contains the train, valid and test corpus (with word2idx applied), as well as the word2idx dictionary.\n","    \"\"\"\n","\n","    def __init__(self, path, lang='en', max_vocab=30000, min_freq=10, word2idx=None):\n","        try:\n","            spacy.load(lang)\n","        except OSError:\n","            print(f'spacy tokenization model is not installed for {lang}.')\n","            lang = lang if lang in ['en', 'de', 'es', 'pt', 'fr', 'it', 'nl'] else 'xx'\n","            print(f'Command: python -m spacy download {lang}')\n","            sys.exit(1)\n","\n","        self.max_vocab = max_vocab\n","        self.min_freq = min_freq\n","        self.lang = lang\n","        self.re1 = re.compile(r'  +')\n","\n","        self.train_txt_path = os.path.join(path, 'train.txt')  # train.txt has 1 801 349 lines\n","        self.valid_txt_path = os.path.join(path, 'valid.txt')\n","        self.test_txt_path = os.path.join(path, 'test.txt')\n","\n","        tokenized_train = self.tokenize(self.train_txt_path)\n","        if not word2idx:\n","            # Create word2idx from training corpus\n","            self.word2idx = self.create_word2idx(tokenized_train)\n","        else:\n","            self.word2idx = word2idx\n","        self.idx2word = {idx: word for word, idx in self.word2idx.items()}\n","\n","        self.train = [self.word2idx[word] for word in tokenized_train]\n","        del tokenized_train\n","\n","        tokenized_valid = self.tokenize(self.valid_txt_path)\n","        self.valid = [self.word2idx[word] for word in tokenized_valid]\n","        del tokenized_valid\n","\n","        tokenized_test = self.tokenize(self.test_txt_path)\n","        self.test = [self.word2idx[word] for word in tokenized_test]\n","        del tokenized_test\n","\n","        self.word2idx = dict(word2idx)\n","\n","    def fixup(self, x):\n","        \"\"\"\n","        Fixes some observed weird tokens\n","        :param x:\n","        :return:\n","        \"\"\"\n","        x = x.replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n","            'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n","            '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>','u_n').replace(' @.@ ','.').replace(\n","            ' @-@ ','-').replace('\\\\', ' \\\\ ')\n","        return self.re1.sub(' ', html.unescape(x))\n","\n","    def create_word2idx(self, tokenized_corpus):\n","        freq = Counter(tokenized_corpus)\n","        print(freq.most_common(25))\n","\n","        idx2word = [o for o, c in freq.most_common(self.max_vocab) if c > self.min_freq]\n","        idx2word.insert(0, '_pad_')\n","        idx2word.insert(0, '_unk_')\n","\n","        word2idx = {v: k for k, v in enumerate(idx2word)}\n","        word2idx = Missingdict(word2idx, default_value=0)\n","\n","        return word2idx\n","\n","    def tokenize(self, path):\n","        \"\"\"\n","        Tokenizes a text file. Every 50000 lines of a file, the text will be tokenized with\n","        fast.ai's tokenizer.\n","        :param path:\n","        :param num_tokens:\n","        :return:\n","        \"\"\"\n","        tokenized_corpus = []\n","        text = ''\n","        with open(path, 'r') as f:\n","            for i, line in tqdm(enumerate(f)):\n","                line = self.fixup(line)\n","                text += 'BOS' + line + 'EOS'\n","                if i % 50000 == 0 and i > 0:\n","                    tokenized_corpus += self._apply_tokenizer(text)\n","                    text = ''\n","        tokenized_corpus += self._apply_tokenizer(text)\n","\n","        return tokenized_corpus\n","\n","    def _apply_tokenizer(self, text):\n","        # Tokenizer creates a list, containing a list of each word pairs, such as\n","        #tokens = [[' ', 't_up', 'bos'], ['senjō'], ['no'], [' ', 't_up', 'eos']]\n","        tokens = Tokenizer(lang=self.lang).process_all(partition_by_cores(text.split(), n_cpus=8))\n","        # Flatten nested list\n","        return [word for sublist in tokens for word in sublist]\n","\n","\n","def create_corpus(PYTORCH_IDX2WORD_FILEPATH, WIKI103_FOLDER):\n","    # 1. Load words from pretrained pytorch language model\n","    # The itos_wt103.pkl file can be found on http://files.fast.ai/models/wt103/\n","    with open(PYTORCH_IDX2WORD_FILEPATH, 'rb') as f:\n","        words = pickle.load(f)\n","    word2idx = {word: idx for idx, word in enumerate(words)}\n","    word2idx = Missingdict(word2idx, default_value=0)\n","\n","    # 2. Create corpus from Wiki103 files\n","    corpus = Corpus(WIKI103_FOLDER, word2idx=word2idx)\n","\n","    to_save = [corpus.train, corpus.valid, corpus.test, corpus.word2idx, corpus.idx2word]\n","    with open(os.path.join(WIKI103_FOLDER, 'wikitext-103.corpus'), 'wb') as f:\n","        pickle.dump(to_save, f)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"raFXD4NXVzwy","colab_type":"code","outputId":"144ae29a-4a10-48bf-fd4f-1a3060c58b8c","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["\n","\n","params = LoadParameters()\n","WIKI103_FOLDER = params.params['wiki103_text_folder']\n","PYTORCH_IDX2WORD_FILEPATH = params.params['pytorch_idx2word_filepath']\n","from utils.utils import LoadParameters\n","\n","create_corpus(PYTORCH_IDX2WORD_FILEPATH, WIKI103_FOLDER)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["1346349it [58:13, 525.29it/s]"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"o1_ZhCrQVzw6","colab_type":"text"},"source":["# Train generic LM"]},{"cell_type":"code","metadata":{"id":"tRNa9o2rlX7y","colab_type":"code","colab":{}},"source":["import numpy as np\n","\n","\n","class BatchGenerator:\n","\n","    def __init__(self, tokenized_text, batch_size, seq_len, modify_seq_len=True, only_last=False):\n","        \"\"\"\n","        :param array tokenized_text: array of encoded text\n","        :param batch_size:\n","        :param bool only_last: Names the model, for which we need batches\n","        :param bool modify_seq_len: Determines, whether the sequence length should be randomly changed after each batch\n","        \"\"\"\n","        self.tokenized_text = tokenized_text\n","        self.batch_size = batch_size\n","        self.modify_seq_len = modify_seq_len\n","        self.only_last = only_last\n","        self.seq_len = seq_len\n","\n","        self.pos = 0\n","\n","    @staticmethod\n","    def _random_modify_seq_len(seq_len):\n","        seq_len = seq_len if np.random.random() < 0.95 else seq_len / 2.\n","        # Prevent excessively small or negative sequence lengths\n","        return max(5, int(np.random.normal(seq_len, 5)))\n","\n","\n","    def get_sample(self, pos, batch_seq_len):\n","        \"\"\"\n","        Returns one x, y pair\n","        :param pos:\n","        :param batch_seq_len:\n","        :return:\n","        \"\"\"\n","        start = (pos + 1) % len(self.tokenized_text)\n","        end = (pos + 1 + batch_seq_len) % len(self.tokenized_text)\n","        if start > end:  # text sequence is exhausted -> roll over to the start again\n","            start = len(self.tokenized_text) - batch_seq_len\n","            end = len(self.tokenized_text)\n","           \n","        data = self.tokenized_text[start -1: end - 1]\n","        target = self.tokenized_text[end - 1] if self.only_last else self.tokenized_text[start:end]\n","        # need to expand the dimension, such that Y has shape (batch_size, seq_len, 1) (for normal model),\n","        # or (batch_size, 1) (for many_to_one model)\n","        # -> needed for sparse_categorical_crossentropy loss\n","        target = np.expand_dims(target, axis=1)\n","\n","        return data, target\n","\n","    def generate_one_batch(self, pos, batch_seq_len):\n","        \"\"\"\n","        Batch is of size:\n","        X: (batch_size, seq_len) Y: (batch_size, seq_len, 1) for self.model_description == normal\n","        X: (batch_size, seq_len) Y: (batch_size, 1) for self.model_description == many_to_one\n","        X: (2, batch_size, seq_len) Y: (batch_size, seq_len) for self.model_description == fast\n","        :param int pos:\n","        :param int seq_len:\n","        :return:\n","        \"\"\"\n","        X = []\n","        Y = []\n","        for i in range(self.batch_size):\n","            x, y = self.get_sample(pos, batch_seq_len)\n","            X.append(x)\n","            Y.append(y)\n","            pos = (pos + batch_seq_len) % len(self.tokenized_text)\n","\n","        return np.array(X), np.array(Y)\n","\n","    def __iter__(self):\n","        return self\n","\n","    def __next__(self):\n","        \"\"\"\n","        Generates a batch for training/validation\n","        :param seq_len:\n","        :return:\n","        \"\"\"\n","        batch_seq_len = self.seq_len\n","        if self.modify_seq_len:\n","            batch_seq_len = self._random_modify_seq_len(batch_seq_len)\n","\n","        X, Y = self.generate_one_batch(self.pos, batch_seq_len)\n","        self.pos = (self.pos + batch_seq_len * self.batch_size) % len(self.tokenized_text)\n","\n","        return X, Y\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mjWv30eMlhAl","colab_type":"code","colab":{}},"source":["import os\n","\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n","from keras.layers import Input, CuDNNLSTM, Embedding, Dense, LSTM, TimeDistributed, Dropout\n","from keras.models import Model\n","\n","# -*- coding: utf-8 -*-\n","# from https://raw.githubusercontent.com/DingKe/nn_playground/master/qrnn/qrnn.py\n","\n","\n","from __future__ import absolute_import\n","import numpy as np\n","\n","from keras import backend as K\n","from keras import activations, initializers, regularizers, constraints\n","from keras.layers import Layer, InputSpec\n","from keras.utils.conv_utils import conv_output_length\n","\n","\n","def _dropout(x, level, noise_shape=None, seed=None):\n","    x = K.dropout(x, level, noise_shape, seed)\n","    x *= (1. - level)  # compensate for the scaling by the dropout\n","    return x\n","\n","\n","class QRNN(Layer):\n","    '''Quasi RNN\n","\n","    # Arguments\n","        units: dimension of the internal projections and the final output.\n","\n","    # References\n","        - [Quasi-recurrent Neural Networks](http://arxiv.org/abs/1611.01576)\n","    '''\n","\n","    def __init__(self, units, window_size=2, stride=1,\n","                 return_sequences=False, go_backwards=False,\n","                 stateful=False, unroll=False, activation='tanh',\n","                 kernel_initializer='uniform', bias_initializer='zero',\n","                 kernel_regularizer=None, bias_regularizer=None,\n","                 activity_regularizer=None,\n","                 kernel_constraint=None, bias_constraint=None,\n","                 dropout=0, use_bias=True, input_dim=None, input_length=None,\n","                 **kwargs):\n","        self.return_sequences = return_sequences\n","        self.go_backwards = go_backwards\n","        self.stateful = stateful\n","        self.unroll = unroll\n","\n","        self.units = units\n","        self.window_size = window_size\n","        self.strides = (stride, 1)\n","\n","        self.use_bias = use_bias\n","        self.activation = activations.get(activation)\n","        self.kernel_initializer = initializers.get(kernel_initializer)\n","        self.bias_initializer = initializers.get(bias_initializer)\n","        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n","        self.bias_regularizer = regularizers.get(bias_regularizer)\n","        self.activity_regularizer = regularizers.get(activity_regularizer)\n","        self.kernel_constraint = constraints.get(kernel_constraint)\n","        self.bias_constraint = constraints.get(bias_constraint)\n","\n","        self.dropout = dropout\n","        self.supports_masking = True\n","        self.input_spec = [InputSpec(ndim=3)]\n","        self.input_dim = input_dim\n","        self.input_length = input_length\n","        if self.input_dim:\n","            kwargs['input_shape'] = (self.input_length, self.input_dim)\n","        super(QRNN, self).__init__(**kwargs)\n","\n","    def build(self, input_shape):\n","        if isinstance(input_shape, list):\n","            input_shape = input_shape[0]\n","\n","        batch_size = input_shape[0] if self.stateful else None\n","        self.input_dim = input_shape[2]\n","        self.input_spec = InputSpec(shape=(batch_size, None, self.input_dim))\n","        self.state_spec = InputSpec(shape=(batch_size, self.units))\n","\n","        self.states = [None]\n","        if self.stateful:\n","            self.reset_states()\n","\n","        kernel_shape = (self.window_size, 1, self.input_dim, self.units * 3)\n","        self.kernel = self.add_weight(name='kernel',\n","                                      shape=kernel_shape,\n","                                      initializer=self.kernel_initializer,\n","                                      regularizer=self.kernel_regularizer,\n","                                      constraint=self.kernel_constraint)\n","        if self.use_bias:\n","            self.bias = self.add_weight(name='bias',\n","                                        shape=(self.units * 3,),\n","                                        initializer=self.bias_initializer,\n","                                        regularizer=self.bias_regularizer,\n","                                        constraint=self.bias_constraint)\n","\n","        self.built = True\n","\n","    def compute_output_shape(self, input_shape):\n","        if isinstance(input_shape, list):\n","            input_shape = input_shape[0]\n","\n","        length = input_shape[1]\n","        if length:\n","            length = conv_output_length(length + self.window_size - 1,\n","                                        self.window_size, 'valid',\n","                                        self.strides[0])\n","        if self.return_sequences:\n","            return (input_shape[0], length, self.units)\n","        else:\n","            return (input_shape[0], self.units)\n","\n","    def compute_mask(self, inputs, mask):\n","        if self.return_sequences:\n","            return mask\n","        else:\n","            return None\n","\n","    def get_initial_states(self, inputs):\n","        # build an all-zero tensor of shape (samples, units)\n","        initial_state = K.zeros_like(inputs)  # (samples, timesteps, input_dim)\n","        initial_state = K.sum(initial_state, axis=(1, 2))  # (samples,)\n","        initial_state = K.expand_dims(initial_state)  # (samples, 1)\n","        initial_state = K.tile(initial_state, [1, self.units])  # (samples, units)\n","        initial_states = [initial_state for _ in range(len(self.states))]\n","        return initial_states\n","\n","    def reset_states(self, states=None):\n","        if not self.stateful:\n","            raise AttributeError('Layer must be stateful.')\n","        if not self.input_spec:\n","            raise RuntimeError('Layer has never been called '\n","                               'and thus has no states.')\n","\n","        batch_size = self.input_spec.shape[0]\n","        if not batch_size:\n","            raise ValueError('If a QRNN is stateful, it needs to know '\n","                             'its batch size. Specify the batch size '\n","                             'of your input tensors: \\n'\n","                             '- If using a Sequential model, '\n","                             'specify the batch size by passing '\n","                             'a `batch_input_shape` '\n","                             'argument to your first layer.\\n'\n","                             '- If using the functional API, specify '\n","                             'the time dimension by passing a '\n","                             '`batch_shape` argument to your Input layer.')\n","\n","        if self.states[0] is None:\n","            self.states = [K.zeros((batch_size, self.units))\n","                           for _ in self.states]\n","        elif states is None:\n","            for state in self.states:\n","                K.set_value(state, np.zeros((batch_size, self.units)))\n","        else:\n","            if not isinstance(states, (list, tuple)):\n","                states = [states]\n","            if len(states) != len(self.states):\n","                raise ValueError('Layer ' + self.name + ' expects ' +\n","                                 str(len(self.states)) + ' states, '\n","                                                         'but it received ' + str(len(states)) +\n","                                 'state values. Input received: ' +\n","                                 str(states))\n","            for index, (value, state) in enumerate(zip(states, self.states)):\n","                if value.shape != (batch_size, self.units):\n","                    raise ValueError('State ' + str(index) +\n","                                     ' is incompatible with layer ' +\n","                                     self.name + ': expected shape=' +\n","                                     str((batch_size, self.units)) +\n","                                     ', found shape=' + str(value.shape))\n","                K.set_value(state, value)\n","\n","    def __call__(self, inputs, initial_state=None, **kwargs):\n","        # If `initial_state` is specified,\n","        # and if it a Keras tensor,\n","        # then add it to the inputs and temporarily\n","        # modify the input spec to include the state.\n","        if initial_state is not None:\n","            if hasattr(initial_state, '_keras_history'):\n","                # Compute the full input spec, including state\n","                input_spec = self.input_spec\n","                state_spec = self.state_spec\n","                if not isinstance(state_spec, list):\n","                    state_spec = [state_spec]\n","                self.input_spec = [input_spec] + state_spec\n","\n","                # Compute the full inputs, including state\n","                if not isinstance(initial_state, (list, tuple)):\n","                    initial_state = [initial_state]\n","                inputs = [inputs] + list(initial_state)\n","\n","                # Perform the call\n","                output = super(QRNN, self).__call__(inputs, **kwargs)\n","\n","                # Restore original input spec\n","                self.input_spec = input_spec\n","                return output\n","            else:\n","                kwargs['initial_state'] = initial_state\n","        return super(QRNN, self).__call__(inputs, **kwargs)\n","\n","    def call(self, inputs, mask=None, initial_state=None, training=None):\n","        # input shape: `(samples, time (padded with zeros), input_dim)`\n","        # note that the .build() method of subclasses MUST define\n","        # self.input_spec and self.state_spec with complete input shapes.\n","        if isinstance(inputs, list):\n","            initial_states = inputs[1:]\n","            inputs = inputs[0]\n","        elif initial_state is not None:\n","            pass\n","        elif self.stateful:\n","            initial_states = self.states\n","        else:\n","            initial_states = self.get_initial_states(inputs)\n","\n","        if len(initial_states) != len(self.states):\n","            raise ValueError('Layer has ' + str(len(self.states)) +\n","                             ' states but was passed ' +\n","                             str(len(initial_states)) +\n","                             ' initial states.')\n","        input_shape = K.int_shape(inputs)\n","        if self.unroll and input_shape[1] is None:\n","            raise ValueError('Cannot unroll a RNN if the '\n","                             'time dimension is undefined. \\n'\n","                             '- If using a Sequential model, '\n","                             'specify the time dimension by passing '\n","                             'an `input_shape` or `batch_input_shape` '\n","                             'argument to your first layer. If your '\n","                             'first layer is an Embedding, you can '\n","                             'also use the `input_length` argument.\\n'\n","                             '- If using the functional API, specify '\n","                             'the time dimension by passing a `shape` '\n","                             'or `batch_shape` argument to your Input layer.')\n","        constants = self.get_constants(inputs, training=None)\n","        preprocessed_input = self.preprocess_input(inputs, training=None)\n","\n","        last_output, outputs, states = K.rnn(self.step, preprocessed_input,\n","                                             initial_states,\n","                                             go_backwards=self.go_backwards,\n","                                             mask=mask,\n","                                             constants=constants,\n","                                             unroll=self.unroll,\n","                                             input_length=input_shape[1])\n","        if self.stateful:\n","            updates = []\n","            for i in range(len(states)):\n","                updates.append((self.states[i], states[i]))\n","            self.add_update(updates, inputs)\n","\n","        # Properly set learning phase\n","        if 0 < self.dropout < 1:\n","            last_output._uses_learning_phase = True\n","            outputs._uses_learning_phase = True\n","\n","        if self.return_sequences:\n","            return outputs\n","        else:\n","            return last_output\n","\n","    def preprocess_input(self, inputs, training=None):\n","        if self.window_size > 1:\n","            inputs = K.temporal_padding(inputs, (self.window_size - 1, 0))\n","        inputs = K.expand_dims(inputs, 2)  # add a dummy dimension\n","\n","        output = K.conv2d(inputs, self.kernel, strides=self.strides,\n","                          padding='valid',\n","                          data_format='channels_last')\n","        output = K.squeeze(output, 2)  # remove the dummy dimension\n","        if self.use_bias:\n","            output = K.bias_add(output, self.bias, data_format='channels_last')\n","\n","        if self.dropout is not None and 0. < self.dropout < 1.:\n","            z = output[:, :, :self.units]\n","            f = output[:, :, self.units:2 * self.units]\n","            o = output[:, :, 2 * self.units:]\n","            f = K.in_train_phase(1 - _dropout(1 - f, self.dropout), f, training=training)\n","            return K.concatenate([z, f, o], -1)\n","        else:\n","            return output\n","\n","    def step(self, inputs, states):\n","        prev_output = states[0]\n","\n","        z = inputs[:, :self.units]\n","        f = inputs[:, self.units:2 * self.units]\n","        o = inputs[:, 2 * self.units:]\n","\n","        z = self.activation(z)\n","        f = f if self.dropout is not None and 0. < self.dropout < 1. else K.sigmoid(f)\n","        o = K.sigmoid(o)\n","\n","        output = f * prev_output + (1 - f) * z\n","        output = o * output\n","\n","        return output, [output]\n","\n","    def get_constants(self, inputs, training=None):\n","        return []\n","\n","    def get_config(self):\n","        config = {'units': self.units,\n","                  'window_size': self.window_size,\n","                  'stride': self.strides[0],\n","                  'return_sequences': self.return_sequences,\n","                  'go_backwards': self.go_backwards,\n","                  'stateful': self.stateful,\n","                  'unroll': self.unroll,\n","                  'use_bias': self.use_bias,\n","                  'dropout': self.dropout,\n","                  'activation': activations.serialize(self.activation),\n","                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n","                  'bias_initializer': initializers.serialize(self.bias_initializer),\n","                  'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n","                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n","                  'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n","                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n","                  'bias_constraint': constraints.serialize(self.bias_constraint),\n","                  'input_dim': self.input_dim,\n","                  'input_length': self.input_length}\n","        base_config = super(QRNN, self).get_config()\n","        return dict(list(base_config.items()) + list(config.items()))\n","\n","\n","class TiedEmbeddingsTransposed(Layer):\n","    \"\"\"Layer for tying embeddings in an output layer.\n","    A regular embedding layer has the shape: V x H (V: size of the vocabulary. H: size of the projected space).\n","    In this layer, we'll go: H x V.\n","    With the same weights than the regular embedding.\n","    In addition, it may have an activation.\n","    # References\n","        - [ Using the Output Embedding to Improve Language Models](https://arxiv.org/abs/1608.05859)\n","    \"\"\"\n","\n","    def __init__(self, tied_to=None,\n","                 activation=None,\n","                 **kwargs):\n","        super(TiedEmbeddingsTransposed, self).__init__(**kwargs)\n","        self.tied_to = tied_to\n","        self.activation = activations.get(activation)\n","\n","    def build(self, input_shape):\n","        self.transposed_weights = K.transpose(self.tied_to.weights[0])\n","        self.built = True\n","\n","    def compute_mask(self, inputs, mask=None):\n","        return mask\n","\n","    def compute_output_shape(self, input_shape):\n","        return input_shape[0], K.int_shape(self.tied_to.weights[0])[0]\n","\n","    def call(self, inputs, mask=None):\n","        output = K.dot(inputs, self.transposed_weights)\n","        if self.activation is not None:\n","            output = self.activation(output)\n","        return output\n","\n","\n","    def get_config(self):\n","        config = {'activation': activations.serialize(self.activation)\n","                  }\n","        base_config = super(TiedEmbeddingsTransposed, self).get_config()\n","        return dict(list(base_config.items()) + list(config.items()))\n","\n","\n","\n","\n","def build_language_model(num_words, embedding_size=400, rnn_sizes=(1150, 1150),\n","                         dropout=0.1, dropouth=0.3, dropouti=0.2, dropoute=0.1, wdrop=0.5,\n","                         tie_weights=True, use_qrnn=False, use_gpu=True, only_last=False):\n","\n","    inp = Input(shape=(None,), name='input')\n","    emb = Embedding(num_words, embedding_size, name='embedding')\n","    emb_inp = emb(inp)\n","    emb_inp = Dropout(dropouti)(emb_inp)\n","\n","    if use_qrnn:\n","        rnn = QRNN(rnn_sizes[0], return_sequences=True, window_size=2)(emb_inp)\n","        for rnn_size in rnn_sizes[1:]:\n","            rnn = QRNN(rnn_size, return_sequences=True, window_size=1)(rnn)\n","        if only_last:\n","            rnn = QRNN(embedding_size, return_sequences=False, window_size=1, name='final_rnn_layer')(rnn)\n","        else:\n","            rnn = QRNN(embedding_size, return_sequences=True, window_size=1, name='final_rnn_layer')(rnn)\n","    else:\n","        RnnUnit = CuDNNLSTM if use_gpu else LSTM\n","        rnn = RnnUnit(rnn_sizes[0], return_sequences=True, name='0_rnn_layer')(emb_inp)\n","        for i, rnn_size in enumerate(rnn_sizes[1:]):\n","            rnn = RnnUnit(rnn_size, return_sequences=True, name='{}_rnn_layer'.format(i + 1))(rnn)\n","        if only_last:\n","            rnn = RnnUnit(embedding_size, return_sequences=False, name='final_rnn_layer')(rnn)\n","        else:\n","            rnn = RnnUnit(embedding_size, return_sequences=True, name='final_rnn_layer')(rnn)\n","\n","    if tie_weights:\n","        softmax_layer = TiedEmbeddingsTransposed(tied_to=emb, activation='softmax')\n","    else:\n","        softmax_layer = Dense(num_words, activation='softmax')\n","\n","    if only_last:\n","        logits = softmax_layer(rnn)\n","    else:\n","        logits = TimeDistributed(softmax_layer)(rnn)\n","\n","    out = Dropout(dropout)(logits)\n","    model = Model(inputs=inp, outputs=out)\n","    return model\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-nNBl6rKVzw8","colab_type":"code","colab":{}},"source":["import pickle\n","\n","import keras.backend as K\n","import numpy as np\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\n","from keras.optimizers import Adam\n","\n","\n","\n","def evaluate_model(model, word2idx, test_sentence, num_predictions=5):\n","    \"\"\"\n","    Visual preidictions of the language model. The test_sentence is appended with num_predictions words,\n","    which are predicted as the next words from the model.\n","    :param str test_sentence:\n","    :param int num_predictions:\n","    :return: None\n","    \"\"\"\n","\n","    idx2word = {i: w for w, i in word2idx.items()}\n","    test_sentence = test_sentence.split()\n","    encoded_sentence = [word2idx[w] for w in test_sentence]\n","\n","    for i in range(num_predictions):\n","        X = np.reshape(encoded_sentence, (1, len(encoded_sentence)))\n","\n","        pred = model.predict(X)\n","        answer = np.argmax(pred, axis=2)\n","\n","        predicted_idx = answer[0][-2]\n","        encoded_sentence.append(predicted_idx)\n","\n","    print(' '.join([idx2word[i] for i in encoded_sentence]))\n","\n","\n","\n","\n","# 1. Load parameters from config.yaml file\n","params = LoadParameters()\n","WIKIPEDIA_CORPUS_FILE = params.params['wikipedia_corpus_file']\n","LANGUAGE_MODEL_WEIGHT = params.params['language_model_weight']\n","LANGUAGE_MODEL_PARAMS = params.params['lm_params']\n","\n","epochs = params.params['lm_epochs']\n","batch_size = params.params['lm_batch_size']\n","valid_batch_size = params.params['lm_valid_batch_size']\n","seq_len = params.params['lm_seq_len']\n","\n","# 2. Load Corpus\n","[train, valid, test, word2idx, idx2word] = pickle.load(open(WIKIPEDIA_CORPUS_FILE, 'rb'))\n","\n","train_gen = BatchGenerator(train, batch_size, seq_len, modify_seq_len=True)\n","valid_gen = BatchGenerator(valid, valid_batch_size, seq_len, modify_seq_len=True)\n","\n","K.clear_session()\n","num_words = len(word2idx) + 1\n","\n","model = build_language_model(num_words, **LANGUAGE_MODEL_PARAMS)\n","model.compile(loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy'],\n","              optimizer=Adam(lr=3e-4, beta_1=0.8, beta_2=0.99))\n","\n","model.summary()\n","\n","callbacks = [EarlyStopping(patience=5),\n","             ModelCheckpoint(LANGUAGE_MODEL_WEIGHT, save_weights_only=True)]\n","\n","history = model.fit_generator(train_gen,\n","                              steps_per_epoch=len(train) // (seq_len * batch_size),\n","                              epochs=epochs,\n","                              validation_data=valid_gen,\n","                              validation_steps=len(valid) // (seq_len * batch_size),\n","                              callbacks=callbacks,\n","                              )\n","\n","evaluate_model(model, word2idx, 'i feel sick and go to the ')\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AsmrbsbPVzxG","colab_type":"text"},"source":["# Fine tune language model"]},{"cell_type":"code","metadata":{"id":"HlwScRROmVe-","colab_type":"code","colab":{}},"source":["\"\"\"\n","Implementation of Discriminative fine-tuning as explained in https://arxiv.org/pdf/1801.06146.pdf\n","\n","Adatped from https://github.com/brunoklein99/srcnn/blob/5e874eb161d4d27cfdb6ac9b2196b3ad154fc672/LRMultiplierSGD.py#L46\n","\"\"\"\n","\n","import keras.backend as K\n","from keras.legacy import interfaces\n","from keras.optimizers import Optimizer\n","\n","\n","class LRMultiplierSGD(Optimizer):\n","    \"\"\"Stochastic gradient descent optimizer.\n","    Implements the Discriminative fine-tuning as explained in https://arxiv.org/pdf/1801.06146.pdf\n","    Current implementation is such that it is assumed that each layer consists of two update parameters,\n","    namely weights and bias.\n","\n","    Includes support for momentum,\n","    learning rate decay, and Nesterov momentum.\n","    Example use:\n","    opt = LRMultiplierSGD(lr=1e-4, momentum=0.9, multipliers=[1, 1, 1, 1, 0.1, 0.1])\n","    model.compile(optimizer=opt, loss=mean_squared_error, metrics=[psnr])\n","\n","    # Arguments\n","        lr: float >= 0. Learning rate.\n","        momentum: float >= 0. Parameter that accelerates SGD\n","            in the relevant direction and dampens oscillations.\n","        decay: float >= 0. Learning rate decay over each update.\n","        nesterov: boolean. Whether to apply Nesterov momentum.\n","    \"\"\"\n","\n","    def __init__(self, lr=0.01, momentum=0., decay=0., discriminative_decay=1 / 2.6,\n","                 nesterov=False, **kwargs):\n","        super(LRMultiplierSGD, self).__init__(**kwargs)\n","        with K.name_scope(self.__class__.__name__):\n","            self.iterations = K.variable(0, dtype='int64', name='iterations')\n","            self.lr = K.variable(lr, name='lr')\n","            self.momentum = K.variable(momentum, name='momentum')\n","            self.decay = K.variable(decay, name='decay')\n","            self.discriminative_decay = discriminative_decay\n","        self.initial_decay = decay\n","        self.nesterov = nesterov\n","\n","\n","    def _set_up_discriminative_fine_tuning(self, params):\n","        \"\"\"\n","        Sets up the decay for different layers in the network. The decay is calculated according to\n","        decay[layer_(i + 1)] = decay[layer_i] * self.discriminative_decay. The layer last to the output is layer_0\n","        and is set such that decay[layer_0] = 1.\n","        :param params:\n","        :return:\n","        \"\"\"\n","        # Example of param.name:\n","        # lstm_1/kernel:0\n","        # lstm_1/recurrent_kernel:0\n","        # lstm_1/bias:0\n","        # lstm_2/kernel:0\n","        # lstm_2/recurrent_kernel:0\n","        # lstm_2/bias:0\n","        # dense_1/kernel:0\n","        # dense_1/bias:0\n","        names = [param.name.split('/')[0] for param in params]\n","        number_of_layers = len(set(names))\n","\n","        def list_to_depth(names):\n","            \"\"\"\n","            ['lstm_1', 'lstm_1', 'lstm_1', 'lstm_2', 'lstm_2', 'lstm_2', 'dense_1', 'dense_1']\n","            goes to\n","            [1, 1, 1, 2, 2, 2, 3, 3]\n","            :param names:\n","            :return:\n","            \"\"\"\n","            # TODO: refactor\n","            layer_depths = [1]\n","            layer_depth = 1\n","            for i, name in enumerate(names[1:]):\n","                if name != names[i]:\n","                    layer_depth += 1\n","                layer_depths.append(layer_depth)\n","            return layer_depths\n","\n","        layer_depths = list_to_depth(names)\n","\n","        layer_decay = [K.variable(self.discriminative_decay ** (number_of_layers - depth)) for depth in layer_depths]\n","        return layer_decay\n","\n","    @interfaces.legacy_get_updates_support\n","    def get_updates(self, loss, params):\n","        \"\"\"\n","\n","        :param loss:\n","        :param list[tf.Variable] params: list of tensorflow weights and biases\n","        :return:\n","        \"\"\"\n","        grads = self.get_gradients(loss, params)\n","        self.updates = [K.update_add(self.iterations, 1)]\n","\n","        lr = self.lr\n","        if self.initial_decay > 0:\n","            lr *= (1. / (1. + self.decay * K.cast(self.iterations,\n","                                                  K.dtype(self.decay))))\n","        # momentum\n","        shapes = [K.int_shape(p) for p in params]\n","        moments = [K.zeros(shape) for shape in shapes]\n","        self.weights = [self.iterations] + moments\n","\n","        # Discriminative fine-tuning:\n","        # each layer is updated according to the idea from https://arxiv.org/pdf/1801.06146.pdf, after eq(2)\n","\n","        # The learning rate of the uppermost trainable layer is decreased by discriminative_fine_tuning\n","        # len(params) // 2 is (approximately) the number of layers on the model -> weight+ bias per each layer\n","        discriminative_fine_tuning = self._set_up_discriminative_fine_tuning(params)\n","\n","        for i, (p, g, m) in enumerate(zip(params, grads, moments)):\n","\n","            v = self.momentum * m - (lr * discriminative_fine_tuning[i]) * g  # velocity\n","\n","            self.updates.append(K.update(m, v))\n","\n","            if self.nesterov:\n","                new_p = p + self.momentum * v - (lr * discriminative_fine_tuning[i]) * g\n","            else:\n","                new_p = p + v\n","\n","            # Apply constraints.\n","            if getattr(p, 'constraint', None) is not None:\n","                new_p = p.constraint(new_p)\n","\n","            self.updates.append(K.update(p, new_p))\n","        return self.updates\n","\n","    def get_config(self):\n","        config = {'lr': float(K.get_value(self.lr)),\n","                  'momentum': float(K.get_value(self.momentum)),\n","                  'decay': float(K.get_value(self.decay)),\n","                  'nesterov': self.nesterov}\n","        base_config = super(LRMultiplierSGD, self).get_config()\n","        return dict(list(base_config.items()) + list(config.items()))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZYFLzDJnmIS7","colab_type":"code","colab":{}},"source":["\"\"\"\n","Fintunes a pretrained language model on a new language corpus.\n","\"\"\"\n","\n","import os\n","import pickle\n","\n","import keras.backend as K\n","import numpy as np\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\n","from keras.losses import sparse_categorical_crossentropy\n","\n","\n","def read_df(df):\n","    \"\"\"\n","    Maps all entries in df.text into one single list.\n","    :param df:\n","    :return:\n","    \"\"\"\n","    texts = [text for text in df.text]\n","    texts = ' '.join(texts)\n","    return texts\n","\n","\n","def update_word2idx(word_list, word2idx):\n","    '''\n","    Updates the word2idx dictionary with all words which are in the word_list, but do not appear\n","    in the word2idx.keys()\n","    '''\n","    words_not_in_corpus = list(set(word_list) - set(word2idx.keys()))\n","    additional_word2idx = {words_not_in_corpus[i]: len(word2idx) + i\n","                           for i in range(len(words_not_in_corpus))\n","                           }\n","    word2idx.update(additional_word2idx)\n","\n","    return word2idx\n","\n","\n","def update_embedding_weights(embedding_weights, num_words_not_in_corpus):\n","    embedding_weights = embedding_weights[0]  # shape (len(language_corpus.word2idx), embedding_size)\n","    mean_embedding_vector = embedding_weights.mean(axis=0)\n","    embedding_weights = np.append(embedding_weights, [mean_embedding_vector for _ in range(num_words_not_in_corpus)], axis=0)\n","    return embedding_weights\n","\n","\n","def update_language_model(language_model, num_words_not_in_corpus, **kwargs):\n","    weights = language_model.get_weights()\n","\n","    old_embedding_weights = language_model.get_layer('embedding').get_weights()  # shape (len(language_corpus.word2idx), embedding_size)\n","    new_embedding_weights = update_embedding_weights(old_embedding_weights, num_words_not_in_corpus)\n","\n","    weights[0] = new_embedding_weights\n","\n","    updated_language_model = build_language_model(num_words=new_embedding_weights.shape[0],\n","                                                  **kwargs)\n","\n","    updated_language_model.set_weights(weights)\n","\n","    return updated_language_model\n","\n","\n","import pandas as pd\n","from collections import Counter, defaultdict\n","\n","\n","\n","# 1. Load parameters from config.yaml\n","params = LoadParameters()\n","\n","PYTORCH_ITOS_FILEPATH = params.params['pytorch_idx2word_filepath']\n","WEIGTHS_FILEPATH = params.params['language_model_weight']\n","FINETUNED_WEIGTHS_FILEPATH = params.params['finetuned_language_model_weight']\n","FINETUNED_WORD2IDX_FILEPATH = params.params['finetuned_word2idx_filepath']\n","LANGUAGE_MODEL_PARAMS = params.params['lm_params']\n","FINETUNED_CORPUS_FILEPATH = params.params['finetuned_corpus_filepath']\n","\n","batch_size = params.params['lm_batch_size']\n","seq_length = params.params['lm_seq_len']\n","\n","# 2. Initialize pretrained language model.\n","K.clear_session()\n","with open(PYTORCH_ITOS_FILEPATH, 'rb') as f:\n","    words = pickle.load(f)\n","\n","word2idx = {word: idx for idx, word in enumerate(words)}\n","\n","word2idx = defaultdict(lambda: word2idx['_unk_'], word2idx)\n","\n","num_words = len(word2idx)\n","\n","language_model = build_language_model(num_words, **LANGUAGE_MODEL_PARAMS)\n","language_model.summary()\n","language_model.load_weights(WEIGTHS_FILEPATH)\n","\n","# 3. Open target training dataset. We assume that the dataframes contains the already tokenized sentences.\n","train_df = pd.read_csv(os.path.join(FINETUNED_CORPUS_FILEPATH, 'train.csv'), names=['mood', 'text'])\n","valid_df = pd.read_csv(os.path.join(FINETUNED_CORPUS_FILEPATH, 'test.csv'), names=['mood', 'text'])\n","\n","train_text = read_df(train_df)\n","valid_text = read_df(valid_df)\n","\n","text = train_text + valid_text\n","\n","unique_words = [o for o, c in Counter(text).most_common(100000) if c > 10]\n","\n","# 4. Add new words to the word2idx dictionary and update the language model.\n","num_words_not_in_corpus = len(set(unique_words) - set(word2idx.keys()))\n","word2idx = update_word2idx(unique_words, word2idx)\n","language_model = update_language_model(language_model, num_words_not_in_corpus, **LANGUAGE_MODEL_PARAMS)\n","\n","language_model.summary()\n","\n","# 5. Prepare training and validation data\n","train = [word2idx[word] for word in train_text]\n","valid = [word2idx[word] for word in valid_text]\n","\n","# 6. Finetune model\n","train_gen = BatchGenerator(train, batch_size, seq_length, modify_seq_len=True)\n","valid_gen = BatchGenerator(valid, batch_size, seq_length, modify_seq_len=True)\n","\n","callbacks = [EarlyStopping(patience=5),\n","             ModelCheckpoint(FINETUNED_WEIGTHS_FILEPATH, save_weights_only=True)]\n","\n","language_model.compile(loss=sparse_categorical_crossentropy,\n","                       metrics=['sparse_categorical_accuracy'],\n","                       optimizer=LRMultiplierSGD(lr=0.2 * 3e-4, momentum=0., decay=0., nesterov=False)\n","                       )\n","\n","language_model.fit_generator(train_gen,\n","                             steps_per_epoch=len(train) // (seq_length * batch_size),\n","                             epochs=20,\n","                             validation_data=valid_gen,\n","                             validation_steps=len(valid) // (seq_length * batch_size),\n","                             callbacks=callbacks,\n","                             )\n","\n","evaluate_model(language_model, word2idx, 'i feel sick and go to the', num_predictions=5)\n","\n","# 7. Save word2idx dictionary\n","with open(FINETUNED_WORD2IDX_FILEPATH, 'wb') as f:\n","    pickle.dump(word2idx, f)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CjkFO7FSVzxN","colab_type":"text"},"source":["# ULMFiT integration (LM+Cls)"]},{"cell_type":"code","metadata":{"id":"Z6GbrqXpmozW","colab_type":"code","colab":{}},"source":["from keras.layers import Lambda, GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, BatchNormalization, Dense, Dropout\n","from keras.models import Model\n","\n","\n","def build_classification_model(language_model, num_labels, dense_units=(128, 128), dropouts=(0.1, 0.1)):\n","    \"\"\"\n","    Transfer model for language classification. Implementation of the transfer model as explained in\n","    https://arxiv.org/abs/1801.06146.\n","    :param language_model:\n","    :param lr:\n","    :param lr_d:\n","    :param kernel_size1:\n","    :param kernel_size2:\n","    :param dense_units:\n","    :param dropout:\n","    :param filters:\n","    :return:\n","    \"\"\"\n","    rnn_output = language_model.get_layer('final_rnn_layer').output\n","\n","    avg_pool = GlobalAveragePooling1D()(rnn_output)\n","    max_pool = GlobalMaxPooling1D()(rnn_output)\n","    last_rnn_output = Lambda(lambda x: x[:, -1, :])(rnn_output)\n","\n","    x = concatenate([avg_pool, max_pool, last_rnn_output])\n","\n","    for num_units, dropout in zip(dense_units, dropouts):\n","        x = Dense(num_units, activation='relu') (x)\n","        x = Dropout(dropout)(x)\n","        x = BatchNormalization()(x)\n","\n","    x = Dense(num_labels, activation=\"sigmoid\")(x)\n","\n","    model = Model(inputs=language_model.input, outputs=x)\n","\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MZ-eyKJCmv8w","colab_type":"code","colab":{}},"source":["import pickle\n","\n","import keras.backend as K\n","from keras.losses import categorical_crossentropy\n","\n","def train_classifiaction_model(classification_model, X_train, y_train, epochs_list, learning_rates_list):\n","    \"\"\"\n","    Implementation of transfer training by unfreezing the layers step by step.\n","    :param classification_model:\n","    :param X_train:\n","    :param y_train:\n","    :param List[int] epochs_list: list containing the number of epochs for each unfreezing step\n","    :param List[int] learning_rates_list: list containing the learning rates for each unfreezing step\n","    :return:\n","    \"\"\"\n","    for layer in classification_model.layers:\n","        layer.trainable = False\n","\n","    for i, layer in enumerate(reversed(classification_model.layers)):\n","        layer.trainable = True\n","\n","        classification_model.compile(loss=categorical_crossentropy, optimizer=LRMultiplierSGD(lr=learning_rates_list[i],\n","                                                                                               momentum=0., decay=0.,\n","                                                                                               nesterov=False))\n","\n","        classification_model.fit(X_train, y_train,\n","                                 batch_size=None,\n","                                 epochs=epochs_list[i],\n","                                 verbose=1,\n","                                 callbacks=None,\n","                                 validation_split=0.1,\n","                                 shuffle=True)\n","\n","    return classification_model\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"blfooTRKVzxO","colab_type":"code","colab":{}},"source":["\n","# 1. Load parameters from config.yaml\n","params = LoadParameters()\n","\n","CORPUS_FILEPATH = params.params['wikipedia_corpus_file']\n","TRAINING_DATA_FILEPATH = params.params['classification_csv']\n","CLASSIFICATON_MODEL_FILE = params.params['classifiaction_language_model_weight']\n","\n","FINETUNED_WEIGTHS_FILEPATH = params.params['finetuned_language_model_weight']\n","NUMBER_OF_LABELS = params.params['number_of_labels']\n","\n","# 2. Initialize pretrained language model.\n","K.clear_session()\n","wikitext_corpus = pickle.load(open(CORPUS_FILEPATH,'rb'))\n","num_words = len(wikitext_corpus.word2idx) +1\n","\n","language_model = build_language_model(num_words, embedding_size=300, use_gpu=True)\n","language_model.summary()\n","language_model.load_weights(FINETUNED_WEIGTHS_FILEPATH)\n","\n","# 3. Initialize classifiaction_model\n","classification_model = build_classification_model(language_model, NUMBER_OF_LABELS, **params.params['cm_params'])\n","\n","# 4. Load X_train from pickle file\n","with open(TRAINING_DATA_FILEPATH, 'rb') as f:\n","    X_train, y_train = pickle.load(f)\n","\n","# 5. Train classification model\n","epochs_list = [1 for layer in classification_model.layers]\n","learning_rates_list = [0.01 for layer in classification_model.layers]\n","train_classifiaction_model(classification_model, X_train, y_train, epochs_list, learning_rates_list)\n","\n","# 6. Save classification model\n","classification_model.save(CLASSIFICATON_MODEL_FILE, overwrite=True)"],"execution_count":0,"outputs":[]}]}