{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <Center>Neural Language Modeling with Hierarichal Attention Seq2Seq models</Center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Char level input: OOV. Good for spelling correction\n",
    "- Word level output: Good for language knowledge\n",
    "- Hierarichy is good to capture language structure and semantics. Also, it's good to keep the LSTM sequence at reasonable length. Example: char2char is good up to 50 chars then it becomes worse.\n",
    "- Importance of Embedding for char embedding, with mask_zero=0 to suppress padding. PAD symbole must take 0 entry in the car2int vocab to be masked\n",
    "- Importance of attention: idea, two ways (Bahdanau and Lyoung) before and after for LM and spelling. Visualization.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
